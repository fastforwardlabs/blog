<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />
    
<title>Transfer Learning - from the ground up</title>
<meta property="og:title" content="Transfer Learning - from the ground up">
<meta property="description" content="Machine learning enables us to build systems that can predict the world around us: like what movies we’d like to watch, how much traffic we’ll experience on our morning commute, or what words we’ll type next in our emails.
There are many types of models and tasks. Face detection models transform raw image pixels into high level signals (like the presence and position of eyes, noses, and ears) and then use those signals to locate all faces in an image.">
<meta property="og:description" content="Machine learning enables us to build systems that can predict the world around us: like what movies we’d like to watch, how much traffic we’ll experience on our morning commute, or what words we’ll type next in our emails.
There are many types of models and tasks. Face detection models transform raw image pixels into high level signals (like the presence and position of eyes, noses, and ears) and then use those signals to locate all faces in an image.">
<meta property="og:image" content="https://blog.fastforwardlabs.com/images/editor_uploads/2019-09-06-213558-fig_10.png">
<meta property="og:url" content="https://blog.fastforwardlabs.com/2019/09/05/transfer-learning-from-the-ground-up.html">
<meta property="twitter:card" content="summary_large_image">
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <link rel="stylesheet" type="text/css" href="/style.css" />
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-53030428-5', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

  </head>
  <body>
      <div class="container">
        <div class="spacer"></div>
        <div style="height: 2.5rem; padding-top: 0.35rem;">
          <a target="_blank" href="https://www.cloudera.com/products/fast-forward-labs-research.html">
            <img style="height: 2rem;" src="/images/cloudera-fast-forward-logo.png" />
          </a>
        </div>
        <div class="spacer"></div>
      </div>
      <main id="main">
        
<div class="container">
  <div>
    <h3 class="clear"><a href="/">Blog</a></h3>
  </div>
  <div class="spacer"></div>
  <div class="post">
    <h5 class="clear">
      <span>Sep 5, 2019</span> &middot;
      <span style="text-transform: capitalize;">
        featured post
      </span>
    </h5>
    <h1>Transfer Learning - from the ground up</h1>
    <p>Machine learning enables us to build systems that can predict the world around us: like what movies we’d like to watch, how much traffic we’ll experience on our morning commute, or what words we’ll type next in our emails.</p>
<p>There are many types of models and tasks. Face detection models transform raw image pixels into high level signals (like the presence and position of eyes, noses, and ears) and then use those signals to locate all faces in an image. Time series models can use sensor measurements to extract long-term trends and seasonal patterns in order to predict future observations. Text prediction models extract information about the meaning of past sentences, grammaticality, and emotions in the text in order to predict the next word or phrase that you’ll type.</p>
<p>To do this, we use algorithms to train a model to accomplish a task. At the beginning, the model knows nothing, then we show it examples of training data for that task. As the model sees more and more examples it learns the basic concepts, then finer details, of how to carry out the task. Importantly, models are never explicitly told what types of information to extract; they learn to extract whatever types of information are useful to solving a specific task. Once trained, the model can transform raw data (text, images, gps coordinates, etc.) into useful signals that can then be used to make predictions.</p>
<h3 id="transfer-learning">Transfer Learning</h3>
<p>Traditionally, a model is trained from scratch, and on only one training dataset, for one specific task. This process is naive and brittle, and it requires lots of data. Transfer learning provides a smarter alternative.</p>
<p>Consider two image classification tasks: in one case a model should learn to distinguish between pictures of cats and dogs, and in a second case a model should learn to distinguish between pictures of cats and bears. Without transfer learning, we would train two totally separate models, each learning the concept of cats and dogs or cats and bears from scratch. But it seems obvious that the type of information that would be useful in distinguishing between cats and dogs (identifying different types of ears, eyes, fur, or common surrounding scenery) would also be helpful in distinguishing cats from bears. With transfer learning, we can use models that already know how to extract useful information because they have learned from other, related tasks.</p>
<p><img src="/images/editor_uploads/2019-09-06-213347-fig_29.png" alt=""></p>
<p>Imagine that a customer service manager would like to build an automated system to answer simple questions from customers so the customers don’t need to be forwarded to human representatives. Current machine learning technology is sufficiently advanced to build such a system, but would require tens of thousands of question/answer examples, a fleet of expensive processors called GPUs, and an expert practitioner in both deep learning and natural language processing. Not knowing about transfer learning, the manager decides these resources are just too difficult to come by, so the project never gets off the ground.</p>
<p>Now imagine that the manager does know about transfer learning. With transfer learning, the data requirement may be reduced to a few hundred examples, a single GPU will suffice, and a non-expert data scientist can build the system. Transfer learning enables the data scientist to reuse a model that is already trained to extract information from text. This pre-trained model has been trained using easily available, generic, self-labeling data to learn the basics of language. This general understanding of language helps the model with the specific task of question answering. The data scientist can download this model freely from the internet, and then train it on a small amount of data so that it can specialize on the type of data the company has. With transfer learning, the project becomes tenable and a minimum viable product is deployed in a matter of weeks.</p>
<p>Let’s explore how natural language processing models work.</p>
<h3 id="modern-nlp-systems">Modern NLP Systems</h3>
<p>One extremely common input to machine learning systems is natural (human) language, often in the form of text. Machine learning systems can do numerous tasks by processing natural language, including: next-word prediction, identifying sentiment or emotion, extracting topics, or answering questions. Even though these tasks have very different outputs, they all involve extracting similar types of information from text input. In fact, we can break down most any modern NLP system into the following components:</p>
<p><img src="/images/editor_uploads/2019-09-06-213418-fig_02.png" alt=""></p>
<p><strong>Tokenizer.</strong> The tokenizer is a fixed component (no machine learning) that breaks up a piece of text into a sequence of symbols, known as tokens. These tokens might be individual words or letters depending on the type of tokenization used.</p>
<p><strong>Token Embedder.</strong> The token embedder turns the individual tokens into numerical form. There are various ways to do this, but most commonly a token embedder learns individual vector representations for each different token. These vector representations are often called word embeddings.</p>
<p><strong>Contextual Token Embedder.</strong> The contextual token embedder is responsible for extracting information by analyzing all tokens in tandem. It may learn how tokens interact (e.g., how past adjectives modify a noun), it may take token order into account, and it may build hierarchical features of language. In deep NLP systems, the model is often a type of recurrent neural network. Its output is essentially contextualized token embeddings - continuous representations of tokens that take past or future context into account.</p>
<p><strong>Predictor.</strong> The predictor is responsible for taking the contextualized token embeddings and turning them into a prediction of the correct type. When predicting ratings from restaurant reviews, that would be a numerical star rating. For a machine translation system, that would be a string of text in another language.</p>
<p>This common structure for NLP systems is something we can take advantage of. Regardless of the end goal, each system needs a token embedder and a contextual token embedder to efficiently extract useful information from text. In short, a token embedder or contextual token embedder that was trained on one task (prediction) is probably usable and useful in a separate task. Only the prediction component is particularly specific to the prediction task. This means that we don’t need to train these complex components (the token embedder and contextual token embedder) from scratch every time - they can be reused. This helps circumvent challenges relating to lack of data.</p>
<p>One common example of this is the transfer of token embeddings through algorithms like Word2Vec. Word2Vec uses abundant, publicly available data (like Wikipedia) to learn continuous embeddings for words. The output of Word2Vec is a set of word vectors that preserve some of the meaning of words.</p>
<p><img src="/images/editor_uploads/2019-09-06-213531-fig_01.png" alt=""></p>
<p>For example, synonyms like “dwelling,” “house,” and “home” may be clustered together. Even analogies like “Paris is to France as Berlin is to Germany” may be preserved. It is clear that token embeddings store some information about the meaning of words that is useful. The meanings of these tokens are the same for most datasets; “dwelling,” “house,” and “home” are synonyms (or at least very closely related) regardless of the dataset. Therefore, it is wasteful to re-learn these embeddings for every new task. With small datasets, it may be impossible to learn them anyway. Instead, we can learn them once from a large, generic dataset, and then reuse them for any model that requires token embeddings.</p>
<p>Transferring the token embedder provides better token embeddings, but the impact is somewhat limited. The contextual token embedder has a far more difficult task - while the token embedder learns the meaning of words, the contextual token embedder learns the meaning of language. Recent breakthroughs in academic research have shown that it is not only possible but extremely advantageous to transfer the contextual token embedder.</p>
<h3 id="transferring-the-contextual-token-embedder">Transferring the Contextual Token Embedder</h3>
<p>The contextual token embedder is the workhorse of modern NLP systems. It is usually a sophisticated deep learning model (like an RNN or a Transformer) that can do powerful things given enough data. However, in many valuable business applications there simply isn’t enough data to train these models. The consequence is that simpler models must be used instead, which can yield low-performance.</p>
<p>With recent breakthroughs, we can train the contextual token embedder using a related task called language modeling and transfer the “knowledge” of that training to another task. Consider a case where we want to train a model that can extract the sentiment of restaurant reviews. The classic approach would be to hand label a selection of reviews as positive or negative, and then train a model to learn from these examples. But we may not have or be able to get enough data to train a powerful model from scratch. Instead, we can use the task of language modeling which doesn’t require any labels. Language modeling is the task of predicting missing words in a sequence of text.</p>
<p><img src="/images/editor_uploads/2019-09-06-213558-fig_10.png" alt=""></p>
<p>The key is that language models must learn to extract pieces of information that are relevant to other tasks as well. In this example, a language model needs to understand how negation in the first sentence impacts the missing word - that it is probably positive since “isn’t bad” is similar to ”good.” Similarly, negation is a very important concept in sentiment analysis. So, in the process of training a good language model, we end up training a contextual token embedder that could be useful in sentiment analysis. Crucially, we have far more data points for a language model because it doesn’t require hand-labeling; that is, the labels come from the text itself. The label is simply the next word in the text.</p>
<p>This is one example of a very general strategy for training models: train a language model using an extremely large corpus of text, and then transfer the contextual token embedder from that model to do a related task that is valuable to your business.</p>
<p>There are a number of pretrained models available for download. Google has released a powerful model called BERT, Fast.ai has made ULMFiT available as a part of its libraries, OpenAI has released GPT-2, And AllenNLP has released ELMo. Each of these models has been extensively trained and made available at no cost for use in your applications.</p>
<h3 id="our-prototype-textflix">Our Prototype: Textflix</h3>
<p>To test and demonstrate this training strategy, we built Textflix, a prototype of a social network for movie watchers. Texflix incorporates automatic sentiment analysis to help users understand which movies are popular and why.</p>
<p><img src="/images/editor_uploads/2019-09-06-215600-textflix_sample.png" alt=""></p>
<p>While the dataset we used to build the model behind Textflix had up to 50,000 labeled training examples available, we tested the system with varying-sized subsamples of the dataset down to as few as ten examples. We did this because we wanted to understand how different models, including transfer learning models, would perform with limited data. After all, in real-world problems there is often little or no labeled data available.</p>
<p>To explore how dataset size affects performance, we tested models fine tuned with subsamples of the examples in the dataset to get performance curves for a variety of models.</p>
<p><img src="/images/editor_uploads/2019-09-06-213658-figure_graph_3.png" alt=""></p>
<p>We compared state-of-the-art models BERT and ULMFiT with baseline models based on naive Bayes support vector machine (NB-SVM) and simple word vector summation. When trained on plenty of data (i.e., 10,000 examples) the transfer learning models greatly outperform the baseline models, reducing the error rate by a factor of 3 (from 15% to 5%). This is due to the fact that the transfer learning models have significantly higher capacity - they are large deep learning models. Because they were pretrained on massive datasets, they do not overfit the relatively small amount of data here; these models were already skilled language processors to begin with. We expect them to outperform the baseline models.</p>
<p>But the more exciting part of the chart is what happens around 100 samples. At 100 examples the baseline models are useless - hardly better than random guessing. The transfer learning models, however, still provide about 88% accuracy which is better than the baseline models can do even at 10,000 samples.</p>
<p>This isn’t just a better model, it’s a new capability: a human can generate enough samples in a short time to get reasonable performance in an NLP system. 100 data samples is manageable for a human to label in a single workday (albeit a tedious one). This lifts the constraint of having labeled data; if no labels are available, simply have a human spend a day generating labels and use transfer learning for the rest.</p>
<p>We built our prototype Textflix using a BERT model trained on just 500 labeled examples. This more closely mimics real-world scenarios where data is extremely limited. The model behind Textflix still provides near state-of-the-art performance despite this constraint.</p>
<h3 id="transfer-learning-in-nlp-is-advancing-quickly">Transfer Learning in NLP Is Advancing Quickly</h3>
<p>Transfer learning in natural language processing is one of the hottest topics in research today, and we’re still exploring its limits. New, more powerful models trained on ever-larger datasets are published frequently, and the state-of-the-art is being pushed higher each time. The improvements to models by simply training them on more and more data is expected to reach a ceiling at some point, but it’s not clear yet where that ceiling is. While this research is technically interesting, end users need not get bogged down by the details. Because these models all obey the common abstractions discussed above, you can benefit by simply downloading the latest models and plugging them in to existing systems today.</p>

    <div class="spacer"></div>
    <div>
      <div style="width: 100%; height: 2px; background: #ccc; margin-top: -1px; margin-bottom: -1px;"></div>
    </div>
    <div class="spacer"></div>
  </div>
</div>


<div class="container">
  <div class="spacer"></div>
  <h2 class="clear">Read more</h2>
  <div class="spacer"></div>
  <div style="display: grid; grid-template-columns: 1fr 1fr; grid-column-gap: 2ch;">
    <div>
      
      <div class="small">Newer</div>
      <div>
  <h5 style="margin-bottom: 0px;">
    <span>Sep 27, 2019</span> &middot;
    <span style="text-transform: capitalize;">
      newsletter
    </span>
  </h5>
  
    <div><a href="/2019/09/27/automating-weak-supervision.html"><strong>Automating Weak Supervision</strong></a></div>
  
  <div class="spacer"></div>
</div>


      
    </div>
    <div>
      
      <div class="small">Older</div>
      <div>
  <h5 style="margin-bottom: 0px;">
    <span>Aug 28, 2019</span> &middot;
    <span style="text-transform: capitalize;">
      newsletter
    </span>
  </h5>
  
    <div><a href="/2019/08/28/two-approaches-for-data-validation-in-ml-production.html"><strong>Two approaches for data validation in ML production</strong></a></div>
  
  <div class="spacer"></div>
</div>


      
    </div>
  </div>
</div>

<div class="container">
<div class="spacer"></div>
<div>
  <div style="width: 100%; height: 2px; background: #ccc; margin-top: -1px; margin-bottom: -1px;"></div>
</div>
<div class="spacer"></div>
<div class="spacer"></div>
</div>

<div class="container">
  

<h2 class="clear">Latest posts</h2>
<div class="spacer"></div>

<div id="posts-holder"> 
  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Nov 15, 2022</span> &middot;
    <span style="text-transform: capitalize;">
      newsletter
    </span>
  </h5>
  
  <a href="/newsletters/2022-11.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/horse_zebra_combo-1668558267.png" />
  </a>
  
  <div>
    
    <a href="/newsletters/2022-11.html"
       ><h2 style="margin-bottom: 4px;">CFFL November Newsletter</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    November 2022 Perhaps November conjures thoughts of holiday feasts and festivities, but for us, it’s the perfect time to chew the fat about machine learning! Make room on your plate for a peek behind the scenes into our current research on harnessing synthetic image generation to improve classification tasks. And, as usual, we reflect on our favorite reads of the month.
 New Research! In the first half of this year, we focused on natural language processing with our Text Style Transfer blog series.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/newsletters/2022-11.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Nov 14, 2022</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
  <a href="/2022/11/14/implementing-cyclegan.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/Screen_Shot_2022-10-18_at_3.06.46_PM-1668023835.png" />
  </a>
  
  <div>
    
    <a href="/2022/11/14/implementing-cyclegan.html"
       ><h2 style="margin-bottom: 4px;">Implementing CycleGAN</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="mailto:mgallaspy@cloudera.com">Michael Gallaspy</a>
        &middot; </span
      >
    </span>
    Introduction This post documents the first part of a research effort to quantify the impact of synthetic data augmentation in training a deep learning model for detecting manufacturing defects on steel surfaces. We chose to generate synthetic data using CycleGAN,1 an architecture involving several networks that jointly learn a mapping between two image domains from unpaired examples (I’ll elaborate below). Research from recent years has demonstrated improvement on tasks like defect detection2 and image segmentation3 by augmenting real image data sets with synthetic data, since deep learning algorithms require massive amounts of data, and data collection can easily become a bottleneck.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/2022/11/14/implementing-cyclegan.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Oct 20, 2022</span> &middot;
    <span style="text-transform: capitalize;">
      newsletter
    </span>
  </h5>
  
  <a href="/newsletters/2022-10.html" class="preview-image-holder">
    <img class="preview-image" src="https://blog.fastforwardlabs.com/images/hugo/datastream-1666304289.svg" />
  </a>
  
  <div>
    
    <a href="/newsletters/2022-10.html"
       ><h2 style="margin-bottom: 4px;">CFFL October Newsletter</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    October 2022 We’ve got another action-packed newsletter for October! Highlights this month include the re-release of a classic CFFL research report, an example-heavy tutorial on Dask for distributed ML, and our picks for the best reads of the month.
 Open Data Science Conference Cloudera Fast Forward Labs will be at ODSC West near San Fransisco on November 1st-3rd, 2022! If you’ll be in the Bay Area, don’t miss Andrew and Melanie who will be presenting our recent research on Neutralizing Subjectivity Bias with HuggingFace Transformers.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/newsletters/2022-10.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Sep 21, 2022</span> &middot;
    <span style="text-transform: capitalize;">
      newsletter
    </span>
  </h5>
  
  <a href="/newsletters/2022-09.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/tst_bart_cover-1663888335.png" />
  </a>
  
  <div>
    
    <a href="/newsletters/2022-09.html"
       ><h2 style="margin-bottom: 4px;">CFFL September Newsletter</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    September 2022 Welcome to the September edition of the Cloudera Fast Forward Labs newsletter. This month we’re talking about ethics and we have all kinds of goodies to share including the final installment of our Text Style Transfer series and a couple of offerings from our newest research engineer. Throw in some choice must-reads and an ASR demo, and you’ve got yourself an action-packed newsletter!
 New Research! Ethical Considerations When Designing an NLG System In the final post of our blog series on Text Style Transfer, we discuss some ethical considerations when working with natural language generation systems, and describe the design of our prototype application: Exploring Intelligent Writing Assistance.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/newsletters/2022-09.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Sep 8, 2022</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
  <a href="/2022/09/08/thought-experiment-human-centric-machine-learning-for-comic-book-creation.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/making_comic_books_diagram-1661792770.jpg" />
  </a>
  
  <div>
    
    <a href="/2022/09/08/thought-experiment-human-centric-machine-learning-for-comic-book-creation.html"
       ><h2 style="margin-bottom: 4px;">Thought experiment: Human-centric machine learning for comic book creation</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="mailto:mgallaspy@cloudera.com">Michael Gallaspy</a>
        &middot; </span
      >
    </span>
    This post has a companion piece: Ethics Sheet for AI-assisted Comic Book Art Generation  I want to make a comic book. Actually, I want to make tools for making comic books. See, the problem is, I can’t draw too good. I mean, I’m working on it. Check out these self portraits drawn 6 months apart:
Left: “Sad Face”. February 2022. Right: “Eyyyy”. August 2022. But I have a long way to go until my illustrations would be considered professional quality, notwithstanding the time it would take me to develop the many other skills needed for making comic books.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/2022/09/08/thought-experiment-human-centric-machine-learning-for-comic-book-creation.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Aug 18, 2022</span> &middot;
    <span style="text-transform: capitalize;">
      newsletter
    </span>
  </h5>
  
  <a href="/newsletters/2022-08.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/self_portrait-1660863172.jpeg" />
  </a>
  
  <div>
    
    <a href="/newsletters/2022-08.html"
       ><h2 style="margin-bottom: 4px;">CFFL August Newsletter</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    August 2022 Welcome to the August edition of the Cloudera Fast Forward Labs newsletter. This month we’re thrilled to introduce a new member of the FFL team, share TWO new applied machine learning prototypes we’ve built, and, as always, offer up some intriguing reads.
 New Research Engineer! If you’re a regular reader of our newsletter, you likely noticed that we’ve been searching for new research engineers to join the Cloudera Fast Forward Labs team.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/newsletters/2022-08.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
</div>

<div>
  <button id="load_more" style="width: 100%;">load more</button>
</div>
<div class="spacer"></div>
<div class="spacer"></div>

<script>
  window.addEventListener('load', () => {
    let $posts_holder = document.getElementById('posts-holder')
    let $load_more = document.getElementById('load_more')
    let next_page = 2
    $load_more.addEventListener('click', () => {
      fetch(`/newsletters/page/${next_page}.html`).then(r =>r.text()).then(r => {
        let el = document.createElement('html')
        el.innerHTML = r
        next_page += 1
        let $posts = el.querySelector('#posts-holder').children
        for (let i=0; i< $posts.length; i++) {
          let $post = $posts[i].cloneNode(true)
          $posts_holder.appendChild($post)
        }
      })
    })
  })
</script>


  <h3 class="clear">Popular posts</h3>
<div class="spacer"></div>
<div>
  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Oct 30, 2019</span> &middot;
    <span style="text-transform: capitalize;">
      newsletter
    </span>
  </h5>
  
    <div><a href="/2019/10/30/exciting-applications-of-graph-neural-networks.html"><strong>Exciting Applications of Graph Neural Networks</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Nov 14, 2018</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2018/11/14/federated-learning-distributed-machine-learning-with-data-locality-and-privacy.html"><strong>Federated learning: distributed machine learning with data locality and privacy</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Apr 10, 2018</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2018/04/10/pytorch-for-recommenders-101.html"><strong>PyTorch for Recommenders 101</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Oct 4, 2017</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2017/10/04/first-look-using-three.js-for-2d-data-visualization.html"><strong>First Look: Using Three.js for 2D Data Visualization</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Aug 22, 2016</span> &middot;
    <span style="text-transform: capitalize;">
      whitepaper
    </span>
  </h5>
  
    <div><a href="/2016/08/22/under-the-hood-of-the-variational-autoencoder-in-prose-and-code.html"><strong>Under the Hood of the Variational Autoencoder (in Prose and Code)</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Feb 24, 2016</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2016/02/24/hello-world-in-keras-or-scikit-learn-versus-keras.html"><strong>&#34;Hello world&#34; in Keras (or, Scikit-learn versus Keras)</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
</div>

</div>

<div class="spacer"></div>
<div style="background: #efefef;">
  <div class="spacer"></div>
  <div class="spacer"></div>
  <div class="container">
  <h1 class="clear">Reports</h1>
  <div style="color: #444;">In-depth guides to specific machine learning capabilities</div>
</div>
<div class="spacer"></div>
<div id="reports-holder" style="max-width: 96ch; margin: 0 auto; padding-left: 1ch; padding-right: 1ch;">
  
  <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF24</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://text-style-transfer.fastforwardlabs.com/" target="_blank">Text Style Transfer</a></h2>
  <a class="report-image" href="https://text-style-transfer.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff24-combo.png" />
  </a>
  <div class="small">The NLP task of text style transfer (TST) aims to automatically control the style attributes of a piece of text while preserving the content, which is an important consideration for making NLP more user-centric. In this report, we explore text style transfer through an applied use case — neutralizing subjectivity bias in free text. Along the way, we describe our sequence-to-sequence modeling approach leveraging HuggingFace Transformers, and present a set of custom, reference-free evaluation metrics for quantifying model performance. Finally, we conclude with a discussion of ethics centered around our prototype: Exploring Intelligent Writing Assistance.</div>
  <div><a href="https://text-style-transfer.fastforwardlabs.com/" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
  <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF22</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://concept-drift.fastforwardlabs.com/" target="_blank">Inferring Concept Drift Without Labeled Data</a></h2>
  <a class="report-image" href="https://concept-drift.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff22-combo.png" />
  </a>
  <div class="small">Concept drift occurs when the statistical properties of a target domain change overtime causing model performance to degrade. Drift detection is generally achieved by monitoring a performance metric of interest and triggering a retraining pipeline when that metric falls below some designated threshold. However, this approach assumes ample labeled data is available at prediction time - an unrealistic constraint for many production systems. In this report, we explore various approaches for dealing with concept drift when labeled data is not readily accessible.</div>
  <div><a href="https://concept-drift.fastforwardlabs.com/" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
  <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF19</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://session-based-recommenders.fastforwardlabs.com/" target="_blank">Session-based Recommender Systems</a></h2>
  <a class="report-image" href="https://session-based-recommenders.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff19-combo.png" />
  </a>
  <div class="small">Being able to recommend an item of interest to a user (based on their past preferences) is a highly relevant problem in practice. A key trend over the past few years has been session-based recommendation algorithms that provide recommendations solely based on a user’s interactions in an ongoing session, and which do not require the existence of user profiles or their entire historical preferences. This report explores a simple, yet powerful, NLP-based approach (word2vec) to recommend a next item to a user. While NLP-based approaches are generally employed for linguistic tasks, here we exploit them to learn the structure induced by a user’s behavior or an item’s nature.</div>
  <div><a href="https://session-based-recommenders.fastforwardlabs.com/" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
  <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF18</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://few-shot-text-classification.fastforwardlabs.com/" target="_blank">Few-Shot Text Classification</a></h2>
  <a class="report-image" href="https://few-shot-text-classification.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff18-combo.png" />
  </a>
  <div class="small">Text classification can be used for sentiment analysis, topic assignment, document identification, article recommendation, and more. While dozens of techniques now exist for this fundamental task, many of them require massive amounts of labeled data in order to be useful. Collecting annotations for your use case is typically one of the most costly parts of any machine learning application. In this report, we explore how latent text embeddings can be used with few (or even zero) training examples and provide insights into best practices for implementing this method.</div>
  <div><a href="https://few-shot-text-classification.fastforwardlabs.com/" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
</div>
<div class="container">
  <div ><button id="load_all_reports" style="width: 100%;">load all</button></div>
</div>

<script>
  
  window.addEventListener('load', () => {
    let $reports_holder = document.getElementById('reports-holder')
    let $load_more = document.getElementById('load_all_reports')
    $load_more.addEventListener('click', () => {
      fetch(`/reports.html`).then(r =>r.text()).then(r => {
        $load_more.remove()
        let el = document.createElement('html')
        el.innerHTML = r
        let $posts = el.querySelector('#reports-holder')
        $reports_holder.innerHTML = $posts.innerHTML
      })
    })
  })
</script>
</div>

<div class="spacer"></div>
<div class="spacer"></div>
 
<div class="container">
  <h1 class="clear">Prototypes</h1>
  <div style="color: #444;">Machine learning prototypes and interactive notebooks</div>
  <div class="spacer"></div>
</div>
<div id="prototypes-holder">
  
  <div style="display: flex; flex-direction: column; position: relative; padding-left: 1ch; padding-right: 1ch; height: 360px;">
  <div>
    <h5 style="margin-bottom: 0">Notebook</h5>
    <h2 style="padding-top: 0; margin-bottom: 0;"><a href="https://colab.research.google.com/github/fastforwardlabs/whisper-openai/blob/master/WhisperDemo.ipynb" target="_blank">ASR with Whisper</a></h2>
  </div>
  <div style="flex: 1 1 auto; position: relative;">
    <a href="https://colab.research.google.com/github/fastforwardlabs/whisper-openai/blob/master/WhisperDemo.ipynb" target="_blank" style="display: block; position: absolute; top: 0.375rem; width: 100%; height: calc(100% - 0.875rem); background-image: url('/images/hugo/whisper_screenshot-1666368542.png'); background-size: contain; background-position: center left; background-repeat: no-repeat;"></a>
  </div>
  <div>
    <div class="small">Explore the capabilities of OpenAI&#39;s Whisper for automatic speech recognition by creating your own voice recordings!</div>
    <div style="width: 100%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"><a href="https://colab.research.google.com/github/fastforwardlabs/whisper-openai/blob/master/WhisperDemo.ipynb" target="_blank">https://colab.research.google.com/github/fastforwardlabs/whisper-openai/blob/master/WhisperDemo.ipynb</a></div>
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
  <div style="display: flex; flex-direction: column; position: relative; padding-left: 1ch; padding-right: 1ch; height: 360px;">
  <div>
    <h5 style="margin-bottom: 0">Library</h5>
    <h2 style="padding-top: 0; margin-bottom: 0;"><a href="https://neuralqa.fastforwardlabs.com" target="_blank">NeuralQA</a></h2>
  </div>
  <div style="flex: 1 1 auto; position: relative;">
    <a href="https://neuralqa.fastforwardlabs.com" target="_blank" style="display: block; position: absolute; top: 0.375rem; width: 100%; height: calc(100% - 0.875rem); background-image: url('/images/hugo/neuralqa-1596123511.jpg'); background-size: contain; background-position: center left; background-repeat: no-repeat;"></a>
  </div>
  <div>
    <div class="small">A usable library for question answering on large datasets.</div>
    <div style="width: 100%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"><a href="https://neuralqa.fastforwardlabs.com" target="_blank">https://neuralqa.fastforwardlabs.com</a></div>
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
  <div style="display: flex; flex-direction: column; position: relative; padding-left: 1ch; padding-right: 1ch; height: 360px;">
  <div>
    <h5 style="margin-bottom: 0">Notebook</h5>
    <h2 style="padding-top: 0; margin-bottom: 0;"><a href="https://colab.research.google.com/drive/1tTiOgJ7xvy3sjfiFC9OozbjAX1ho8WN9?usp=sharing" target="_blank">Explain BERT for Question Answering Models</a></h2>
  </div>
  <div style="flex: 1 1 auto; position: relative;">
    <a href="https://colab.research.google.com/drive/1tTiOgJ7xvy3sjfiFC9OozbjAX1ho8WN9?usp=sharing" target="_blank" style="display: block; position: absolute; top: 0.375rem; width: 100%; height: calc(100% - 0.875rem); background-image: url('/images/hugo/distilexplanation-1592852137.jpg'); background-size: contain; background-position: center left; background-repeat: no-repeat;"></a>
  </div>
  <div>
    <div class="small">Tensorflow 2.0 notebook to explain and visualize a HuggingFace BERT for Question Answering model.</div>
    <div style="width: 100%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"><a href="https://colab.research.google.com/drive/1tTiOgJ7xvy3sjfiFC9OozbjAX1ho8WN9?usp=sharing" target="_blank">https://colab.research.google.com/drive/1tTiOgJ7xvy3sjfiFC9OozbjAX1ho8WN9?usp=sharing</a></div>
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
  <div style="display: flex; flex-direction: column; position: relative; padding-left: 1ch; padding-right: 1ch; height: 360px;">
  <div>
    <h5 style="margin-bottom: 0">Notebooks</h5>
    <h2 style="padding-top: 0; margin-bottom: 0;"><a href="https://qa.fastforwardlabs.com" target="_blank">NLP for Question Answering</a></h2>
  </div>
  <div style="flex: 1 1 auto; position: relative;">
    <a href="https://qa.fastforwardlabs.com" target="_blank" style="display: block; position: absolute; top: 0.375rem; width: 100%; height: calc(100% - 0.875rem); background-image: url('/images/uploads/qa.png'); background-size: contain; background-position: center left; background-repeat: no-repeat;"></a>
  </div>
  <div>
    <div class="small">Ongoing posts and code documenting the process of building a question answering model.</div>
    <div style="width: 100%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"><a href="https://qa.fastforwardlabs.com" target="_blank">https://qa.fastforwardlabs.com</a></div>
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
</div>
<div class="container">
  <div ><button id="load_all_prototypes" style="width: 100%;">load all</button></div>
</div>

<script>
  
  window.addEventListener('load', () => {
    let $prototypes_holder = document.getElementById('prototypes-holder')
    let $load_more = document.getElementById('load_all_prototypes')
    $load_more.addEventListener('click', () => {
      fetch(`/prototypes.html`).then(r =>r.text()).then(r => {
        $load_more.remove()
        let el = document.createElement('html')
        el.innerHTML = r
        let $posts = el.querySelector('#prototypes-holder')
        $prototypes_holder.innerHTML = $posts.innerHTML
      })
    })
  })
</script>



<div class="spacer"></div>
<div class="spacer"></div>

<div class="container">
  <div>
    <h1 class="clear">Cloudera Fast Forward Labs</h1>
    <div>
      <i>Making the recently possible useful.</i><br />
      <p></p>
      <p>Cloudera Fast Forward Labs is an applied machine learning research group. Our mission is to empower enterprise data science practitioners to apply emergent academic research to production machine learning use cases in practical and socially responsible ways, while also driving innovation through the Cloudera ecosystem.  Our team brings thoughtful, creative, and diverse perspectives to deeply researched work. In this way, we strive to help organizations make the most of their ML investment as well as educate and inspire the broader machine learning and data science community.</p>
      <a
        href="https://www.cloudera.com/products/fast-forward-labs-research.html"
        >Cloudera</a
      >&nbsp;&nbsp;
      <a
        href="https://blog.fastforwardlabs.com"
        >Blog</a
      >&nbsp;&nbsp;
      <a href="https://twitter.com/fastforwardlabs">Twitter</a>
      <p></p>
      <p>©2022 Cloudera, Inc. All rights reserved.</p>
    </div>
  </div>
</div>



<div class="spacer"></div>
<div class="spacer"></div>


      </main>
 </body>
</html>
