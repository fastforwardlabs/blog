<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deep learning on Blog</title>
    <link>https://blog.fastforwardlabs.com/tags/deep-learning.html</link>
    <description>Recent content in deep learning on Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 26 Aug 2016 17:43:24 +0000</lastBuildDate>
    
    <atom:link href="https://blog.fastforwardlabs.com/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Exploring Deep Learning on Satellite Data</title>
       
      <link>https://blog.fastforwardlabs.com/2016/08/26/exploring-deep-learning-on-satellite-data.html</link>
      
      <pubDate>Fri, 26 Aug 2016 17:43:24 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/08/26/exploring-deep-learning-on-satellite-data.html</guid>
      <description>This is a guest post featuring a project Patrick Doupe, now a Senior Data Analyst at Icahn School of Medicine at Mount Sinai, completed as a fellow in the Insight Data Science program. In our partnership with Insight, we occassionally advise fellows on month-long projects and how to build a career in data science. Machines are getting better at identifying objects in images. These technologies are used to do more than organise your photos or chat your family and friends with snappy augmented pictures and movies.</description>
    </item>
    
    <item>
      <title>New TensorFlow Code for Text Summarization</title>
       
      <link>https://blog.fastforwardlabs.com/2016/08/25/new-tensorflow-code-for-text-summarization.html</link>
      
      <pubDate>Thu, 25 Aug 2016 17:24:14 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/08/25/new-tensorflow-code-for-text-summarization.html</guid>
      <description>Yesterday, Google released new TensorFlow model code for text summarization, specifically for generating news headlines on the Annotated English Gigaword dataset. We’re excited to see others working on summarization, as we did in our last report: our ability to “digest large amounts of information in a compressed form” will only become more important as unstructured information grows. The TensorFlow release uses sequence-to-sequence learning to train models that write headlines for news articles.</description>
    </item>
    
    <item>
      <title>Next Economics: Interview with Jimi Crawford</title>
       
      <link>https://blog.fastforwardlabs.com/2016/08/24/next-economics-interview-with-jimi-crawford.html</link>
      
      <pubDate>Wed, 24 Aug 2016 16:18:12 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/08/24/next-economics-interview-with-jimi-crawford.html</guid>
      <description>&lt;figure class=&#34;tmblr-full&#34; data-orig-height=&#34;656&#34; data-orig-width=&#34;1629&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/de9fc93bc4bda585c1bb51a785fc7801/tumblr_inline_ocf6npUoHK1ta78fg_540.png&#34; data-orig-height=&#34;656&#34; data-orig-width=&#34;1629&#34;/&gt;&lt;/figure&gt;&lt;figure class=&#34;tmblr-full&#34; data-orig-height=&#34;923&#34; data-orig-width=&#34;2291&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/e65b096c51449dd8ad7ef4b2f0e4ad10/tumblr_inline_ocf6k5SHv61ta78fg_540.png&#34; data-orig-height=&#34;923&#34; data-orig-width=&#34;2291&#34;/&gt;&lt;/figure&gt;
&lt;h5 id=&#34;building-shadows-as-proxies-for-construction-rates-in-shanghai-photos-courtesy-of-orbital-insightdigital-globe&#34;&gt;Building shadows as proxies for construction rates in Shanghai. Photos courtesy of Orbital Insight/Digital Globe.&lt;/h5&gt;
&lt;p&gt;It’s no small feat to commercialize new technologies that arise from scientific and academic research. The useful is a small subset of the possible, and the features technology users (let alone corporate buyers) care about rarely align with the problems researchers want to solve. But it’s immensely exciting when it works. When the phase transition is complete. When the general public starts to appreciate how a bunch of mathematics can impact their business, their lives, and their understanding of how the world works. It’s why the Fast Forward Labs team wakes up every day. It’s why we love what we do. It drives us. And it’s why we’re always on the lookout for people who are doing it well. &lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://orbitalinsight.com/&#34;&gt;Orbital Insight&lt;/a&gt; is an excellent example of a company that is successfully commercializing deep learning technologies. 2015 saw a &lt;a href=&#34;http://www.nytimes.com/2015/12/11/science/an-advance-in-artificial-intelligence-rivals-human-vision-abilities.html?_r=0&#34;&gt;series of improvements&lt;/a&gt; in the performance of object recognition and computer vision systems. The technology is being applied across domains, to &lt;a href=&#34;http://www.enlitic.com/&#34;&gt;improve medical diagnosis&lt;/a&gt;, gain &lt;a href=&#34;https://www.clarifai.com/&#34;&gt;brand insights&lt;/a&gt;, or update our &lt;a href=&#34;http://pictograph.us&#34;&gt;social media experience&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;Building on his experience at The Climate Corporation, Orbital Insight CEO  &amp;amp; Founder &lt;a href=&#34;https://www.linkedin.com/in/jmcrawfordjr&#34;&gt;Jimi Crawford&lt;/a&gt; decided to aim big and apply the latest in computer vision to satellite imagery. His team focused their first commercial offering on the financial services industry, honing their tools to count cars in parking lots to infer company performance and, transitively, stock market behavior. But hedge funds are just the beginning. Crawford’s long-term ambition (as that of &lt;a href=&#34;http://www.featurex.ai/&#34;&gt;FeatureX&lt;/a&gt;) is to reform macroeconomics, to replace government reports with quantified observations about the physical world. &lt;a href=&#34;https://techcrunch.com/2016/06/27/orbital-insight-lands-20-million-from-investors-led-by-gv/&#34;&gt;Investors have taken notice&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;We interviewed Jimi, discussing what he learned in the past, what he does in the present, and what he envisions for the future. Read on for highlights. &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Under the Hood of the Variational Autoencoder (in Prose and Code)</title>
       
      <link>https://blog.fastforwardlabs.com/2016/08/22/under-the-hood-of-the-variational-autoencoder-in-prose-and-code.html</link>
      
      <pubDate>Mon, 22 Aug 2016 18:02:08 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/08/22/under-the-hood-of-the-variational-autoencoder-in-prose-and-code.html</guid>
      <description>&lt;h5 id=&#34;the-a-hrefhttpsarxivorgabs13126114variationala-a-hrefhttpsarxivorgabs14014082autoencodera-vae-neatly-synthesizes-unsupervised-deep-learning-and-variational-bayesian-methods-into-one-sleek-package-in-a-hrefhttpblogfastforwardlabscom20160812introducing-variational-autoencoders-in-prose-andhtmlpart-ia-of-this-series-we-introduced-the-theory-and-intuition-behind-the-vae-an-exciting-development-in-machine-learning-for-combined-generative-modeling-and-inferencea-hrefhttpshakirmcomslidesdlsummerschool_aug2016_compresspdfmachines-that-imagine-and-reasona&#34;&gt;The &lt;a href=&#34;https://arxiv.org/abs/1312.6114&#34;&gt;Variational&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/1401.4082&#34;&gt;Autoencoder&lt;/a&gt; (VAE) neatly synthesizes unsupervised deep learning and variational Bayesian methods into one sleek package. In &lt;a href=&#34;http://blog.fastforwardlabs.com/2016/08/12/introducing-variational-autoencoders-in-prose-and.html&#34;&gt;Part I&lt;/a&gt; of this series, we introduced the theory and intuition behind the VAE, an exciting development in machine learning for combined generative modeling and inference—&lt;a href=&#34;http://shakirm.com/slides/DLSummerSchool_Aug2016_compress.pdf&#34;&gt;“machines that imagine and reason.”&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;To recap: VAEs put a probabilistic spin on the basic autoencoder paradigm—treating their inputs, hidden representations, and reconstructed outputs as probabilistic random variables within a directed graphical model. With this &lt;a href=&#34;https://xkcd.com/1236/&#34;&gt;Bayesian&lt;/a&gt; perspective, the encoder becomes a variational &lt;em&gt;inference network&lt;/em&gt;, mapping observed inputs to (approximate) posterior distributions over latent space, and the decoder becomes a &lt;em&gt;generative network&lt;/em&gt;, capable of mapping arbitrary latent coordinates back to distributions over the original data space.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://fastforwardlabs.github.io/blog-images/miriam/imgs_code/vae.4.png&#34; title=&#34;A variational autoencoder&#34; style=&#34;width:80.0%&#34;/&gt;&lt;/div&gt;
&lt;p&gt;The beauty of this setup is that we can take a principled Bayesian approach toward building systems with a rich internal “mental model” of the observed world, all by training a single, cleverly-designed deep neural network.&lt;/p&gt;
&lt;p&gt;These benefits derive from an enriched understanding of data as merely the tip of the iceberg—the observed result of an underlying causative probabilistic process.&lt;/p&gt;
&lt;p&gt;The power of the resulting model is captured by Feynman’s famous &lt;a href=&#34;http://archives-dc.library.caltech.edu/islandora/object/ct1:483&#34;&gt;chalkboard quote&lt;/a&gt;: “What I cannot create, I do not understand.” When trained on MNIST handwritten digits, our VAE model can parse the information spread thinly over the high-dimensional observed world of pixels, and condense the most meaningful features into a structured distribution over reduced latent dimensions.&lt;/p&gt;
&lt;p&gt;Having recovered the latent manifold and assigned it a coordinate system, it becomes trivial to walk from one point to another along the manifold, creatively generating realistic digits all the while:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://fastforwardlabs.github.io/blog-images/miriam/imgs_code/160816_1754_reloaded_latent_784_500_500_50_round_65536_morph_4730816952.gif&#34; title=&#34;Generatively morphing digits&#34;/&gt;&lt;/div&gt;
&lt;p&gt;In this post, we’ll take a look under the hood at the math and technical details that allow us to optimize the VAE model we sketched in &lt;a href=&#34;http://fastforwardlabs.github.io/2016/08/12/introducing-variational-autoencoders-in-prose-and.html&#34;&gt;Part I&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Along the way, we’ll show how to implement a VAE in &lt;a href=&#34;http://tensorflow.org/&#34;&gt;TensorFlow&lt;/a&gt;—a library for efficient numerical computation using data flow graphs, with key features like &lt;a href=&#34;http://alexey.radul.name/ideas/2013/introduction-to-automatic-differentiation/&#34;&gt;automatic differentiation&lt;/a&gt; and parallelizability (across clusters, CPUs, GPUs…and &lt;a href=&#34;https://cloudplatform.googleblog.com/2016/05/Google-supercharges-machine-learning-tasks-with-custom-chip.html&#34;&gt;TPUs&lt;/a&gt; if you’re lucky). You can find (and tinker with!) the full implementation &lt;a href=&#34;https://github.com/fastforwardlabs/vae-tf/tree/master&#34;&gt;here&lt;/a&gt;, along with a couple &lt;a href=&#34;https://github.com/fastforwardlabs/vae-tf/tree/master/out&#34;&gt;pre-trained models&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Introducing Variational Autoencoders (in Prose and Code)</title>
       
      <link>https://blog.fastforwardlabs.com/2016/08/12/introducing-variational-autoencoders-in-prose-and-code.html</link>
      
      <pubDate>Fri, 12 Aug 2016 17:09:50 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/08/12/introducing-variational-autoencoders-in-prose-and-code.html</guid>
      <description>Effective machine learning means building expressive models that sift out signal from noise—that simplify the complexity of real-world data, yet accurately intuit and capture its subtle underlying patterns.
Whatever the downstream application, a primary challenge often boils down to this: How do we represent, or even synthesize, complex data in the context of a tractable model?
This challenge is compounded when working in a limited data setting—especially when samples are in the form of richly-structured, high-dimensional observations like natural images, audio waveforms, or gene expression data.</description>
    </item>
    
    <item>
      <title>Machine Listening: Interview with Juan Pablo Bello</title>
       
      <link>https://blog.fastforwardlabs.com/2016/06/10/machine-listening-interview-with-juan-pablo-bello.html</link>
      
      <pubDate>Fri, 10 Jun 2016 14:23:11 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/06/10/machine-listening-interview-with-juan-pablo-bello.html</guid>
      <description>&lt;figure data-orig-width=&#34;360&#34; data-orig-height=&#34;270&#34; class=&#34;tmblr-full&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/5cfcc6b830aa09f63bf9e9b2333524a1/tumblr_inline_o8k7rqJDmA1ta78fg_540.png&#34; alt=&#34;image&#34; data-orig-width=&#34;360&#34; data-orig-height=&#34;270&#34;/&gt;&lt;/figure&gt;
&lt;h5 id=&#34;a-probabilistic-latent-component-analysis-of-a-pitch-class-sequence-for-the-beatles-good-day-sunshine-the-top-layer-shows-the-original-representation-time-vs-pitch-class-subsequent-layers-show-latent-components&#34;&gt;A probabilistic latent component analysis of a pitch class sequence for The Beatles’ Good Day Sunshine. The top layer shows the original representation (time vs pitch class). Subsequent layers show latent components.&lt;/h5&gt;
&lt;p&gt;What is music? Or rather, what differentiates music from noise?&lt;b&gt;&lt;br/&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;If you ask John Cage, “everything we do is music.” Forced to &lt;a href=&#34;https://www.youtube.com/watch?v=JTEFKFiXSx4&#34;&gt;sit silently for 4’33”&lt;/a&gt;, we masters of &lt;a href=&#34;https://en.wikipedia.org/wiki/Apophenia&#34;&gt;apophenia&lt;/a&gt; end up hearing music in noise (or just squirm in discomfort&amp;hellip;), perceiving order and meaning in sounds that normally escape notice. For Cage, music is in the ears of the listener. To study it is to study how we perceive.&lt;/p&gt;&lt;p&gt;But Cage wrote 4’33” at time when many artists were challenging inherited notions of art. Others, dating back to Pythagoras (who defined harmony in terms of &lt;a href=&#34;https://en.wikipedia.org/wiki/Pythagorean_hammers#cite_note-8&#34;&gt;ratios and proportions&lt;/a&gt;), have defined music through the structural properties that make music &lt;i&gt;music, &lt;/i&gt;and separate different musical styles. &lt;/p&gt;&lt;p&gt;The latest efforts to understand music lie in the field of machine listening, where researchers use computers to analyze audio data to identify meaning and structure in it like humans do. Some machine listening researchers analyze urban and environmental sounds, as at &lt;a href=&#34;https://wp.nyu.edu/sonyc/&#34;&gt;SONYC&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;This August in NYC, researchers in machine listening and related fields will convene at the &lt;a href=&#34;https://wp.nyu.edu/ismir2016/&#34;&gt;International Society for Music Information Retrieval (ISMIR) conference&lt;/a&gt;. The conference is of interest to anyone working in data or digital media, offering practical workshops and hackathons for the NYC data community. &lt;/p&gt;&lt;p&gt;We interviewed NYU Steinhardt Professor &lt;a href=&#34;http://steinhardt.nyu.edu/marl/people/bello&#34;&gt;Juan Pablo Bello&lt;/a&gt;, an organizer of ISMIR 2016 working in machine listening, to learn more about the conference and the latest developments in the field. Keep reading for highlights!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>&#34;Hello world&#34; in Keras (or, Scikit-learn versus Keras)</title>
       
      <link>https://blog.fastforwardlabs.com/2016/02/24/hello-world-in-keras-or-scikit-learn-versus-keras.html</link>
      
      <pubDate>Wed, 24 Feb 2016 18:58:10 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/02/24/hello-world-in-keras-or-scikit-learn-versus-keras.html</guid>
      <description>This article is available as a notebook on Github. Please refer to that notebook for a more detailed discussion and code fixes and updates. Despite all the recent excitement around deep learning, neural networks have a reputation among non-specialists as complicated to build and difficult to interpret.
And while interpretability remains an issue, there are now high-level neural network libraries that enable developers to quickly build neural network models without worrying about the numerical details of floating point operations and linear algebra.</description>
    </item>
    
    <item>
      <title>NeuralTalk with Kyle McDonald</title>
       
      <link>https://blog.fastforwardlabs.com/2016/02/18/neuraltalk-with-kyle-mcdonald.html</link>
      
      <pubDate>Thu, 18 Feb 2016 15:09:51 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/02/18/neuraltalk-with-kyle-mcdonald.html</guid>
      <description>&lt;figure data-orig-width=&#34;640&#34; data-orig-height=&#34;427&#34; class=&#34;tmblr-full&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/806e99a04e9f647a198d0d887942562b/tumblr_inline_o2pgqgwhP01ta78fg_540.jpg&#34; alt=&#34;image&#34; data-orig-width=&#34;640&#34; data-orig-height=&#34;427&#34;/&gt;&lt;/figure&gt;
&lt;h5 id=&#34;image-from-a-hrefhttpsvimeocom90547410social-soula-an-immersive-experience-of-being-inside-a-social-media-stream-by-a-hrefhttplauren-mccarthycomlauren-mccarthya-and-kyle-mcdonald&#34;&gt;Image from &lt;a href=&#34;https://vimeo.com/90547410&#34;&gt;Social Soul&lt;/a&gt;, an immersive experience of being inside a social media stream, by &lt;a href=&#34;http://lauren-mccarthy.com/&#34;&gt;Lauren McCarthy&lt;/a&gt; and Kyle McDonald&lt;/h5&gt;
&lt;p&gt;A few weeks ago, &lt;a href=&#34;http://siliconangle.tv/&#34;&gt;theCUBE&lt;/a&gt; stopped by the Fast Forward Labs offices to &lt;a href=&#34;http://siliconangle.tv/innovation-day-fast-forward-labs/&#34;&gt;interview us &lt;/a&gt;about our approach to innovation. In the interview, we highlighted that &lt;a href=&#34;http://blog.fastforwardlabs.com/2016/02/16/machines-and-metaphors.html&#34;&gt;artists have an important role to play&lt;/a&gt; in shaping the future of machine intelligence. Unconstrained by market demands and product management requirements, artists are free to probe the potential of new technologies. And by optimizing for intuitive power or emotional resonance over theoretical accuracy or usability, they open channels to understand how machine intelligence is always, at its essence, a study of our own humanity.&lt;br/&gt;&lt;/p&gt;&lt;p&gt;One provocative artist exploring the creative potential of new machine learning tools is &lt;a href=&#34;http://kylemcdonald.net/&#34;&gt;Kyle McDonald&lt;/a&gt;. McDonald has seized the deep learning moment, undertaking projects that use neural networks to document a stroll down the Amsterdam canals, &lt;a href=&#34;https://medium.com/@kcimc/comparing-artificial-artists-7d889428fce4#.ltnl33b5p&#34;&gt;recreate images&lt;/a&gt; in the style of famous painters, or challenge our awareness of what we hold to be reality. &lt;/p&gt;&lt;p&gt;We interviewed Kyle to understand how he understands his work. Keep reading for highlights:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Machines and Metaphors</title>
       
      <link>https://blog.fastforwardlabs.com/2016/02/16/machines-and-metaphors.html</link>
      
      <pubDate>Tue, 16 Feb 2016 16:35:11 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/02/16/machines-and-metaphors.html</guid>
      <description>&lt;figure data-orig-width=&#34;703&#34; data-orig-height=&#34;376&#34; class=&#34;tmblr-full&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/2cf1b0924a1bfc66737d96f4797ef6f7/tumblr_inline_o2neravFOu1ta78fg_540.png&#34; alt=&#34;image&#34; data-orig-width=&#34;703&#34; data-orig-height=&#34;376&#34;/&gt;&lt;/figure&gt;
&lt;h5 id=&#34;this-is-a-guest-post-by-a-hrefhttpwwwgenekogancomgene-kogana-an-artist-and-programmer-who-applies-emerging-technology-into-artistic-and-expressive-contexts-and-teaches-courses-and-workshops-on-topics-related-to-code-and-art&#34;&gt;This is a guest post by &lt;a href=&#34;http://www.genekogan.com/&#34;&gt;Gene Kogan&lt;/a&gt;, an artist and programmer who applies emerging technology into artistic and expressive contexts, and teaches courses and workshops on topics related to code and art.&lt;/h5&gt;
&lt;p&gt;Recent advances in deep learning research have renewed popular interest in machine intelligence. With new benchmarks set in tough problems (e.g., image classification and speech recognition), researchers are exploring unexpected and exciting applications, and eliciting public engagement and private investment. These recent breakthroughs have captured the attention of many for whom AI was previously obscure, as new capabilities spur applications of interest to wider public audiences.&lt;br/&gt;&lt;/p&gt;&lt;p&gt;But these advances have captured more than just our attention; they&amp;rsquo;ve captured our  imagination. Artists have been quick to apply these new techniques for novel creations, exploring the uncharted territories of machine creativity, slyly provoking questions of greater importance. What is creativity anyway? How do machines perceive, learn, and imitate?&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Fashion Goes Deep: Data Science at Lyst</title>
       
      <link>https://blog.fastforwardlabs.com/2015/12/09/fashion-goes-deep-data-science-at-lyst.html</link>
      
      <pubDate>Wed, 09 Dec 2015 17:44:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/12/09/fashion-goes-deep-data-science-at-lyst.html</guid>
      <description>&lt;figure data-orig-width=&#34;694&#34; data-orig-height=&#34;376&#34; class=&#34;tmblr-full&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/993f6c53159f5715d9c127b790bb4bf5/tumblr_inline_nz3b9rPB5w1ta78fg_540.png&#34; alt=&#34;image&#34; data-orig-width=&#34;694&#34; data-orig-height=&#34;376&#34;/&gt;&lt;/figure&gt;&lt;p&gt;On November 16, 2015, Lyst, an online fashion marketplace based in the United Kingdom, &lt;a href=&#34;https://www.linkedin.com/pulse/lyst-ad-campaign-fashion-meets-data-science-flavia-young&#34;&gt;launched its first advertising campaign&lt;/a&gt;. Featuring a series of ironic headlines (one simply says “Rip-off”) etched over beautiful images, the campaign emphasizes the company’s identity as a “challenger brand,” whose success “has been driven by marrying insights from data science with the emotional nature of fashion.” (CEO Chris Morton) &lt;/p&gt;&lt;p&gt;&lt;a href=&#34;http://www.lyst.com&#34;&gt;Lyst&lt;/a&gt; provides fashion consumers with a central platform where they can mix and match millions of products from 11,500 different brands. In this context, data science serves as a virtual personal shopper, recommending products to users based upon insights from their behavior using the site. One might think these recommendations are powered by &lt;a href=&#34;https://en.wikipedia.org/wiki/Collaborative_filtering&#34;&gt;collaborative filtering&lt;/a&gt;, but the world of fashion is far too transient and fickle to support data models matching similar users. Matrices are sparse and inventory drains rapidly (consider flash sales sites like &lt;a href=&#34;http://www.gilt.com/&#34;&gt;Gilt&lt;/a&gt;). Instead, recommendation algorithms align consumer behavior with product features. And as the world of fashion is one dominated by image and appearance, fashion data science has a lot to do with image analysis. &lt;/p&gt;&lt;p&gt;We interviewed &lt;a href=&#34;https://twitter.com/ejlbell&#34;&gt;Eddie Bell&lt;/a&gt;, Lyst’s lead data scientist, about his team’s current efforts to use deep learning to analyze images and personalize recommendations to consumers. We talked about his past, his team’s present, and the fashion industry’s future. &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>When Dog Is Enough: Using Hypernyms To Improve Neural Network Predictions</title>
       
      <link>https://blog.fastforwardlabs.com/2015/11/17/when-dog-is-enough-using-hypernyms-to-improve-neural-network-predictions.html</link>
      
      <pubDate>Tue, 17 Nov 2015 16:35:14 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/11/17/when-dog-is-enough-using-hypernyms-to-improve-neural-network-predictions.html</guid>
      <description>&lt;p&gt;Possibly true statement: the Fast Forward Labs dog is the cutest dog in the world. &lt;br/&gt;&lt;/p&gt;&lt;figure data-orig-width=&#34;399&#34; data-orig-height=&#34;288&#34; class=&#34;tmblr-full&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/7df25fbff433a17f0a2d414666eff600/tumblr_inline_nxxgbxBEMd1ta78fg_540.png&#34; alt=&#34;image&#34; data-orig-width=&#34;399&#34; data-orig-height=&#34;288&#34;/&gt;&lt;/figure&gt;&lt;p&gt;Our General Counsel Ryan picked up the puppy a month ago and we’ve yet to name him. Ryan likes Renfield, which, as Bram Stoker fans know, evokes slightly different thoughts than “super cute,” particularly when &lt;a href=&#34;https://www.youtube.com/watch?v=WaVZmo8CsGQ&#34;&gt;played by&lt;/a&gt; the ever-guttural Tom Waits. But the fact that we’re in no rush to name him tells us something about how we label and identify things. We know he’s a dog, we love him for his dogness, and thus far that’s been just fine. I personally tend to forget what breed he is, as my knowledge of dog breeds is shamefully sparse. &lt;/p&gt;&lt;p&gt;Pictograph, in contrast, does an excellent job recognizing that our puppy is in fact a blenheim spaniel. Pictograph is the public app we built to illustrate how neural nets identify objects in images. &lt;a href=&#34;http://pictograph.us&#34;&gt;Try it&lt;/a&gt; on your personal Instagram feed!&lt;/p&gt;&lt;figure data-orig-width=&#34;658&#34; data-orig-height=&#34;216&#34; class=&#34;tmblr-full&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/36ad21fb7fbc61e892ca66af5f684fe4/tumblr_inline_nxxgkiSgKa1ta78fg_540.png&#34; alt=&#34;image&#34; data-orig-width=&#34;658&#34; data-orig-height=&#34;216&#34;/&gt;&lt;/figure&gt;&lt;p&gt;A 97% confidence rate in the accuracy of the prediction is a dream for automated classification. Here, the confidence is so high for two reasons. &lt;/p&gt;&lt;p&gt;First, the &lt;a href=&#34;http://www.image-net.org/&#34;&gt;ImageNet &lt;/a&gt;database used to train the Pictograph neural network has a lot of pictures of blenheim spaniels (971&amp;hellip;and yep, it’s prime). This labelled data informs the network what a correct classification &lt;i&gt;should&lt;/i&gt; look like. The learning mechanism (called &lt;a href=&#34;http://blog.fastforwardlabs.com/2015/09/24/how-do-neural-networks-learn.html&#34;&gt;backpropagation&lt;/a&gt;) then steps in and learns the network to predict the label “blenheim spaniel” when presented with new images that have similar features.&lt;br/&gt;&lt;/p&gt;&lt;p&gt;Second, the images in Ryan’s Instagram feed aren’t noisy. Note how the two images with 97% confidence rates show our puppy alone and facing the camera. This pose is similar to the stock images available on ImageNet, rendering it easier for the neural net to detect similarities. The confidence rate in the right-hand image including the human face drops to 62% because the data is noisier. It would likely drop further when presented an image of our puppy unconsciously playing &lt;a href=&#34;https://en.wikipedia.org/wiki/Ouroboros&#34;&gt;Ouroboros&lt;/a&gt; (the mythical snake that eats its own tail).&lt;/p&gt;&lt;figure data-orig-width=&#34;462&#34; data-orig-height=&#34;328&#34; class=&#34;tmblr-full&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/bac3509687c5af7c21fd2fc853bf0501/tumblr_inline_nxxhlkZ4zi1ta78fg_540.png&#34; alt=&#34;image&#34; data-orig-width=&#34;462&#34; data-orig-height=&#34;328&#34;/&gt;&lt;/figure&gt;&lt;p&gt;But Instagram, like most data in the wild, rarely contains clean data that maps neatly to a model’s parameters or the stock photos in a training set like ImageNet. In turn, classification systems can yield confidence rates as low as 20% or 30% (or lower), generating doubts as to whether it’s worth using the technology at all. One way to improve unsatisfying results from a machine learning tool is to adopt a “&lt;a href=&#34;https://medium.com/the-wtf-economy/artificial-intelligence-and-the-future-of-work-a0eaabea7c41&#34;&gt;human in the loop&lt;/a&gt;” approach, where humans step in and manually label images technology misclassifies or classifies with low confidence rates. But we decided to adopt a different approach.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Hello Deep Learning</title>
       
      <link>https://blog.fastforwardlabs.com/2015/10/26/hello-deep-learning.html</link>
      
      <pubDate>Mon, 26 Oct 2015 16:20:07 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/10/26/hello-deep-learning.html</guid>
      <description>Deep learning is a hot and fascinating research area, particularly when applied to classifying images. While researching the Fast Forward Labs Deep Learning: Image Analysis report, we played with a lot of very cool technology. In this blog post, we offer a guide to getting started with deep learning by using APIs from some of the most interesting deep-learning-as-a-service startups.
These APIs accept images and/or video, and quickly classify objects, ideas, and items shown in the images and video.</description>
    </item>
    
    <item>
      <title>How do neural networks learn?</title>
       
      <link>https://blog.fastforwardlabs.com/2015/09/24/how-do-neural-networks-learn.html</link>
      
      <pubDate>Thu, 24 Sep 2015 18:56:09 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/09/24/how-do-neural-networks-learn.html</guid>
      <description>Neural networks are generating a lot of excitement, as they are quickly proving to be a promising and practical form of machine intelligence. At Fast Forward Labs, we just finished a project researching and building systems that use neural networks for image analysis, as shown in our toy applicationPictograph. Our companion deep learning report explains this technology in depth and explores applications and opportunities across industries.
As we built Pictograph, we came to appreciate just how challenging it is to understand how neural networks work.</description>
    </item>
    
    <item>
      <title>Fast Forward Labs Interviews Clarifai about Deep Learning</title>
       
      <link>https://blog.fastforwardlabs.com/2015/09/22/fast-forward-labs-interviews-clarifai-about-deep-learning.html</link>
      
      <pubDate>Tue, 22 Sep 2015 18:58:45 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/09/22/fast-forward-labs-interviews-clarifai-about-deep-learning.html</guid>
      <description>Last Thursday Hilary and I headed to Clarifai’s offices in the Flatiron District to ask CEO Matt Zeiler about using deep learning for image analysis. A few highlights from the interview:
1) The success of a deep learning project depends on the quality of the initial training data set. Deep learning algorithms start by scanning massive data sets to identify features (inputs) that can be correlated with categories (outputs) to make sense of the data.</description>
    </item>
    
    <item>
      <title>Pictograph: Unlock Your Images</title>
       
      <link>https://blog.fastforwardlabs.com/2015/09/15/pictograph-unlock-your-images.html</link>
      
      <pubDate>Tue, 15 Sep 2015 19:16:19 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/09/15/pictograph-unlock-your-images.html</guid>
      <description>Have you ever wondered what your photos say about how you look at the world and who you are? Your images won’t say much about what types of things you tend to post unless you routinely tag them. Our new toy application, Pictograph, catalogs the objects that make up your Instagram identity. Pictograph analyzes your Instagram photos and creates a visualization, or pictograph, of what you like to photograph. It’s a fun way to play with new deep learning algorithms for image analysis, and it makes some pretty hilarious mistakes.</description>
    </item>
    
    <item>
      <title>D’Alembert’s Deep Dream: Bees and Nonlinear Transformations</title>
       
      <link>https://blog.fastforwardlabs.com/2015/09/02/dalemberts-deep-dream-bees-and-nonlinear-transformations.html</link>
      
      <pubDate>Wed, 02 Sep 2015 14:08:33 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/09/02/dalemberts-deep-dream-bees-and-nonlinear-transformations.html</guid>
      <description>&lt;figure data-orig-width=&#34;284&#34; data-orig-height=&#34;178&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/5e441cb1ef3adcd99999f7b29da5ee5e/tumblr_inline_nu1ujeo4cc1ta78fg_540.jpg&#34; alt=&#34;image&#34; data-orig-width=&#34;284&#34; data-orig-height=&#34;178&#34;/&gt;&lt;/figure&gt;&lt;p&gt;Hold your hats! In the next couple of weeks we’re launching an arsenal of deep learning resources, including a feature report, a public prototype that will classify your Instagram identity and a webinar exploring the past, present and future of deep learning. Sign up &lt;a href=&#34;https://deeplearningwebinar.splashthat.com/&#34;&gt;here&lt;/a&gt;!&lt;/p&gt;&lt;p&gt;As a philosophical prelude to the upcoming report, we wanted to invite you to think about the emergent properties of neural nets. Let’s explore what 18th-century philosopher Denis Diderot can teach us about artificial intelligence.&lt;/p&gt;&lt;p&gt;Diderot was not your average Enlightenment philosopher. A &lt;i&gt;philosophe, &lt;/i&gt;he grappled with the colossal inheritance of mechanical philosophy that dominated 18th-century intellectual circles. Spearheaded by Descartes and Newton, the mechanical view held that the material world was composed of complicated machines governed and determined by immutable laws. &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Why Now? Some Preconditions for Technology Innovations</title>
       
      <link>https://blog.fastforwardlabs.com/2015/08/14/why-now-some-preconditions-for-technology-innovations.html</link>
      
      <pubDate>Fri, 14 Aug 2015 18:57:31 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/08/14/why-now-some-preconditions-for-technology-innovations.html</guid>
      <description>We like to hold fast to the myth of the individual creative genius as the source of the world’s most impactful scientific revolutions or disruptive innovations. But it’s consoling to recall how Isaac Newton consoled his rival Robert Hooke: “If I’ve seen further than others, it was by standing on the shoulders of giants.”
 This is French how painter Nicolas Poussin represents Cedalion providing sight to the blind Orion, a mythological pair associated with each generation’s progress over its predecessors.</description>
    </item>
    
    <item>
      <title>On Stirling Engines and Orchids: A Prelude to Deep Learning</title>
       
      <link>https://blog.fastforwardlabs.com/2015/08/07/on-stirling-engines-and-orchids-a-prelude-to-deep-learning.html</link>
      
      <pubDate>Fri, 07 Aug 2015 19:08:04 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/08/07/on-stirling-engines-and-orchids-a-prelude-to-deep-learning.html</guid>
      <description>Today’s post is inspired by a slow-motion recording we captured of a Stirling engine that Ryan, Fast Forward’s General Counsel, just so happened to have lying around our New York City offices. For the non-mechanics among us, a Stirling engine is a heat engine that operates by cyclic compression and expansion of air and other gas at different temperatures; the temperature differential translates heat into mechanical work. The slowmo in the video renders a hypnotic locomotive sound that differs greatly from the mechanic buzz of realtime.</description>
    </item>
    
  </channel>
</rss>
