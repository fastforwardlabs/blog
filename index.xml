<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog</title>
    <link>https://blog.fastforwardlabs.com/</link>
    <description>Recent content on Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 25 Jun 2021 14:49:47 +0000</lastBuildDate>
    
    <atom:link href="https://blog.fastforwardlabs.com/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Deep Metric Learning for Signature Verification</title>
       
      <link>https://blog.fastforwardlabs.com/2021/06/09/deep-metric-learning-for-signature-verification.html</link>
      
      <pubDate>Wed, 09 Jun 2021 08:48:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2021/06/09/deep-metric-learning-for-signature-verification.html</guid>
      <description>By Victor and Andrew.
TLDR; This post provides an overview of metric learning loss functions (constrastive, triplet, quadruplet and group loss), and results from applying contrastive and triplet loss to the task of signature verification. Other posts in the series are listed below: Pretrained Models as Baselines for Signature Verification  --   Part 1: Deep Learning for Automatic Offline Signature Verification: An Introduction    Part 2: Pretrained Models as Baselines for Signature Verification    Part 3: Deep Metric Learning for Signature Verification      In our previous blog post, , we discussed how pretrained models can serve as strong baselines for the task of signature verification.</description>
    </item>
    
    <item>
      <title>Pre-trained Models as a Strong Baseline for Automatic Signature Verification</title>
       
      <link>https://blog.fastforwardlabs.com/2021/05/27/pre-trained-models-as-a-strong-baseline-for-automatic-signature-verification.html</link>
      
      <pubDate>Thu, 27 May 2021 07:35:06 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2021/05/27/pre-trained-models-as-a-strong-baseline-for-automatic-signature-verification.html</guid>
      <description>By Victor and Andrew.
Figure 1. Baseline approach for automatic signature verification using pre-trained models TLDR; This post describes how pretrained image classification models can be used as strong baselines for the task of signature verification. Other posts in the series are listed below: Pretrained Models as Baselines for Signature Verification  --   Part 1: Deep Learning for Automatic Offline Signature Verification: An Introduction    Part 2: Pretrained Models as Baselines for Signature Verification    Part 3: Deep Metric Learning for Signature Verification      As discussed in our introductory blog post, offline signature verification is a biometric verification task that aims to discriminate between genuine and forged samples of handwritten signatures.</description>
    </item>
    
    <item>
      <title>Deep Learning for Automatic Offline Signature Verification: An Introduction</title>
       
      <link>https://blog.fastforwardlabs.com/2021/05/26/deep-learning-for-automatic-offline-signature-verification-an-introduction.html</link>
      
      <pubDate>Wed, 26 May 2021 17:04:38 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2021/05/26/deep-learning-for-automatic-offline-signature-verification-an-introduction.html</guid>
      <description>By Victor and Andrew.
Figure 1. A summary of tasks that comprise the automatic signature verification pipeline (and related machine learning problems). TLDR; This post provides an overview of the signature verification task, use cases, and challenges. A complete list of the posts in this series is outlined below: Pretrained Models as Baselines for Signature Verification  --   Part 1: Deep Learning for Automatic Offline Signature Verification: An Introduction    Part 2: Pretrained Models as Baselines for Signature Verification    Part 3: Deep Metric Learning for Signature Verification      Given two signatures, automatic signature verification (ASV) seeks to determine if they are produced by the same user (genuine signatures) or different users (potential forgeries).</description>
    </item>
    
    <item>
      <title>Representation Learning 101 for Software Engineers</title>
       
      <link>https://blog.fastforwardlabs.com/2020/11/15/representation-learning-101-for-software-engineers.html</link>
      
      <pubDate>Sun, 15 Nov 2020 21:09:50 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2020/11/15/representation-learning-101-for-software-engineers.html</guid>
      <description>Figure 1: Overview of representation learning methods. TLDR; Good representations of data (e.g., text, images) are critical for solving many tasks (e.g., search or recommendations). Deep representation learning yields state of the art results when used to create these representations. In this article, we review methods for representation learning and walk through an example using pretrained models.  Introduction Deep Neural Networks (DNNs) have become a particularly useful tool in building intelligent systems that simplify cognitive tasks for users.</description>
    </item>
    
    <item>
      <title>How to Explain HuggingFace BERT for Question Answering NLP Models with TF 2.0</title>
       
      <link>https://blog.fastforwardlabs.com/2020/06/22/how-to-explain-huggingface-bert-for-question-answering-nlp-models-with-tf-2.0.html</link>
      
      <pubDate>Mon, 22 Jun 2020 11:24:19 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2020/06/22/how-to-explain-huggingface-bert-for-question-answering-nlp-models-with-tf-2.0.html</guid>
      <description>Given a question and a passage, the task of Question Answering (QA) focuses on identifying the exact span within the passage that answers the question.
Figure 1: In this sample, a BERTbase model gets the answer correct (Achaemenid Persia). Model gradients show that the token &amp;ldquo;subordinate ..&amp;rdquo; is impactful in the selection of an answer to the question &amp;ldquo;Macedonia was under the rule of which country?&amp;quot;. This makes sense .. good for BERTbase.</description>
    </item>
    
    <item>
      <title>Evaluating QA: Metrics, Predictions, and the Null Response</title>
      
      <link>https://qa.fastforwardlabs.com/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html</link>
      
      <pubDate>Tue, 16 Jun 2020 09:29:59 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2020/06/16/evaluating-qa-metrics-predictions-and-the-null-response.html</guid>
      <description>A deep dive into computing QA predictions and when to tell BERT to zip it! In our last post, Building a QA System with BERT on Wikipedia, we used the HuggingFace framework to train BERT on the SQuAD2.0 dataset and built a simple QA system on top of the Wikipedia search engine. This time, we&amp;rsquo;ll look at how to assess the quality of a BERT-like model for Question Answering.</description>
    </item>
    
    <item>
      <title>Building a QA System with BERT on Wikipedia</title>
      
      <link>https://qa.fastforwardlabs.com/pytorch/hugging%20face/wikipedia/bert/transformers/2020/05/19/Getting_Started_with_QA.html</link>
      
      <pubDate>Tue, 19 May 2020 09:09:36 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2020/05/19/building-a-qa-system-with-bert-on-wikipedia.html</guid>
      <description>So you&amp;rsquo;ve decided to build a QA system. You want to start with something simple and general so you plan to make it open domain using Wikipedia as a corpus for answering questions. You want to use the best NLP that your compute resources allow (you&amp;rsquo;re lucky enough to have access to a GPU) so you&amp;rsquo;re going to focus on the big, flashy Transformer models that are all the rage these days.</description>
    </item>
    
    <item>
      <title>Intro to Automated Question Answering</title>
      
      <link>https://qa.fastforwardlabs.com/methods/background/2020/04/28/Intro-to-QA.html</link>
      
      <pubDate>Tue, 28 Apr 2020 15:14:47 -0400</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2020/04/28/intro-to-automated-question-answering.html</guid>
      <description>Welcome to the first edition of the Cloudera Fast Forward blog on Natural Language Processing for Question Answering! Throughout this series, we’ll build a Question Answering (QA) system with off-the-shelf algorithms and libraries and blog about our process and what we find along the way. We hope to wind up with a beginning-to-end documentary that provides:</description>
    </item>
    
    <item>
      <title>Bias in Knowledge Graphs - Part 1</title>
       
      <link>https://blog.fastforwardlabs.com/2020/04/01/bias-in-knowledge-graphs-part-1.html</link>
      
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2020/04/01/bias-in-knowledge-graphs-part-1.html</guid>
      <description>Introduction This is the first part of a series to review Bias in Knowledge Graphs (KG). We aim to describe methods of identifying bias, measuring its impact, and mitigating that impact. For this part, we’ll give a broad overview of this topic.
image credit: Mediamodifier from Pixabay Motivation Knowledge graphs, graphs with built-in ontologies, create unique opportunities for data analytics, machine learning, and data mining. They do this by enhancing data with the power of connections and human knowledge.</description>
    </item>
    
    <item>
      <title>Enterprise Grade ML</title>
       
      <link>https://blog.fastforwardlabs.com/2020/04/01/enterprise-grade-ml.html</link>
      
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2020/04/01/enterprise-grade-ml.html</guid>
      <description>At Cloudera Fast Forward, one of the mechanisms we use to tightly couple machine learning research with application is through application development projects for both internal and external clients. The problems we tackle in these projects are wide ranging and cut across various industries; the end goal is a production system that translates data into business impact.
What is Enterprise Grade Machine Learning? Enterprise grade ML, a term mentioned in a paper put forth by Microsoft, refers to ML applications where there is a high level of scrutiny for data handling, model fairness, user privacy, and debuggability.</description>
    </item>
    
    <item>
      <title>Privacy, data governance, and machine learning: the regulatory perspective</title>
       
      <link>https://blog.fastforwardlabs.com/2020/02/27/privacy-data-governance-and-machine-learning-the-regulatory-perspective.html</link>
      
      <pubDate>Thu, 27 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2020/02/27/privacy-data-governance-and-machine-learning-the-regulatory-perspective.html</guid>
      <description>Why do privacy and governance matter? Data privacy has been a common conversation topic among the general public since the Cambridge Analytica scandal in 2018. The data &amp;ldquo;breach,&amp;rdquo; in which user information was hoovered up through a Facebook quiz and subsequently misrepresented as being used for academic purposes, resulted in over $5 billion in fines for Facebook. However, Facebook&amp;rsquo;s infringements were, in fact, relatively narrow in scope (though nonetheless egregious) compared to the growing remit of privacy law.</description>
    </item>
    
    <item>
      <title>Building Blip: behind the scenes of our anomaly detection prototype</title>
       
      <link>https://blog.fastforwardlabs.com/2020/02/20/building-blip-behind-the-scenes-of-our-anomaly-detection-prototype.html</link>
      
      <pubDate>Thu, 20 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2020/02/20/building-blip-behind-the-scenes-of-our-anomaly-detection-prototype.html</guid>
      <description>Our anomaly detection prototype, Blip, shows how four different algorithms perform at detecting network attacks. Here is a look at some of the design, visualization, and front-end programming decisions that went into it. (For more on the algorithms themselves, check out the prototype section of our report.)
Tomato sorting The concept of an anomaly is easy to visualize: it&amp;rsquo;s something that doesn&amp;rsquo;t look the same. The conceptual simplicity of it actually makes our prototype&amp;rsquo;s job trickier.</description>
    </item>
    
    <item>
      <title>Deep Learning for Anomaly Detection</title>
       
      <link>https://blog.fastforwardlabs.com/2020/02/05/deep-learning-for-anomaly-detection.html</link>
      
      <pubDate>Wed, 05 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2020/02/05/deep-learning-for-anomaly-detection.html</guid>
      <description>The full Deep Learning for Anomaly Detection report is now available.
You can also catch a replay of the webinar we reference below on demand here.
 In recent years, we have seen an unprecedented increase in the availability of data in a variety of domains: manufacturing, health care, finance, IT, and others. Applications leverage this data to make informed decisions. This comes with its own set of challenges (and opportunities) when things start to fail; for instance, what happens when a piece of equipment fails or a network suffers from a security vulnerability?</description>
    </item>
    
    <item>
      <title>A Symbiotic Relationship: Knowledge Graphs &amp; Machine Learning</title>
       
      <link>https://blog.fastforwardlabs.com/2020/01/29/a-symbiotic-relationship-knowledge-graphs-machine-learning.html</link>
      
      <pubDate>Wed, 29 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2020/01/29/a-symbiotic-relationship-knowledge-graphs-machine-learning.html</guid>
      <description>For the past decade, humans have unknowingly come to depend on Knowledge Graphs on a daily basis. From personalized shopping recommendations to intelligent assistants and user-friendly search results, many of these accepted (and expected) features have come to fruition through the exploitation of knowledge graphs. Despite their longstanding conceptual and practical existence, knowledge graphs were just added to the Gartner Hype Cycle for Emerging Technologies in 2018 and have continued to garner attention as an area of active research and development for their distinct ability to represent real-world relationships.</description>
    </item>
    
    <item>
      <title>Squote - semantic quote search</title>
       
      <link>https://blog.fastforwardlabs.com/2019/12/24/squote-semantic-quote-search.html</link>
      
      <pubDate>Tue, 24 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2019/12/24/squote-semantic-quote-search.html</guid>
      <description>Here in the (virtual) Fast Forward lab, we’re currently deep in the topic selection process for FF13. While one or two of our research engineers take the lead for each report, we work through several rounds of discussion and debate about the merits and demerits of topics on our collective radar as a team. Part of that process is dreaming up potential prototype applications.
Our multi-talented designer/dev Grant proposed a prototype: given a block of text, find a relevant quote.</description>
    </item>
    
    <item>
      <title>Privacy-Preserving Machine Learning: A Primer</title>
       
      <link>https://blog.fastforwardlabs.com/2019/11/21/privacy-preserving-machine-learning-a-primer.html</link>
      
      <pubDate>Thu, 21 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2019/11/21/privacy-preserving-machine-learning-a-primer.html</guid>
      <description>I have a bit of a cybersecurity background, so a year or two ago I started paying attention to how often data breaches happen - and I noticed something depressing: they happen literally 3+ times each day on average in the United States alone! (For a particularly sobering review check out this blog post.) All that information, your information, is out there, floating through the ether of the internet, along with much of my information: email addresses, old passwords, and phone numbers.</description>
    </item>
    
    <item>
      <title>Exciting Applications of Graph Neural Networks</title>
       
      <link>https://blog.fastforwardlabs.com/2019/10/30/exciting-applications-of-graph-neural-networks.html</link>
      
      <pubDate>Wed, 30 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2019/10/30/exciting-applications-of-graph-neural-networks.html</guid>
      <description>Graph Neural Networks (GNNs) are neural networks that take graphs as inputs. These models operate on the relational information in data to produce insights not possible in other neural network architectures and algorithms.
While there is much excitement in the deep learning community around GNNs, in industry circles, this is sometimes less so. So, I’ll review a few exciting applications empowered by GNNs.
Overview of Graphs and GNNs A graph (sometimes called a network) is a data structure that highlights the relationships between components in the data.</description>
    </item>
    
    <item>
      <title>Fuzzy People</title>
       
      <link>https://blog.fastforwardlabs.com/2019/10/30/fuzzy-people.html</link>
      
      <pubDate>Wed, 30 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2019/10/30/fuzzy-people.html</guid>
      <description>While preparing for a recent edition of the Federated Learning talk I often give at conferences, I encountered this tweet, which includes a demonstration of real-time multi-person segmentation on a smartphone.
Some useful terminology here:
 Object detection typically refers to identifying and localising objects (such as people!) in an image, and surrounding them with a bounding box. Object segmentation labels each pixel of the image with a class (for instance: &amp;ldquo;person,&amp;rdquo; &amp;ldquo;car,&amp;rdquo; &amp;ldquo;cat,&amp;rdquo; &amp;hellip;).</description>
    </item>
    
    <item>
      <title>Automating Weak Supervision</title>
       
      <link>https://blog.fastforwardlabs.com/2019/09/27/automating-weak-supervision.html</link>
      
      <pubDate>Fri, 27 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2019/09/27/automating-weak-supervision.html</guid>
      <description>What is weak supervision? We recently explored Snorkel, a weak supervision framework for learning when there are limited high-quality labels (see blog post and notebook). To use Snorkel, subject matter experts first write labeling functions to programmatically create labels. Very often these labeling functions attempt to capture heuristics. The labels are then fed into a generative model. The job of the generative model is to estimate the accuracy of the labeling functions while automatically taking into account the pairwise correlation between these functions and labeling propensity (how often a function actually creates a label).</description>
    </item>
    
    <item>
      <title>Other Orders: Re-contextualizing the Familiar</title>
       
      <link>https://blog.fastforwardlabs.com/2019/09/27/other-orders-re-contextualizing-the-familiar.html</link>
      
      <pubDate>Fri, 27 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2019/09/27/other-orders-re-contextualizing-the-familiar.html</guid>
      <description>My Twitter feed sorted by semantic similarity to &amp;ldquo;crudely understood Marxism&amp;rdquo; using Other Orders. To illustrate how categories shape our thoughts, the philosopher Michel Foucault cites Jorge Luis Borges&amp;rsquo; (made-up) quotation from a &amp;ldquo;certain Chinese encyclopedia,&amp;quot; where it is supposedly written that:
 Animals are divided into: (a) belonging to the Emperor, (b) embalmed, (c) tame, (d) suckling pigs, (e) sirens, (f) fabulous, (g) stray dogs, (h) included in the present classification, (i) frenzied, (j) innumerable, (k) drawn with a very fine camelhair brush, (l) et cetera, (m) having just broken the water pitcher, (n) that from a long way off look like flies.</description>
    </item>
    
    <item>
      <title>Theories of (machine) learning should shape horizon scanning - not just application</title>
       
      <link>https://blog.fastforwardlabs.com/2019/09/27/theories-of-machine-learning-should-shape-horizon-scanning-not-just-application.html</link>
      
      <pubDate>Fri, 27 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2019/09/27/theories-of-machine-learning-should-shape-horizon-scanning-not-just-application.html</guid>
      <description>And no, not this kind of horizon&amp;hellip; (image credit) In a recent newsletter, Alice mused about how evolving views and theories of learning are shaping machine learning research and practice. If you’re an enterprise data scientist you’re very much focused on the practice of machine learning. Limited awareness of what’s shaping the machine learning breakthroughs that you’re trying to apply to real-life problems makes it easy to get locked into certain approaches and to inherit blind spots.</description>
    </item>
    
    <item>
      <title>Transfer Learning - from the ground up</title>
       
      <link>https://blog.fastforwardlabs.com/2019/09/05/transfer-learning-from-the-ground-up.html</link>
      
      <pubDate>Thu, 05 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2019/09/05/transfer-learning-from-the-ground-up.html</guid>
      <description>Machine learning enables us to build systems that can predict the world around us: like what movies we’d like to watch, how much traffic we’ll experience on our morning commute, or what words we’ll type next in our emails.
There are many types of models and tasks. Face detection models transform raw image pixels into high level signals (like the presence and position of eyes, noses, and ears) and then use those signals to locate all faces in an image.</description>
    </item>
    
    <item>
      <title> Is machine learning research moving in the right direction</title>
       
      <link>https://blog.fastforwardlabs.com/2019/08/28/is-machine-learning-research-moving-in-the-right-direction.html</link>
      
      <pubDate>Wed, 28 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2019/08/28/is-machine-learning-research-moving-in-the-right-direction.html</guid>
      <description>Research in machine learning has seen some of the biggest and brightest minds of our time - and copious amounts of funding - funneled into the pursuit of better, safer, and more generalizable algorithms. As the field grows, there is vigorous debate around the direction that growth should take (for a less biased take, see here). This week, I give some background on the major algorithm types being researched, help frame aspects of the ongoing debate, and ultimately conclude that there is no single direction to build toward - but that through collaboration, we’ll see advances on all fronts.</description>
    </item>
    
    <item>
      <title>NLP and Transfer Learning</title>
       
      <link>https://blog.fastforwardlabs.com/2019/08/28/nlp-and-transfer-learning.html</link>
      
      <pubDate>Wed, 28 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2019/08/28/nlp-and-transfer-learning.html</guid>
      <description>NLP and Transfer Learning Natural language processing just took a leap forward with the release of new high-quality language models combined with transfer learning. As you know, at Cloudera Fast Forward Labs, we have been researching this leap forward and have just finished a new report on the topic.
These new NLP models rely on deep neural networks, which are data hungry and compute intensive to train. Various research groups trained these new models with tremendous amounts of data and compute power.</description>
    </item>
    
    <item>
      <title>Two approaches for data validation in ML production</title>
       
      <link>https://blog.fastforwardlabs.com/2019/08/28/two-approaches-for-data-validation-in-ml-production.html</link>
      
      <pubDate>Wed, 28 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2019/08/28/two-approaches-for-data-validation-in-ml-production.html</guid>
      <description>Machine learning models start to deteriorate once we deploy them. This is partly because real life data changes, and models need to be re-trained to maintain their performance. Typical ML pipelines re-train periodically (daily, for example) using newly available data. But how do we validate data fed into the pipelines to make sure tainted data does not accidentally sneak into the production system? Tainted data could cause system crashes or lead to slow degradation of model performance.</description>
    </item>
    
    <item>
      <title>New research: Deep Learning for Image Analysis</title>
       
      <link>https://blog.fastforwardlabs.com/2019/07/22/new-research-deep-learning-for-image-analysis.html</link>
      
      <pubDate>Mon, 22 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2019/07/22/new-research-deep-learning-for-image-analysis.html</guid>
      <description>We discussed this research as part of our virtual event on Wednesday, July 24th; you can watch the replay here!
Convolutional Neural Networks (CNNs or ConvNets) excel at learning meaningful representations of features and concepts within images. These capabilities make CNNs extremely valuable for solving problems in the image analysis domain. We can automatically identify defects in manufactured items, reducing costs associated with quality assurance processes; we can now infer depth information and reconstruct 3D maps from 2D images without additional metadata, giving new potential to urban planning as well as entertainment experiences; we can perform pixel level separation of objects in images and video with applications ranging from public safety to medical robotics; and we can perform &amp;ldquo;super resolution&amp;rdquo; on photo images (upscale an image up to 10x) with a trained deep-learning system reconstructing, filling in, and thus sharpening images with information that would be omitted and lost using a standard digital zoom.</description>
    </item>
    
    <item>
      <title>New research: transfer learning for natural language processing</title>
       
      <link>https://blog.fastforwardlabs.com/2019/07/17/new-research-transfer-learning-for-natural-language-processing.html</link>
      
      <pubDate>Wed, 17 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2019/07/17/new-research-transfer-learning-for-natural-language-processing.html</guid>
      <description>We discussed this research as part of our virtual event on Wednesday, July 24th; you can watch the replay here! Machine learning powers systems that can translate language, guide searches, and interact with humans. All around us we are seeing automated systems that are getting better and better at processing natural language. Machines that can work directly with natural language are powerful, especially as a human interface, because language is the most direct way in which we communicate.</description>
    </item>
    
    <item>
      <title>Taking Snorkel for a spin</title>
       
      <link>https://blog.fastforwardlabs.com/2019/07/08/taking-snorkel-for-a-spin.html</link>
      
      <pubDate>Mon, 08 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2019/07/08/taking-snorkel-for-a-spin.html</guid>
      <description>Active learning, which we explored in our report on Learning with Limited Labeled Data, makes it possible to build machine learning models with a small set of labeled data. The typical simplified workflow when tackling a supervised machine learning problem is to i) locate the data, ii) create labels for all available data (the more the merrier), and iii) build a model. Instead of labeling all available data, active learning takes advantage of collaboration between humans and machines to smartly pick a small subset of data to be labeled.</description>
    </item>
    
    <item>
      <title>Seeing is not necessarily believing</title>
       
      <link>https://blog.fastforwardlabs.com/2019/06/26/seeing-is-not-necessarily-believing.html</link>
      
      <pubDate>Wed, 26 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2019/06/26/seeing-is-not-necessarily-believing.html</guid>
      <description>Advancements in machine learning have evolved to such an extent that machines can not only understand the input data but have also learned to create it. Generative models are one of the most promising approaches towards this goal. To train such a model we first collect a large amount of data (be it images, text, etc.) and then train a model to generate data like it.
Generative Adversarial Networks (GANs) are one such class of generative models, that, given a training dataset, learn to generate new data with the same statistics as the training set.</description>
    </item>
    
    <item>
      <title>The trouble with unexplainable algorithms</title>
       
      <link>https://blog.fastforwardlabs.com/2019/06/26/the-trouble-with-unexplainable-algorithms.html</link>
      
      <pubDate>Wed, 26 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2019/06/26/the-trouble-with-unexplainable-algorithms.html</guid>
      <description>Thanks to a mix of technology-driven disruption and savvy competitors, the business environment is an increasingly challenging one. Staying competitive requires a better understanding of customers’ behaviour and preferences. It also requires the ability to optimise internal processes to more efficiently support both these things. This is a big reason that so many organisations are investing in machine learning.
And yet the use of machine learning in organisations has not been universally welcomed by customers and service users.</description>
    </item>
    
    <item>
      <title>Open-ended Text Generation</title>
       
      <link>https://blog.fastforwardlabs.com/2019/05/29/open-ended-text-generation.html</link>
      
      <pubDate>Wed, 29 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2019/05/29/open-ended-text-generation.html</guid>
      <description>The goal in open-ended text generation is to create a coherent portion of text that is a continuation from the given context. For example, given a couple of sentences, this capability makes it possible for machines to self-write a coherent story. One can imagine using such a system for AI-assisted writing, but of course it can also be repurposed to generate misleading (fake) news articles.
Ovid&amp;rsquo;s Unicorn, written by OpenAI&amp;rsquo;s GPT-2, offers a glimpse of the state-of-the art.</description>
    </item>
    
    <item>
      <title>Rise of the Low-Code ML toolboxes</title>
       
      <link>https://blog.fastforwardlabs.com/2019/05/29/rise-of-the-low-code-ml-toolboxes.html</link>
      
      <pubDate>Wed, 29 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2019/05/29/rise-of-the-low-code-ml-toolboxes.html</guid>
      <description>We&amp;rsquo;ve written previously about the rapid growth and adoption of ML tools in the AutoML family. Specifically, we broke down AutoML into 4 categories:
 Citizen Data Science / ML Efficient Data Science / ML Learning to Learn Transfer Learning  Advancement has been especially rapid in tools designed to make Data Science/ML more efficient, and in learning to automate/optimize model architecture design. While these frameworks and tools have continued to mature, their prominence has unveiled another set of challenges.</description>
    </item>
    
    <item>
      <title>Meta-Learners - learning how to learn</title>
       
      <link>https://blog.fastforwardlabs.com/2019/05/22/meta-learners-learning-how-to-learn.html</link>
      
      <pubDate>Wed, 22 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2019/05/22/meta-learners-learning-how-to-learn.html</guid>
      <description>Active learning allows us to be smart about picking the right set of datapoints for which to create labels. Done properly, this approach results in models that are trained on less data performing comparatively to models trained on much more data. In the world of meta-learning, we do not focus on label acquisition; rather, we attempt to build a machine that learns quickly from a small number of training data.</description>
    </item>
    
    <item>
      <title>Teaching machines to recognize faces: a tale of two outcomes</title>
       
      <link>https://blog.fastforwardlabs.com/2019/04/29/teaching-machines-to-recognize-faces-a-tale-of-two-outcomes.html</link>
      
      <pubDate>Mon, 29 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2019/04/29/teaching-machines-to-recognize-faces-a-tale-of-two-outcomes.html</guid>
      <description>Machine learning used for facial recognition can have both potentially life-saving and privacy-killing consequences. The rise of cheap sensors, more robust data storage, and advances in machine learning have propelled many industries forward - for instance, many sensors used in the automative industry have focused on detecting vehicle malfunctions, maintenance needs, and even the early components of self-driving cars.
Last week Affectiva announced that it had closed a significant round of funding to further develop its emotion and object detection software (powered by machine learning algorithms).</description>
    </item>
    
    <item>
      <title>Visualizing Active Learning</title>
       
      <link>https://blog.fastforwardlabs.com/2019/04/29/visualizing-active-learning.html</link>
      
      <pubDate>Mon, 29 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2019/04/29/visualizing-active-learning.html</guid>
      <description>Active Learner shows how active learning selects which data to be labeled. Active Learner, our new research prototype, is an interactive visualization of different active learning strategies for labeling data. It features three different datasets (MNIST, Quickdraw, and Caltech) and four different data selection strategies (Random, Entropy, Adversarial, and Ensemble). By exploring the different datasets and strategies, you can build your intuition about how and why active learning works. (For more on the selection strategies, read our report Learning with Limited Labeled Data.</description>
    </item>
    
    <item>
      <title>An Invitation to Active Learning</title>
       
      <link>https://blog.fastforwardlabs.com/2019/04/03/an-invitation-to-active-learning.html</link>
      
      <pubDate>Wed, 03 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2019/04/03/an-invitation-to-active-learning.html</guid>
      <description>Many interesting learning problems exist in places where labeled data is limited. As such, much thought has been spent on how best to learn from limited labeled data. One obvious answer is simply to collect more data. That is valid, but for some applications, data is difficult or expensive to collect. If we will collect more data, we ought at least be smart about the data we collect. This motivates active learning, which provides strategies for learning in this scenario.</description>
    </item>
    
    <item>
      <title>A Guide to Learning with Limited Labeled Data</title>
       
      <link>https://blog.fastforwardlabs.com/2019/04/02/a-guide-to-learning-with-limited-labeled-data.html</link>
      
      <pubDate>Tue, 02 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2019/04/02/a-guide-to-learning-with-limited-labeled-data.html</guid>
      <description>We are excited to release Learning with Limited Labeled Data, the latest report and prototype from Cloudera Fast Forward Labs.
Being able to learn with limited labeled data relaxes the stringent labeled data requirement for supervised machine learning. Our report focuses on active learning, a technique that relies on collaboration between machines and humans to label smartly.
Active learning makes it possible to build applications using a small set of labeled data, and enables enterprises to leverage their large pools of unlabeled data.</description>
    </item>
    
    <item>
      <title>Three exciting developments at TensorFlow Dev Summit 2019</title>
       
      <link>https://blog.fastforwardlabs.com/2019/03/29/three-exciting-developments-at-tensorflow-dev-summit-2019.html</link>
      
      <pubDate>Fri, 29 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2019/03/29/three-exciting-developments-at-tensorflow-dev-summit-2019.html</guid>
      <description>The third annual TensorFlow Dev Summit was held. The entire first day of talks was live-streamed, and videos are available on the TensorFlow YouTube channel. I stayed up late (from the UK) watching a few of the talks, and here’s an entirely idiosyncratic description of three things I find exciting. A broader recap is provided by the TensorFlow team themselves.
TensorFlow Federated TensorFlow Federated is a library for federated learning. Given the title of our last report, you already know we think this is an exciting area.</description>
    </item>
    
    <item>
      <title>Learning with Limited Labeled Data</title>
       
      <link>https://blog.fastforwardlabs.com/2019/03/20/learning-with-limited-labeled-data.html</link>
      
      <pubDate>Wed, 20 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2019/03/20/learning-with-limited-labeled-data.html</guid>
      <description>In recent years, machine learning technologies - especially deep learning - have made breakthroughs which have turned science fiction into reality. Autonomous cars are almost possible, and machines can comprehend language. These technical advances are unprecedented, but they hinge on the availability of vast amounts of data.
For a form of machine learning known as supervised learning, having data itself is not sufficient. Supervised machine learning, while powerful, needs data in a form that can serve as examples for what machines should learn.</description>
    </item>
    
    <item>
      <title>Causality in machine learning</title>
       
      <link>https://blog.fastforwardlabs.com/2019/02/28/causality-in-machine-learning.html</link>
      
      <pubDate>Thu, 28 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2019/02/28/causality-in-machine-learning.html</guid>
      <description>Judea Pearl, the inventor of Bayesian networks, recently published a book called The Book of Why: The New Science of Cause and Effect. The book covers a great many things, including a detailed history of how the fields of causality and statistics have long been at odds, Pearl&amp;rsquo;s own do-calculus framework for teasing causal inferences from observational data, and why (in Pearl&amp;rsquo;s view) the future of AI depends on causality.</description>
    </item>
    
    <item>
      <title>Making an interactive UMAP visualization of the MNIST data set</title>
       
      <link>https://blog.fastforwardlabs.com/2019/01/29/making-an-interactive-umap-visualization-of-the-mnist-data-set.html</link>
      
      <pubDate>Tue, 29 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2019/01/29/making-an-interactive-umap-visualization-of-the-mnist-data-set.html</guid>
      <description>UMAP explorer: an interactive visualization of the MNIST data set
We&amp;rsquo;re in the middle of work on our next report, Learning with Limited Labeled Data, and the accompanying prototype. For the prototype&amp;rsquo;s front-end we wanted to be able visualize and explore the embedding of a large image data set. Once you get into the tens of thousands of points, this can be a challenge to do in the browser. To determine whether what we wanted to do on the front-end was possible, I decided to make a demo focused on the MNIST hand-written digit data set.</description>
    </item>
    
    <item>
      <title>The business case for federated learning</title>
       
      <link>https://blog.fastforwardlabs.com/2018/12/28/the-business-case-for-federated-learning.html</link>
      
      <pubDate>Fri, 28 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/12/28/the-business-case-for-federated-learning.html</guid>
      <description>Last month, we released Federated Learning, the latest report and prototype from Cloudera Fast Forward Labs.
Federated learning makes it possible to build machine learning systems without direct access to training data. The data remains in its original location, which helps to ensure privacy and reduces communication costs.
Federated learning in a nutshell To train a machine learning model you usually need to move all the data to a single machine or, failing that, to a cluster of machines in a data center.</description>
    </item>
    
    <item>
      <title>Deep Learning for Media Content</title>
       
      <link>https://blog.fastforwardlabs.com/2018/12/18/deep-learning-for-media-content.html</link>
      
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/12/18/deep-learning-for-media-content.html</guid>
      <description>Machine learning continues to make its way into the arts, most recently in film and TV.
In a recent blog post, data scientists at 20th Century Fox and technical staff at Google Cloud described the approach they are using to predict audiences for their movies. (The tone of the post is fairly self-promoting, befitting the subject matter and industries involved.)
Their product, Merlin Video, is a deep learning tool that analyzes movie trailer videos.</description>
    </item>
    
    <item>
      <title>Fine-tuning for Natural Language Processing</title>
       
      <link>https://blog.fastforwardlabs.com/2018/12/18/fine-tuning-for-natural-language-processing.html</link>
      
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/12/18/fine-tuning-for-natural-language-processing.html</guid>
      <description>2018 was a fun and exciting year for natural language processing. A series of papers put forth powerful new ideas that improve the way machines understand and work with language. They challenge the standard way of using pretrained word embeddings like word2vec to initialize the first layer of a neural net, while the rest is trained on data of a particular task. Instead, these papers propose better embeddings (feature-based approach) and pre-trained models that can be fine-tuned for a supervised downstream task (fine-tuning approach).</description>
    </item>
    
    <item>
      <title>Highlights of 2018</title>
       
      <link>https://blog.fastforwardlabs.com/2018/12/18/highlights-of-2018.html</link>
      
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/12/18/highlights-of-2018.html</guid>
      <description>We end 2018 with a round-up of some of the research, talks, sci-fi, visualizations/art, and a grab bag of other stuff we found particularly interesting, enjoyable, or influential this year (and we’re going to be a bit fuzzy about the definition of “this year”)!
Research In addition to our own research, on recommendation engines, multi-task learning, and federated learning, we found three other themes particularly interesting.
&amp;ldquo;The Alchymist, In Search of the Philosopher’s Stone, Discovers Phosphorus, and prays for the successful Conclusion of his operation, as was the custom of the Ancient Chymical Astrologers&amp;rdquo; (great title!</description>
    </item>
    
    <item>
      <title>Designing Turbofan Tycoon</title>
       
      <link>https://blog.fastforwardlabs.com/2018/12/06/designing-turbofan-tycoon.html</link>
      
      <pubDate>Thu, 06 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/12/06/designing-turbofan-tycoon.html</guid>
      <description>Our prototypes are designed to demonstrate the value of the technologies we research. For our most recent prototype, Turbofan Tycoon, we decided that the best way to demonstrate the value of federated learning was to place you in an interactive simulation where you’re in charge of maintaining four turbofan engines. In this post, I’m going to try and explain a bit about why we decided that, and focus on the interaction design.</description>
    </item>
    
    <item>
      <title>Federated learning: distributed machine learning with data locality and privacy</title>
       
      <link>https://blog.fastforwardlabs.com/2018/11/14/federated-learning-distributed-machine-learning-with-data-locality-and-privacy.html</link>
      
      <pubDate>Wed, 14 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/11/14/federated-learning-distributed-machine-learning-with-data-locality-and-privacy.html</guid>
      <description>We’re excited to release Federated Learning, the latest report and prototype from Cloudera Fast Forward Labs.
Federated learning makes it possible to build machine learning systems without direct access to training data. The data remains in its original location, which helps to ensure privacy and reduces communication costs.
This article is about the technical side of federated learning.
If you’d like to learn more:
 Read about the business and product side of federated learning on the Cloudera VISION blog Explore our interactive federated learning prototype, Turbofan Tycoon Register for our webinar (Thursday, November 15, 2018) Get access to the full 85 page report and advising time with the Fast Forward Labs team by becoming a Cloudera Fast Forward Labs client  The federated learning setting Two things define the federated learning setting.</description>
    </item>
    
    <item>
      <title>Coming Soon: Federated Learning</title>
       
      <link>https://blog.fastforwardlabs.com/2018/10/29/coming-soon-federated-learning.html</link>
      
      <pubDate>Mon, 29 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/10/29/coming-soon-federated-learning.html</guid>
      <description>Federated Learning is a technology that allows you to build machine learning systems when your datacenter can&amp;rsquo;t get direct access to model training data. The data remains in its original location, which helps to ensure privacy and reduces communication costs.
Privacy and reduced communication makes federated learning a great fit for smartphones and edge hardware, healthcare and other privacy-sensitive use cases, and industrial applications such as predictive maintenance.
What’s the Status Quo?</description>
    </item>
    
    <item>
      <title>Learning to learn in a model-agnostic way</title>
       
      <link>https://blog.fastforwardlabs.com/2018/10/29/learning-to-learn-in-a-model-agnostic-way.html</link>
      
      <pubDate>Mon, 29 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/10/29/learning-to-learn-in-a-model-agnostic-way.html</guid>
      <description>As humans, we can quickly adapt our actions in new situations, be it recognizing objects from a few examples, or learning new skills and applying them in a matter of just a few minutes. But when it comes to deep learning techniques, an understandably large amount of time and data is required. So the challenge is to help our deep models do the same thing we can - to learn and quickly adapt from only a few examples, and to continue to adapt as more data becomes available.</description>
    </item>
    
    <item>
      <title>The Decentralized Web</title>
       
      <link>https://blog.fastforwardlabs.com/2018/10/29/the-decentralized-web.html</link>
      
      <pubDate>Mon, 29 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/10/29/the-decentralized-web.html</guid>
      <description>Last week, Sir Tim Berners-Lee announced Solid, a project designed to give users more control over their data. Solid is one of a number of recent attempts to rethink how the web works. As part of an effort to get my head around the goals of these different approaches and, more concretely, what they actually do, I made some notes on what I see as the most interesting approaches.
Beaker browser for the peer-to-peer web.</description>
    </item>
    
    <item>
      <title>The quest continues: a look at a new initiative to explore human and machine intelligence</title>
       
      <link>https://blog.fastforwardlabs.com/2018/10/29/the-quest-continues-a-look-at-a-new-initiative-to-explore-human-and-machine-intelligence.html</link>
      
      <pubDate>Mon, 29 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/10/29/the-quest-continues-a-look-at-a-new-initiative-to-explore-human-and-machine-intelligence.html</guid>
      <description>The resurrection of neural networks as a technique has helped propel the field of machine learning to the forefront of commercial applications. Today’s most popular applications focus on finding patterns in data and exploiting those patterns for very narrow tasks. But what if we want more from machine learning? Instead of trying to contort the methods we have today to achieve marginal gains in generalizability, what if we took another angle altogether?</description>
    </item>
    
    <item>
      <title>Apache Spark gets a machine learning makeover</title>
       
      <link>https://blog.fastforwardlabs.com/2018/09/28/apache-spark-gets-a-machine-learning-makeover.html</link>
      
      <pubDate>Fri, 28 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/09/28/apache-spark-gets-a-machine-learning-makeover.html</guid>
      <description>Machine learning on Spark: an abridged history Apache Spark - the cluster computing framework that&amp;rsquo;s been throwing shade at MapReduce since 2011 - has always been a bit remarkable, because it bridged the divide between data engineering and data science. One of the great promises of Spark was that it would be easy, trivial almost, to scale machine learning and data science to arbitrarily large data. Seven years later, Spark has made its place in data science, but perhaps not in the way we originally hoped.</description>
    </item>
    
    <item>
      <title>Realistic Video Generation</title>
       
      <link>https://blog.fastforwardlabs.com/2018/09/28/realistic-video-generation.html</link>
      
      <pubDate>Fri, 28 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/09/28/realistic-video-generation.html</guid>
      <description>Generative Adversarial Networks (GANs) wowed the world in 2014 with their ability to generate what we considered to be realistic images. While these images were quite low resolution, researchers kept working on how to perfect these methods in order to increase the quality of the images and even to apply the algorithm on other types of data like text and sound.
However, until recently there has been little success in making realistic videos.</description>
    </item>
    
    <item>
      <title>Snorkel: Rapid Training Data Creation with Weak Supervision</title>
       
      <link>https://blog.fastforwardlabs.com/2018/09/28/snorkel-rapid-training-data-creation-with-weak-supervision.html</link>
      
      <pubDate>Fri, 28 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/09/28/snorkel-rapid-training-data-creation-with-weak-supervision.html</guid>
      <description>Machine learning (and more specifically, deep learning) techniques are recognized for their ability to obtain high accuracy for a variety of classification problems. Deep learning models are more complex than most traditional models and often have thousands of parameters and commensurately require more labeled training data. Obtaining this data can be an expensive and time-consuming process, and often requires domain experts to help with the labeling process. Many areas of machine learning that are motivated by this bottleneck could be broadly categorized into four areas: active learning, semi-supervised learning, weak supervision and transfer learning.</description>
    </item>
    
    <item>
      <title>The True Competitive Advantage in ML</title>
       
      <link>https://blog.fastforwardlabs.com/2018/09/28/the-true-competitive-advantage-in-ml.html</link>
      
      <pubDate>Fri, 28 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/09/28/the-true-competitive-advantage-in-ml.html</guid>
      <description>Data science, machine learning (ML), and AI are no longer just cute buzzwords. Nearly all organizations, companies and governments now recognize the immense potential of AI-enabled products and services, and many of them have already made the very real investment of hiring employees with skills in these emerging fields.
However, as is true with most things in data science, one size does not fit all! Simply hiring a self-proclaimed data scientist or machine learning expert with advanced degrees isn&amp;rsquo;t likely to fit the bill.</description>
    </item>
    
    <item>
      <title>Deep learning made easier with transfer learning</title>
       
      <link>https://blog.fastforwardlabs.com/2018/09/17/deep-learning-made-easier-with-transfer-learning.html</link>
      
      <pubDate>Mon, 17 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/09/17/deep-learning-made-easier-with-transfer-learning.html</guid>
      <description>.post img { max-width: 460px } Deep learning has provided extraordinary advances in problem spaces that are poorly solved by other approaches. This success is due to several key departures from traditional machine learning that allow it to excel when applied to unstructured data. Today, deep learning models can play games, detect cancer, talk to humans, and drive cars.
But the differences that make deep learning powerful also make it costly.</description>
    </item>
    
    <item>
      <title>Automated Machine Learning: Hype now, reality later?</title>
       
      <link>https://blog.fastforwardlabs.com/2018/08/29/automated-machine-learning-hype-now-reality-later.html</link>
      
      <pubDate>Wed, 29 Aug 2018 20:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/08/29/automated-machine-learning-hype-now-reality-later.html</guid>
      <description>Previously in our newsletter, we had framed automated machine learning around four notions:
 Citizen Data Science / ML: Automated machine learning will allow everyone to do data science and ML. It requires no special training or skills. Efficient Data Science / ML: Automated machine learning will supercharge your data scientists and ML engineers by making them more efficient. Learning to Learn: Automated machine learning will automate architecture and optimization algorithm design(architecture search).</description>
    </item>
    
    <item>
      <title>Hyperparameter Tuning and Meta-Interpretability: Track All Your Experiments!</title>
       
      <link>https://blog.fastforwardlabs.com/2018/08/29/hyperparameter-tuning-and-meta-interpretability-track-all-your-experiments.html</link>
      
      <pubDate>Wed, 29 Aug 2018 20:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/08/29/hyperparameter-tuning-and-meta-interpretability-track-all-your-experiments.html</guid>
      <description>From random forest to neural networks, many modern machine learning algorithms involve a number of parameters that have to be fixed before training the algorithm. These parameters, in contrast to the ones learned by the algorithm during training, are called hyperparameters. The performance of a model on a task given data depends on the specific values of these hyperparameters.
Hyperparamter tuning is the process of determining the hyperparameter values that maximize model performance on a task given data.</description>
    </item>
    
    <item>
      <title>Breakthroughs in transfer learning for natural language processing</title>
       
      <link>https://blog.fastforwardlabs.com/2018/08/29/breakthroughs-in-transfer-learning-for-natural-language-processing.html</link>
      
      <pubDate>Wed, 29 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/08/29/breakthroughs-in-transfer-learning-for-natural-language-processing.html</guid>
      <description>One of the most exciting parts of our jobs at Cloudera Fast Forward Labs is our work on applied machine learning research. Through this research we see and work with some of the most exciting developments in machine learning, deep learning, and AI, but - as with any field that has been overhyped - we sift through a lot of noise. By noise, we generally mean research that is too immature to be of practical use, or research that follows one or more of the troubling trends in machine learning.</description>
    </item>
    
    <item>
      <title>Understanding a generative space</title>
       
      <link>https://blog.fastforwardlabs.com/2018/08/29/understanding-a-generative-space.html</link>
      
      <pubDate>Wed, 29 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/08/29/understanding-a-generative-space.html</guid>
      <description>Kate Compton, maker of many interesting things, recently tweeted a diagram from her work-in-progress dissertation zine. The illustration breaks down how we interact with a procedural generator, which can be anything from a photo filter to a character creation tool.
&amp;ldquo;How we understand a generative space&amp;rdquo; by Kate Compton. Understanding a generative space requires moving between the digital space (where the code is and the results are displayed) and the mental space (where we evaluate the display and make guesses about how it works).</description>
    </item>
    
    <item>
      <title>Multi-Task Sci-Fi: Havurtat</title>
      
      <link>https://scifi.fastforwardlabs.com/ff08/havurtat</link>
      
      <pubDate>Wed, 15 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/08/15/multi-task-sci-fi-havurtat.html</guid>
      <description>Each report we do features a science-fiction story inspired by the report topic. For our multi-task learning report, Umair Kazi explores the paradoxes of knowledge in the lost city of Havurtat.</description>
    </item>
    
    <item>
      <title>Neural reinterpretations of movie trailers</title>
       
      <link>https://blog.fastforwardlabs.com/2018/07/31/neural-reinterpretations-of-movie-trailers.html</link>
      
      <pubDate>Tue, 31 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/07/31/neural-reinterpretations-of-movie-trailers.html</guid>
      <description>In his latest project, artist and coder Mario Klingemann uses a neural network to match archival movie footage with the content of recent movie trailers. He regularly posts the resulting “neural reinterpretations” on his Twitter. The results are technically impressive. They’re also a fascinating view into how to explore the creative possibilities of a machine learning technique.
Looking through Klingemann’s tweets you can trace his explorations:
![A screenshot from Klingemann&amp;rsquo;s video of similar scene classification.</description>
    </item>
    
    <item>
      <title>New Dynamics for Topic Models</title>
       
      <link>https://blog.fastforwardlabs.com/2018/07/31/new-dynamics-for-topic-models.html</link>
      
      <pubDate>Tue, 31 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/07/31/new-dynamics-for-topic-models.html</guid>
      <description>Topic models can extract key themes from large collections of documents in an unsupervised manner, which makes them one of the most powerful tools in organizing, searching, and understanding the vast troves of text data produced by humanity. Their power derives, in part, from their in-built assumptions about the nature of text; specifically, to identify topics, the model has to give the notion of a topic a mathematical structure that echoes its significance to a human reader.</description>
    </item>
    
    <item>
      <title>Progress in machine learning interpretability</title>
       
      <link>https://blog.fastforwardlabs.com/2018/07/31/progress-in-machine-learning-interpretability.html</link>
      
      <pubDate>Tue, 31 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/07/31/progress-in-machine-learning-interpretability.html</guid>
      <description>Our goal when we do research is to address capabilities and technologies that we expect to become production-ready in one to two years. That focus on fast-moving areas means that new algorithmic ideas sometimes come along that allow our clients to extend or improve upon the work in our reports.
We published our report on machine learning interpretability last year. The technical focus of our report was LIME, a tool that computes locally correct explanations of a model&amp;rsquo;s behaviour.</description>
    </item>
    
    <item>
      <title>New Research on Multi-Task Learning</title>
       
      <link>https://blog.fastforwardlabs.com/2018/07/24/new-research-on-multi-task-learning.html</link>
      
      <pubDate>Tue, 24 Jul 2018 09:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/07/24/new-research-on-multi-task-learning.html</guid>
      <description>We are excited to share the latest report and prototype from our machine intelligence R&amp;amp;D team: Multi-Task Learning.
Wax on.. face off! When humans learn new tasks, we take advantage of knowledge we&amp;rsquo;ve gained from learning, or having learned, related tasks. Take the 1984 movie Karate Kid, where Mr. Miyagi takes on Daniel as his martial arts student. He begins Daniel&amp;rsquo;s training by having him complete various lengthy menial chores, from waxing cars to sanding floors.</description>
    </item>
    
    <item>
      <title>Deep Learning Vendor Update: Hyperparameter Tuning Systems</title>
       
      <link>https://blog.fastforwardlabs.com/2018/06/29/deep-learning-vendor-update-hyperparameter-tuning-systems.html</link>
      
      <pubDate>Fri, 29 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/06/29/deep-learning-vendor-update-hyperparameter-tuning-systems.html</guid>
      <description>In our research reports, we cover &amp;ldquo;the recently possible,&amp;rdquo; and what makes &amp;ldquo;the recently possible&amp;rdquo; possible. In addition to a detailed how-to guide of new machine learning capabilities, each of our reports contains a section on open source projects, commercial offerings, and vendors that help implement this new machine learning capability to realize the opportunities opened up by technological innovation. We like to keep an eye on the the technologies we research, of course.</description>
    </item>
    
    <item>
      <title>Sequence labeling with semi-supervised multi-task learning</title>
       
      <link>https://blog.fastforwardlabs.com/2018/06/29/sequence-labeling-with-semi-supervised-multi-task-learning.html</link>
      
      <pubDate>Fri, 29 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/06/29/sequence-labeling-with-semi-supervised-multi-task-learning.html</guid>
      <description>Sequence labeling tasks attempt to assign a categorical label to each member in the sequence. In natural language processing, where a sequence generally refers to a sentence, examples of sequence labeling include named entity recognition (NER), part-of-speech tagging (POS) and error detection. NER, as the name implies, tries to recognize names in a sentence and classify them into pre-defined labels such as Person and Organization. POS tagging assigns labels such as noun, verb, and adjective to each word, while error detection identifies grammatical errors in sentences.</description>
    </item>
    
    <item>
      <title>Supercharging Classification - The Value of Multi-task Learning</title>
       
      <link>https://blog.fastforwardlabs.com/2018/06/26/supercharging-classification-the-value-of-multi-task-learning.html</link>
      
      <pubDate>Tue, 26 Jun 2018 20:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/06/26/supercharging-classification-the-value-of-multi-task-learning.html</guid>
      <description>Today&amp;rsquo;s machines can identify objects in photographs, predict loan repayments or defaults, write short summaries of long articles, or recommend movies you may like. Up until now, machines have achieved mastery through laser-like focus; most machine learning algorithms today train models to master one task, and one task only. We are excited to introduce multi-task learning in our upcoming webinar, report, and prototype. Multi-task learning is an approach to machine learning that goes beyond single-task approaches and supercharges classification by allowing algorithms to master more than one task at once and in parallel.</description>
    </item>
    
    <item>
      <title>Convolve all the things</title>
       
      <link>https://blog.fastforwardlabs.com/2018/05/31/convolve-all-the-things.html</link>
      
      <pubDate>Thu, 31 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/05/31/convolve-all-the-things.html</guid>
      <description>While deep learning can be applied generally, much of the excitement around it has stemmed from significant breakthroughs in two main areas: computer vision and natural language processing. Practitioners have typically applied convolutional neural networks (CNNs) to spatial data (e.g. images) and recurrent neural networks (RNNs) to sequence data (e.g. text). However, a recent research paper has shown that convolutional neural networks are not only capable of performing well on sequential data tasks, but they have inherent advantages over recurrent networks and may be a better default starting point.</description>
    </item>
    
    <item>
      <title>Rules to Learn By</title>
       
      <link>https://blog.fastforwardlabs.com/2018/05/31/rules-to-learn-by.html</link>
      
      <pubDate>Thu, 31 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/05/31/rules-to-learn-by.html</guid>
      <description>Longtime readers of this newsletter know that we follow the Fairness, Accountability, and Transparency in Machine Learning conversation closely (see here and here). These conversations address and attempt to mitigate the potential for technical systems to produce unfairness. Much of this unfairness arises from how algorithmic systems might perpetuate historical inequalities or otherwise produce discriminatory effects. This conversation is broader than could be encapsulated in any newsletter, but we want to point to some recommendations that have come out of this conversation to demonstrate how we think through the challenges of building models that don&amp;rsquo;t learn or perpetuate bias.</description>
    </item>
    
    <item>
      <title>Progress in text summarization</title>
       
      <link>https://blog.fastforwardlabs.com/2018/05/02/progress-in-text-summarization.html</link>
      
      <pubDate>Wed, 02 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/05/02/progress-in-text-summarization.html</guid>
      <description>We published our report on text summarization in 2016. Since then, we&amp;rsquo;ve enjoyed helping our clients make use of techniques such as topic modeling, document embedding, and recurrent neural networks to deal with text that ranges in scope from product reviews to insurance documents to call transcripts to news.
Our goal when we do research is to address capabilities and technologies that we expect to become production-ready in one to two years.</description>
    </item>
    
    <item>
      <title>JavaScript eats the world: deep learning and notebooks edition</title>
       
      <link>https://blog.fastforwardlabs.com/2018/04/25/javascript-eats-the-world-deep-learning-and-notebooks-edition.html</link>
      
      <pubDate>Wed, 25 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/04/25/javascript-eats-the-world-deep-learning-and-notebooks-edition.html</guid>
      <description>Google recently announced TensorFlow.js, an open-source library for running machine learning in the browser, and a successor to the deeplearn.js library. While the majority of machine learning work is unlikely to shift to JavaScript anytime soon, the examples included on the TensorFlow.js site do a good job of showing the promise of machine learning models that run in the browser.
Teachable Machine lets you train a model to help you wave at cats.</description>
    </item>
    
    <item>
      <title>Simple Architectures Outperform Complex Ones in Language Modeling</title>
       
      <link>https://blog.fastforwardlabs.com/2018/04/25/simple-architectures-outperform-complex-ones-in-language-modeling.html</link>
      
      <pubDate>Wed, 25 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/04/25/simple-architectures-outperform-complex-ones-in-language-modeling.html</guid>
      <description>Are novel, complex, and specialized neural network architectures always better for language modeling? Recent papers have shown otherwise. Language models are used to predict the next token given the preceeding tokens. Most operate at word-level or character-level. Word-level models have large vocabulary sizes (how many words are there in the English language?) compared to character-level models (there are 26 letters in the English language). This means that character-level models require less memory.</description>
    </item>
    
    <item>
      <title>Introducing SciFi</title>
       
      <link>https://blog.fastforwardlabs.com/2018/04/18/introducing-scifi.html</link>
      
      <pubDate>Wed, 18 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/04/18/introducing-scifi.html</guid>
      <description>Today we are launching a mini-site featuring our collection of short stories inspired by new developments in machine learning. Beginning with our fourth report, we started including a science-fiction story along with the technical and strategic overviews that are the bulk of each report. Using these stories, we can look at the technologies we profile from a different angle and explore their cultural implications.
The SciFi site includes the four stories we have included so far.</description>
    </item>
    
    <item>
      <title>PyTorch for Recommenders 101</title>
       
      <link>https://blog.fastforwardlabs.com/2018/04/10/pytorch-for-recommenders-101.html</link>
      
      <pubDate>Tue, 10 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/04/10/pytorch-for-recommenders-101.html</guid>
      <description>Recommenders, generally associated with e-commerce, sift though a huge inventory of available items to find and recommend ones that a user will like. Different from search, recommenders rely on historical data to tease out user preference. How does a recommender accomplish this? In this post we explore building simple recommendation systems in PyTorch using the Movielens 100K data, which has 100,000 ratings (1-5) that 943 users provided on 1682 movies.</description>
    </item>
    
    <item>
      <title>New creative possibilities with machine learning</title>
       
      <link>https://blog.fastforwardlabs.com/2018/03/28/new-creative-possibilities-with-machine-learning.html</link>
      
      <pubDate>Wed, 28 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/03/28/new-creative-possibilities-with-machine-learning.html</guid>
      <description>Machine learning techniques are able to organize large amounts of unstructured data. Combined with dimensionality reduction techniques like t-SNE, this capability opens up new ways for us to interact with creative material including sounds, words, and ideas. In this section we highlight three of our favorite recent experiments.
The Infinite Drum Machine is a Google Creative Lab experiment by Manny Tan and Kyle McDonald. It uses machine learning to cluster a large number of samples by similarity of sound.</description>
    </item>
    
    <item>
      <title>Unemployment vs. Augmentation</title>
       
      <link>https://blog.fastforwardlabs.com/2018/03/28/unemployment-vs.-augmentation.html</link>
      
      <pubDate>Wed, 28 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/03/28/unemployment-vs.-augmentation.html</guid>
      <description>In the ongoing debates around whether or not robots are going to take our jobs, listening to those who have a real stake in the technology is critical, and often offers important insights for how we build new technologies, as well as how we talk about what we build. Take, for example, this blog post by Judy Gichoya about the Radiology Society of North America&amp;rsquo;s annual meeting last December, which provides a useful window into the concerns radiologists have about the ways automation will affect their profession, and how those concerns could be taken into account when building capabilities that impact their domain of expertise.</description>
    </item>
    
    <item>
      <title>Using three.js for 2D data visualization</title>
      
      <link>https://beta.observablehq.com/@grantcuster/using-three-js-for-2d-data-visualization</link>
      
      <pubDate>Tue, 06 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/03/06/using-three.js-for-2d-data-visualization.html</guid>
      <description>A code walkthrough of how to use three.js for large 2D data visualizations. Three.js can smoothly render a large number of points in the browser but there are some tricks to making it work for interactive 2D visualizations. This is the guide that I wish I would have had when I built our most recent prototype. It shows how to enable mouse-directed pan and zoom, circular points, and hover interactions. It is presented as an interactive javascript notebook with working code.</description>
    </item>
    
    <item>
      <title>Comparing human and agent performance: DeepMind releases PsychLab</title>
       
      <link>https://blog.fastforwardlabs.com/2018/02/28/comparing-human-and-agent-performance-deepmind-releases-psychlab.html</link>
      
      <pubDate>Wed, 28 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/02/28/comparing-human-and-agent-performance-deepmind-releases-psychlab.html</guid>
      <description>Google’s DeepMind released PsychLab this week, which has been developed internally and released to the public as part of DeepMind’s efforts to apply decades of research in cognitive science/neuroscience to advance the state of the art in machine learning and artificial intelligence. Many modern machine learning models have taken inspiration from principles derived from decades of research in cognitive science/neuroscience. This announcement, along with the accompanying paper, provide an open-source playground for testing how agents (built using LSTM deep learning alrogirthms) perform when compared to humans on a slew of cognitive tasks that are fairly well-understood and widely used to study human perception.</description>
    </item>
    
    <item>
      <title>Machine Learning In The Browser</title>
       
      <link>https://blog.fastforwardlabs.com/2018/02/28/machine-learning-in-the-browser.html</link>
      
      <pubDate>Wed, 28 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/02/28/machine-learning-in-the-browser.html</guid>
      <description>The promise of Machine Learning (ML) on edge devices holds potential to enable new capabilities while reaping the benefits associated with on-device computation. As an environment that is frequently the source of data (user interactions, sensors such as cameras, acelerometers, etc.), the browser within PCs, mobile and IoT devices represents an important edge “platform.” Deploying models in such environments can improve latency for interactive applications, reduce model distribution costs, and enable privacy as data is no longer sent to remote servers for analysis.</description>
    </item>
    
    <item>
      <title>Multi-Task Learning</title>
       
      <link>https://blog.fastforwardlabs.com/2018/02/28/multi-task-learning.html</link>
      
      <pubDate>Wed, 28 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/02/28/multi-task-learning.html</guid>
      <description>The common approach in machine learning is to train and optimize one task at a time. In contrast, multitask learning (MTL) trains related tasks in parallel, using a shared representation. One advantage of MTL is improved generalization - using information regarding related tasks prevents a model from being overly focused on a single task, while it is also learning to produce better results.
MTL is an approach, and is not restricted to any particular algorithm.</description>
    </item>
    
    <item>
      <title>Probabilistic Cookies!</title>
       
      <link>https://blog.fastforwardlabs.com/2018/02/14/probabilistic-cookies.html</link>
      
      <pubDate>Wed, 14 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/02/14/probabilistic-cookies.html</guid>
      <description>In the spirit of Valentine&amp;rsquo;s Day, we at Fast Forward Labs thought it would be fun to bake cookies for our sweethearts. Being DIY nerds, we thought we&amp;rsquo;d math it up a bit.
We used python to generate probability distributions and matplotlib to check our distributions. Then we wrote a python function to generate a SCAD file defining three-dimensional shapes from the distributions. Using OpenSCAD, an open-source CAD program, we checked the 3D models and exported them to STL files for printing.</description>
    </item>
    
    <item>
      <title>Google Maps&#39; Competitive Moat - Why Better Data Matters</title>
       
      <link>https://blog.fastforwardlabs.com/2018/01/26/google-maps-competitive-moat-why-better-data-matters.html</link>
      
      <pubDate>Fri, 26 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/01/26/google-maps-competitive-moat-why-better-data-matters.html</guid>
      <description>Justin O&amp;rsquo;Beirne recently wrote a blog post surveying the landscape of mapping apps and sites (with particular focus on Google Maps and Apple Maps). This is one of a series of pieces O&amp;rsquo;Beirne has written comparing Google and Apple Maps. In short, O&amp;rsquo;Beirne concludes that Google has a substantial competitive advantage in mapping that boils down to having better data.
The article is centered around the example of Google&amp;rsquo;s Areas of Interest feature.</description>
    </item>
    
    <item>
      <title>Serverless data science</title>
       
      <link>https://blog.fastforwardlabs.com/2018/01/26/serverless-data-science.html</link>
      
      <pubDate>Fri, 26 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/01/26/serverless-data-science.html</guid>
      <description>Cloud computing has transformed enterprise IT. It makes it possible to rapidly scale up and down a complex and global infrastructure, without the overhead of a datacenter. But living in the cloud can be expensive, and you still need to maintain computers and their operating systems, even if they are virtual. That&amp;rsquo;s why we&amp;rsquo;ve been watching with interest the rise of &amp;ldquo;serverless&amp;rdquo; computing.
Serverless has the potential to open up &amp;ldquo;big data&amp;rdquo; work to mere mortal data scientists who don&amp;rsquo;t have the budget or engineering support for a long-lived analytics cluster, and to make life simpler and reduce costs for those that do.</description>
    </item>
    
    <item>
      <title>Exploring Recommendation Systems</title>
       
      <link>https://blog.fastforwardlabs.com/2018/01/22/exploring-recommendation-systems.html</link>
      
      <pubDate>Mon, 22 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2018/01/22/exploring-recommendation-systems.html</guid>
      <description>While we commonly associate recommendation systems with e-commerce, their application extends to any decision-making problem which requires pairing two types of things together. To understand why recommenders don&amp;rsquo;t always work as well as we&amp;rsquo;d like them to, we set out to build some basic recommendation systems using publicly available data.
Data The first ingredient for building a recommendation system is user interaction data. We experimented with two different datasets, one from Flickr and one from Amazon.</description>
    </item>
    
    <item>
      <title>Highlights of 2017</title>
       
      <link>https://blog.fastforwardlabs.com/2017/12/20/highlights-of-2017.html</link>
      
      <pubDate>Wed, 20 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/12/20/highlights-of-2017.html</guid>
      <description>We end 2017 with a round-up of some of the research, talks, sci-fi, visualizations/art, and a grab bag of other stuff we found particularly interesting, enjoyable, or influential this year.
Stars from around the world. Image credit: Kyle McDonald Research 2017 saw renewed interest in using the human brain as an inspiration for machine learning &amp;mdash; perhaps most notably, Dynamic Routing Between Capsule Networks by Sabour, Frosst, and Hinton.
Somewhat to counter this trend (and Hinton&amp;rsquo;s criticism of stochastic gradient descent based on its biological implausibility) and to highlight necessary foundational work, we recommend two papers from Arpit, Jastrzębski, and collaborators: Memorization in Deep Neural Networks and The Factors Influencing Minima in SGD (with practical suggestions for developers) are beautiful examples of foundational work the field needs now that neural networks have proven to be effective applied problem solvers.</description>
    </item>
    
    <item>
      <title>The promise of Automated Machine Learning (AutoML)</title>
       
      <link>https://blog.fastforwardlabs.com/2017/11/30/the-promise-of-automated-machine-learning-automl.html</link>
      
      <pubDate>Thu, 30 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/11/30/the-promise-of-automated-machine-learning-automl.html</guid>
      <description>Earlier this month The New York Times published an article on Building A.I. That Can Build A.I.. There is a lot of excitement about AutoML due to the scarcity of machine learning (ML) talent:
 By some estimates, only 10,000 people worldwide have the education, experience and talent needed to build the complex and sometimes mysterious mathematical algorithms that will drive this new breed of artificial intelligence.
 Furthermore, ML/AI experts are expensive: Tech Giants Are Paying Huge Salaries for Scarce A.</description>
    </item>
    
    <item>
      <title>Algorithmic Cookery &amp; Happy Thanksgiving</title>
       
      <link>https://blog.fastforwardlabs.com/2017/11/22/algorithmic-cookery-happy-thanksgiving.html</link>
      
      <pubDate>Wed, 22 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/11/22/algorithmic-cookery-happy-thanksgiving.html</guid>
      <description>As you are preparing for your Thanksgiving meal, just know that a robotic arm is holding the spoon at the Institute for Culinary Education (ICE); progress is relentless. &amp;ldquo;The Chef Watson cookbook is a revolutionary display of the creative collaboration of man and machine.&amp;quot; Cognitive Cooking with Chef Watson, culinary and cognitive creativity at your fingertips. Perhaps you should try the Acorn Squash Meat Roast &amp;hellip; with English Breakfast Tea.</description>
    </item>
    
    <item>
      <title>Interpretability Sci-Fi: The Definition of Success</title>
       
      <link>https://blog.fastforwardlabs.com/2017/11/02/interpretability-sci-fi-the-definition-of-success.html</link>
      
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/11/02/interpretability-sci-fi-the-definition-of-success.html</guid>
      <description>We believe in fiction as an important tool for imagining future relationships to technology. In our reports on new technologies we feature short science fiction stories that imagine the possible implications. The following story, influenced by a certain classic Sci-Fi film, appeared in our Interpretability report. For more on interpretability read a video conversation on interpretability, a guide to using the LIME technique to predict whether couples will stay together, and a look at the business rationale.</description>
    </item>
    
    <item>
      <title>AlphaGo Zero&#39;s win, what it means</title>
       
      <link>https://blog.fastforwardlabs.com/2017/10/26/alphago-zeros-win-what-it-means.html</link>
      
      <pubDate>Thu, 26 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/10/26/alphago-zeros-win-what-it-means.html</guid>
      <description>In an almost Freudian twist, a play on Vatermord by an artificial intelligence, AlphaGo Zero has dethroned its &amp;ldquo;parent,&amp;rdquo; AlphaGo.
In March 2016, AlphaGo defeated 18-time world Go champion Lee Sedol. At the Future of Go Summit in May 2017, AlphaGo prevailed against China&amp;rsquo;s top Go players including Ke Jie, who was considered to be the world&amp;rsquo;s best Go player.
AlphaGo&amp;rsquo;s reign was short lived. On October 18th 2017, in a tournament to which human players were not invited, AlphaGo Zero beat AlphaGo.</description>
    </item>
    
    <item>
      <title>Bias Mitigation Using the Copyright Doctrine of Fair Use</title>
       
      <link>https://blog.fastforwardlabs.com/2017/10/26/bias-mitigation-using-the-copyright-doctrine-of-fair-use.html</link>
      
      <pubDate>Thu, 26 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/10/26/bias-mitigation-using-the-copyright-doctrine-of-fair-use.html</guid>
      <description>Pirating a copyrighted song, video, or e-book to listen to the song, watch the movie, or read the book is an infringement of copyright (which can be severely fined). So how about pirating a song, video, or e-book to train machine learning models?
NYU Teaching and Research Fellow Amanda Levendowski proposes a legal approach to reducing bias in machine learning models. Biased data leads to biased models, she argues, and use of existing public domain data, most of which is over 70 years old, introduces biases from a time before, e.</description>
    </item>
    
    <item>
      <title>Neuroscience-inspired AI</title>
       
      <link>https://blog.fastforwardlabs.com/2017/10/26/neuroscience-inspired-ai.html</link>
      
      <pubDate>Thu, 26 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/10/26/neuroscience-inspired-ai.html</guid>
      <description>Pioneers in artificial intelligence (AI) have worked across multiple related fields, including computer science, AI, neuroscience, and psychology - but as each of these areas of research have grown in complexity and disciplinary boundaries have solidified, collaboration has become less commonplace. In Neuroscience-Inspired Artificial Intelligence, the co-founder of Google DeepMind Demis Hassabis, alongside other renowned neuroscientists, argues to revive collaborative efforts.
The (human) brain is a living case-in-point that human-level general AI is possible, but building it is a daunting task.</description>
    </item>
    
    <item>
      <title>First Look: Using Three.js for 2D Data Visualization</title>
       
      <link>https://blog.fastforwardlabs.com/2017/10/04/first-look-using-three.js-for-2d-data-visualization.html</link>
      
      <pubDate>Wed, 04 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/10/04/first-look-using-three.js-for-2d-data-visualization.html</guid>
      <description>Update: I have a new post on Using three.js for 2D data visualization. It contains all the stuff I learned while working on the project and is the place you should look for updated code. This post may still be interesting if you want to see my thoughts when I first started on the project. We&amp;rsquo;ve started work on our next prototype. While the design is still evolving, we&amp;rsquo;re pretty sure one element of it will be a visualization of tens of thousands of data points, clustered through a dimensional reduction algorithm (most likely using T-SNE).</description>
    </item>
    
    <item>
      <title>Probabilistic programming: an annotated bibliography</title>
       
      <link>https://blog.fastforwardlabs.com/2017/10/02/probabilistic-programming-an-annotated-bibliography.html</link>
      
      <pubDate>Mon, 02 Oct 2017 20:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/10/02/probabilistic-programming-an-annotated-bibliography.html</guid>
      <description>Earlier this year we launched a research report on probabilistic programming, an emerging programming paradigm that makes it easier to describe and train probabilistic models. The Bayesian probabilistic approach to model building and inference has many advantages in practical data science, including the ability to quantify risk (a superpower in industries like finance and insurance) and the ability to insert institutional knowledge (which is particularly useful when data is scarce). The rise of probabilistic programming languages has made it a more practical technique for time-constrained working data scientists.</description>
    </item>
    
    <item>
      <title>Futurism and Artificial Intelligence</title>
       
      <link>https://blog.fastforwardlabs.com/2017/09/29/futurism-and-artificial-intelligence.html</link>
      
      <pubDate>Fri, 29 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/09/29/futurism-and-artificial-intelligence.html</guid>
      <description>For some, Mayweather-McGregor was the prizefight of the summer. For others, it has been Musk-Zuckerberg going toe-to-toe over the risks posed by AI, with Musk voicing his reservations about artificial intelligence while Zuck remains more sanguine. Musk has called AI possibly the &amp;ldquo;biggest threat&amp;rdquo; to humanity and gone so far as to suggest the decidedly un-Catholic opinion that Silicon Valley should be welcoming regulatory oversight of AI in this one exceptional instance.</description>
    </item>
    
    <item>
      <title>The Danger and Promise of Adversarial Samples</title>
       
      <link>https://blog.fastforwardlabs.com/2017/09/29/the-danger-and-promise-of-adversarial-samples.html</link>
      
      <pubDate>Fri, 29 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/09/29/the-danger-and-promise-of-adversarial-samples.html</guid>
      <description>Adversarial samples are inputs designed to fool a model: they are inputs created by applying perturbations to example inputs in the dataset such that the perturbed inputs result in the model outputting an incorrect answer with high confidence. Often, perturbations are so small that they are imperceptible to the human eye — they are inconspicuous.
Adversarial samples are a concern in a world where algorithms make decisions that affect lives: imagine an imperceptibly altered stop sign that the otherwise high-accuracy image recongnition algorithm of a self-driving car misclassifies as a toilet.</description>
    </item>
    
    <item>
      <title>The Product Possibilities of Interpretability</title>
       
      <link>https://blog.fastforwardlabs.com/2017/09/26/the-product-possibilities-of-interpretability.html</link>
      
      <pubDate>Tue, 26 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/09/26/the-product-possibilities-of-interpretability.html</guid>
      <description>This post is part of a series highlighting the importance of interpretability. Previous posts include a video conversation on interpretability, a guide to using the LIME technique to predict whether couples will stay together, and a look at the business rationale. In our post on FairML, we used interpretability techniques to identify discriminatory bias in algorithms. As the use of machine learning algorithms increases, the need to understand them grows as well.</description>
    </item>
    
    <item>
      <title>Interpretability in conversation with Patrick Hall and Sameer Singh</title>
       
      <link>https://blog.fastforwardlabs.com/2017/09/11/interpretability-in-conversation-with-patrick-hall-and-sameer-singh.html</link>
      
      <pubDate>Mon, 11 Sep 2017 20:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/09/11/interpretability-in-conversation-with-patrick-hall-and-sameer-singh.html</guid>
      <description>We&amp;rsquo;re pleased to share the recording of our recent webinar on machine learning interpretability and accompanying resources.
We were joined by guests Patrick Hall (Senior Director for Data Science Products at H2o.ai, co-author of Ideas on Interpreting Machine Learning) and Sameer Singh (Assistant Professor of Computer Science at UC Irvine, co-creator of LIME).
We spoke for an hour and got lots of fantastic questions during that time. We didn&amp;rsquo;t have time to answer them all, so Patrick and Sameer have been kind enough to answer many of them below.</description>
    </item>
    
    <item>
      <title>To the Future...</title>
       
      <link>https://blog.fastforwardlabs.com/2017/09/07/to-the-future....html</link>
      
      <pubDate>Thu, 07 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/09/07/to-the-future....html</guid>
      <description>I started Fast Forward Labs more than three years ago with the vision of creating a new mechanism for applied research, helping businesses large and small understand how recently possible machine learning and applied artificial intelligence technologies could be useful for solving real business problems.
Since then, we’ve published reports, built prototypes, and advised organizations on how to think about their machine learning opportunities, strategically and technically. On the way, we built a profitable company with real impact on our clients’ products and businesses.</description>
    </item>
    
    <item>
      <title>Why your relationship is likely to last (or not): using Local Interpretable Model-Agnostic Explanations (LIME)</title>
       
      <link>https://blog.fastforwardlabs.com/2017/09/01/why-your-relationship-is-likely-to-last-or-not-using-local-interpretable-model-agnostic-explanations-lime.html</link>
      
      <pubDate>Fri, 01 Sep 2017 20:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/09/01/why-your-relationship-is-likely-to-last-or-not-using-local-interpretable-model-agnostic-explanations-lime.html</guid>
      <description>Henry VIII of England had many relationships. We build a classifier to predict whether relationships are going to last, or not, and used Local Interpretable Model-Agnostic Explanations (LIME) to understand the predicted success or failure of given relationships. Last month we launched the latest report and prototype from our machine intelligence R&amp;amp;D team, Interpretability, and we shared our view on why interpretability matters for business.
On September 6, we will host a public webinar on interpretability where we&amp;rsquo;ll be joined by guests Patrick Hall (Senior Director for Data Science Products at H2o.</description>
    </item>
    
    <item>
      <title>Crowdwork for Machine Learning: An Autoethnography</title>
       
      <link>https://blog.fastforwardlabs.com/2017/09/01/crowdwork-for-machine-learning-an-autoethnography.html</link>
      
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/09/01/crowdwork-for-machine-learning-an-autoethnography.html</guid>
      <description>Amazon’s Mechanical Turk is a platform for soliciting work on online tasks that has been used by market researchers, translators, and data scientists to complete surveys, perform work that cannot be easily automated, and create human-labeled data for supervised learning systems. Its namesake, the original Mechanical Turk, was an 18th-century chess-playing automaton gifted to the Austrian Empress Maria Theresa. An elaborate hoax, it concealed a human player amidst the clockwork machinery that appeared to direct each move on the board.</description>
    </item>
    
    <item>
      <title>Encartopedia</title>
       
      <link>https://blog.fastforwardlabs.com/2017/08/08/encartopedia.html</link>
      
      <pubDate>Tue, 08 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/08/08/encartopedia.html</guid>
      <description>The Tabula Rogeriana, a world map created by Muhammad al-Idrisi through traveler interviews in 1154. The Wikipedia corpus is one of the favorite datasets of the machine learning community. It is often used for experimenting, benchmarking and providing how-to examples. These experiments are generally presented separate from the Wikipedia user interface, however, which has remained true to the early hypertext vision of the web. For this experiment, Encartopedia, I used machine learning techniques and visualization to explore new navigation possibilities for Wikipedia while preserving its hypertextual feel.</description>
    </item>
    
    <item>
      <title>The Business Case for Machine Learning Interpretability</title>
       
      <link>https://blog.fastforwardlabs.com/2017/08/07/the-business-case-for-machine-learning-interpretability.html</link>
      
      <pubDate>Mon, 07 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/08/07/the-business-case-for-machine-learning-interpretability.html</guid>
      <description>Last week we launched the latest prototype and report from our machine intelligence R&amp;amp;D team: Interpretability.
Our prototype shows how new ideas in interpretability research can be used to extract actionable insights from black-box machine learning models; our report describes breakthroughs in interpretability research and places them in a commercial, legal and ethical context. This research is relevant to anyone who designs systems using machine learning, from engineers and data scientists to business leaders and executives who are considering new product opportunities.</description>
    </item>
    
    <item>
      <title>New Research on Interpretability</title>
       
      <link>https://blog.fastforwardlabs.com/2017/08/02/new-research-on-interpretability.html</link>
      
      <pubDate>Wed, 02 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/08/02/new-research-on-interpretability.html</guid>
      <description>We&amp;rsquo;re excited to release the latest prototype and report from our machine intelligence R&amp;amp;D team: Interpretability.
An interpretable algorithm is one whose decisions you can explain. You can better rely on such a model to be safe, accurate and useful.
Our prototype shows how new ideas in interpretability research can be used to extract actionable insight from black-box machine learning models.
And our report describes breakthroughs in interpretability research and places them in a commercial, legal and ethical context.</description>
    </item>
    
    <item>
      <title>Probabilistic programming from scratch</title>
       
      <link>https://blog.fastforwardlabs.com/2017/07/05/probabilistic-programming-from-scratch.html</link>
      
      <pubDate>Wed, 05 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/07/05/probabilistic-programming-from-scratch.html</guid>
      <description>This article contains highlights from a series of three interactive video tutorials on probabilistic programming from scratch published on O&amp;rsquo;Reilly Safari (login required). If you&amp;rsquo;re interested in the business case for probabilistic programming the Fast Forward Labs report discusses it in detail, and compares modern industrial strength systems like Stan and PyMC3. Please get in touch if you&amp;rsquo;re interested in working with us. This article is available as a Jupyter Notebook.</description>
    </item>
    
    <item>
      <title>F⁠ingerprinting documents​ with steganography​</title>
       
      <link>https://blog.fastforwardlabs.com/2017/06/25/fingerprinting-documents-with-steganography.html</link>
      
      <pubDate>Sun, 25 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/06/25/fingerprinting-documents-with-steganography.html</guid>
      <description>Steganography is the​ practice​ of​ hiding messages​ anywhere​ they’re​ not expected‏‎. I⁠n a well-executed​ piece of​ steganography, anyone who​ is​ not the intended​ recipient can​ look at​ the​ message​ and not​ realize its there at all‏‎. In a recent headline-making story, The I⁠ntercept inadvertently outed​ their​ source​ by publishing​ a document with​ an embedded steganographic​ message that allowed​ the NSA to identify the​ person​ who printed it‏‎.
These days, information is​ often​ hidden​ in digital media​ like images and​ audio​ files, where flipping a few bits​ doesn’t change​ the​ file to​ the​ human​ eye (or ear)‏‎.</description>
    </item>
    
    <item>
      <title>Probabilistic Programmming Sci-Fi: BayesHead 5000</title>
       
      <link>https://blog.fastforwardlabs.com/2017/06/06/probabilistic-programmming-sci-fi-bayeshead-5000.html</link>
      
      <pubDate>Tue, 06 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/06/06/probabilistic-programmming-sci-fi-bayeshead-5000.html</guid>
      <description>We believe in fiction as an important tool for imagining future relationships to technology. In our reports on new technologies we feature short science fiction stories that imagine the possible implications. The following story, written by Liam Sweeney (email), appeared in our Probabilistic Programming report. After you&amp;rsquo;re finished with this story be sure to check out its inspiration, George Saunders&amp;rsquo;s I Can Speak. Re: Inquiry on Limitations of the BayesHead 5000</description>
    </item>
    
    <item>
      <title>Slack Maestro: Helping Users Stay on Topic</title>
       
      <link>https://blog.fastforwardlabs.com/2017/05/30/slack-maestro-helping-users-stay-on-topic.html</link>
      
      <pubDate>Tue, 30 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/05/30/slack-maestro-helping-users-stay-on-topic.html</guid>
      <description>This is a guest post featuring a project by Andrej Ficnar (now a data scientist at Schireson Associates), which he completed as a fellow in the Insight Data Science program. We are partnered with Insight and occasionally advise fellows on month-long projects from idea to implementation. Slack is a popular messaging app that brings communication together in one place. It provides the abilities for messaging, archiving, and searching for teams, while organizing conversations into channels.</description>
    </item>
    
    <item>
      <title>Learning from Real-World Models: The Mississippi Basin Model and Machine Learning</title>
       
      <link>https://blog.fastforwardlabs.com/2017/05/26/learning-from-real-world-models-the-mississippi-basin-model-and-machine-learning.html</link>
      
      <pubDate>Fri, 26 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/05/26/learning-from-real-world-models-the-mississippi-basin-model-and-machine-learning.html</guid>
      <description>Model-building has a venerable history that long predates parallel processing, python, and perceptrons. Physical, scale models and maps have helped humans think through complex processes and ambitious endeavors, from the rigging of a pharoah&amp;rsquo;s pleasure yacht to the Battle of Britain by reducing the complexity of the real world to the most important, salient features that affect a problem, and by bringing disconnected, distant phenomena into view from a single perspective.</description>
    </item>
    
    <item>
      <title>A Quick Look at the Reply-to-Retweet Ratio</title>
       
      <link>https://blog.fastforwardlabs.com/2017/05/15/a-quick-look-at-the-reply-to-retweet-ratio.html</link>
      
      <pubDate>Mon, 15 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/05/15/a-quick-look-at-the-reply-to-retweet-ratio.html</guid>
      <description>Twitter users can retweet, like or reply to a tweet. If a tweet from a prominent account gets more replies than retweets then that&amp;rsquo;s usually Not Good.
Take, for example, this recent tweet from United Airlines. It got three times more replies than retweets. Those replies are not filled with praise, and that ratio was a sign to United&amp;rsquo;s PR team that they were in trouble, in case it wasn&amp;rsquo;t already obvious.</description>
    </item>
    
    <item>
      <title>Eli Bressert on Data-Driven Processes at Netflix</title>
       
      <link>https://blog.fastforwardlabs.com/2017/04/28/eli-bressert-on-data-driven-processes-at-netflix.html</link>
      
      <pubDate>Fri, 28 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/04/28/eli-bressert-on-data-driven-processes-at-netflix.html</guid>
      <description>Despite their best intentions, companies often struggle to develop processes that provide data-driven decisions through partnership across the business. Netflix stands out as a company that excels at deeply integrating its data science teams into all aspects of the company. We spoke with Eli Bressert, Manager, Data Engineering and Analytics at Netflix, to learn more about how they create the culture and processes to support and sustain that integration. Tell us about your background.</description>
    </item>
    
    <item>
      <title>Visualizing the Taste of a Community of Cinephiles Using t-SNE</title>
      
      <link>http://fastforwardlabs.github.io/cinephile_tsne/</link>
      
      <pubDate>Fri, 14 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/04/14/visualizing-the-taste-of-a-community-of-cinephiles-using-t-sne.html</guid>
      <description>In this post we are going to look at an interactive visualization that clusters movies together based on their ratings by a set of users. This visualization will give us a glimpse into the aesthetic tastes of a community of cinephiles.</description>
    </item>
    
    <item>
      <title>Free eBook: Development Workflows for Data Scientists</title>
       
      <link>https://blog.fastforwardlabs.com/2017/03/31/free-ebook-development-workflows-for-data-scientists.html</link>
      
      <pubDate>Fri, 31 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/03/31/free-ebook-development-workflows-for-data-scientists.html</guid>
      <description>Working with over 30 enterprise clients, in industries like financial services, insurance, publishing, and retail, the Fast Forward Labs team has had ample opportunity to observe the challenges of doing data science in practice. By now, most organizations have moved beyond traditional waterfall software development process to adopt more risk-tolerant and agile methodologies. But directly applying agile to data science can create friction, as data products require more leeway for experimentation and exploration, as well as open communication between business, science, and engineering teams.</description>
    </item>
    
    <item>
      <title>Five Distractions in Thinking about AI</title>
       
      <link>https://blog.fastforwardlabs.com/2017/03/25/five-distractions-in-thinking-about-ai.html</link>
      
      <pubDate>Sat, 25 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/03/25/five-distractions-in-thinking-about-ai.html</guid>
      <description>This post originally appeared on quamproxime.com, the personal blog of our sales and marketing lead, Kathryn Hume One of the main arguments the Israeli historian Yuval Noah Harari makes in Sapiens: A Brief History of Humankind is that mankind differs from other species because we can cooperate flexibly in large numbers, united in cause and spirit not by anything real, but by the fictions of our collective imagination. Examples of these fictions include gods, nations, money, and human rights, which are supported by religions, political structures, trade networks, and legal institutions, respectively.</description>
    </item>
    
    <item>
      <title>Taking Prophet for a Spin</title>
       
      <link>https://blog.fastforwardlabs.com/2017/03/22/taking-prophet-for-a-spin.html</link>
      
      <pubDate>Wed, 22 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/03/22/taking-prophet-for-a-spin.html</guid>
      <description>Facebook recently released Prophet, a general purpose time series forecasting package with both Python and R interfaces.
Python and R already have plenty of time series forecasting options, so why is Prophet interesting? It caught our eye because the backend is implemented in Stan, a probabilistic programming language we researched in our most recent report.
This choice means that Prophet offers many of the advantages of the Bayesian approach. In particular, the models have a simple, interpretable structure (seasonality) on which prior analyst knowledge can be imposed, and forecasts include confidence intervals derived from the full posterior distribution, which means they offer a data-driven estimate of risk.</description>
    </item>
    
    <item>
      <title>Predicting NYC Real Estate Prices with Probabilistic Programming</title>
       
      <link>https://blog.fastforwardlabs.com/2017/03/15/predicting-nyc-real-estate-prices-with-probabilistic-programming.html</link>
      
      <pubDate>Wed, 15 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/03/15/predicting-nyc-real-estate-prices-with-probabilistic-programming.html</guid>
      <description>Probabilistic Real Estate is a prototype we built to explore the New York City real estate market. As explained in a previous post, we used probabilistic programming&amp;rsquo;s ability to incorporate hierarchical models to make predictions across neighborhoods with sparse amounts of pricing data. In this post, we&amp;rsquo;ll focus on how we designed the prototype to capitalize on another strength of probabilistic programming: the ability to generate probability distributions.
Prototype Overview: Price Mode In price mode, Probabilistic Real Estate shows the median price for each neighborhood.</description>
    </item>
    
    <item>
      <title>Practical Deep Learning</title>
       
      <link>https://blog.fastforwardlabs.com/2017/03/13/practical-deep-learning.html</link>
      
      <pubDate>Mon, 13 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/03/13/practical-deep-learning.html</guid>
      <description>Micha&amp;rsquo;s talk demystifies deep learning. View the slides. Our research team spent last week in London hosting sessions and workshops about applied machine learning at the QCon conference. Micha Gorelick gave a talk about building a working product with Keras, a high level deep learning framework. He started by explaining deep learning at a conceptual level (describing neural networks like universal approximation theorems to approximate arbitrary functions by iteratively tuning weights and biases on training data) and then showed the code and design decisions we used to train and deploy our model for automated text summarization.</description>
    </item>
    
    <item>
      <title>Trees, Layers, and Speed: Talking Optimization with Patrick Hayes</title>
       
      <link>https://blog.fastforwardlabs.com/2017/03/10/trees-layers-and-speed-talking-optimization-with-patrick-hayes.html</link>
      
      <pubDate>Fri, 10 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/03/10/trees-layers-and-speed-talking-optimization-with-patrick-hayes.html</guid>
      <description>In a modern twist on Claude Shannon&amp;rsquo;s Theseus, SigOpt explains optimization by teaching a mouse to solve a randomly generated maze The learning of machine learning refers to the process of updating and tuning the parameters of a model. For example, if we take the function f(x) = ax^2 + bx + c, learning would mean to change the values of a, b, and c so that our function does a better job describing our data.</description>
    </item>
    
    <item>
      <title>FairML: Auditing Black-Box Predictive Models</title>
       
      <link>https://blog.fastforwardlabs.com/2017/03/09/fairml-auditing-black-box-predictive-models.html</link>
      
      <pubDate>Thu, 09 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/03/09/fairml-auditing-black-box-predictive-models.html</guid>
      <description>Machine learning models are used for important decisions like determining who has access to bail. The aim is to increase efficiency and spot patterns in data that humans would otherwise miss. But how do we know if a machine learning model is fair? And what does fairness in machine learning mean?
In this post, we&amp;rsquo;ll explore these questions using FairML, a new Python library that audits black-box predictive models and is based on work I did with my advisor while at MIT.</description>
    </item>
    
    <item>
      <title>Mobile Behavioural Authentication</title>
       
      <link>https://blog.fastforwardlabs.com/2017/02/27/mobile-behavioural-authentication.html</link>
      
      <pubDate>Mon, 27 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/02/27/mobile-behavioural-authentication.html</guid>
      <description>As mobile devices become central to our personal and professional lives, their security is more and more important. Passcodes in particular can be lost (or forcibly surrendered) to law enforcement. Recent research has focussed on behavioural authentication based on patterns of user interaction. This could provide an unintrusive authentication method that operates during normal use.
Figure from Touchalytics: On the Applicability of Touchscreen Input as a Behavioral Biometric for Continuous Authentication Research in this field addresses two problems.</description>
    </item>
    
    <item>
      <title>Online Talk: Introduction to Probabilistic Programming</title>
       
      <link>https://blog.fastforwardlabs.com/2017/02/09/online-talk-introduction-to-probabilistic-programming.html</link>
      
      <pubDate>Thu, 09 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/02/09/online-talk-introduction-to-probabilistic-programming.html</guid>
      <description>On Tuesday, we hosted an online talk with the Stan Group discussing why probabilistic programming is generating so much excitement in the fields of machine learning and statistics. In essence, probabilistic programming is a powerful tool to help organizations make rational decisions under uncertainty.
Watching the recording, you will learn:
 What kinds of problems are a good fit for Bayesian inference How a model-centric approach changes data science workflows How to use probabilistic programming for revenue models What Hamiltonian Monte Carlo is and why it&amp;rsquo;s tricky to use What Stan is, what it does well, and what its limitations are What we’re excited about in the near future  This online talk part of a series of educational resources accompanying our recent research on probabilistic programming.</description>
    </item>
    
    <item>
      <title>The Algorithms Behind Probabilistic Programming</title>
       
      <link>https://blog.fastforwardlabs.com/2017/01/30/the-algorithms-behind-probabilistic-programming.html</link>
      
      <pubDate>Mon, 30 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/01/30/the-algorithms-behind-probabilistic-programming.html</guid>
      <description>We recently introduced our report on probabilistic programming. The accompanying prototype allows you to explore the past and future of the New York residential real estate market.
This post gives a feel for the content in our report by introducing the algorithms and technology that make probabilistic programming possible. We&amp;rsquo;ll dive even deeper into these algorithms in conversation with the Stan Group Tuesday, February 7 at 1 pm ET/10am PT. Please join us!</description>
    </item>
    
    <item>
      <title>Privacy and Encryption Above the Data: Interview with Dave Archer</title>
       
      <link>https://blog.fastforwardlabs.com/2017/01/25/privacy-and-encryption-above-the-data-interview-with-dave-archer.html</link>
      
      <pubDate>Wed, 25 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/01/25/privacy-and-encryption-above-the-data-interview-with-dave-archer.html</guid>
      <description>Our mission at Fast Forward Labs is to commercialize artificial intelligence research, to clearly explain how new technologies work and help enterprises apply them in practical products. As our business model is rare, we&amp;rsquo;re always keen to connect with and learn from organizations that straddle this same line between research and application. That&amp;rsquo;s why we&amp;rsquo;re delighted to feature Galois, a Portland, Oregon-based R&amp;amp;D firm that specializes in security and trustworthiness, in this blog post.</description>
    </item>
    
    <item>
      <title>New Research on Probabilistic Programming</title>
       
      <link>https://blog.fastforwardlabs.com/2017/01/18/new-research-on-probabilistic-programming.html</link>
      
      <pubDate>Wed, 18 Jan 2017 10:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/01/18/new-research-on-probabilistic-programming.html</guid>
      <description>We&amp;rsquo;re excited to release the latest research from our machine intelligence R&amp;amp;D team! This report and prototype explore probabilistic programming, an emerging programming paradigm that makes it easier to construct and fit Bayesian inference models in code. It&amp;rsquo;s advanced statistics, simplified for data scientists looking to build models fast.
Bayesian inference has been popular in scientific research for a long time. The statistical technique allows us to encode expert knowledge into a model by stating prior beliefs about what we think our data looks like.</description>
    </item>
    
    <item>
      <title>Thomas Wiecki on Probabilistic Programming with PyMC3</title>
       
      <link>https://blog.fastforwardlabs.com/2017/01/11/thomas-wiecki-on-probabilistic-programming-with-pymc3.html</link>
      
      <pubDate>Wed, 11 Jan 2017 14:23:11 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/01/11/thomas-wiecki-on-probabilistic-programming-with-pymc3.html</guid>
      <description>A rolling regression with PyMC3: instead of the regression coefficients being constant over time (the points are daily stock prices of 2 stocks), this model assumes they follow a random-walk and can thus slowly adapt them over time to fit the data best. Probabilistic programming is coming of age. While normal programming languages denote procedures, probabilistic programming languages denote models and perform inference on these models. Users write code to specify a model for their data, and the languages run sampling algorithms across probability distributions to output answers with confidence rates and levels of uncertainty across a full distribution.</description>
    </item>
    
    <item>
      <title>Five 2016 Trends We Expect to Come to Fruition in 2017</title>
       
      <link>https://blog.fastforwardlabs.com/2017/01/10/five-2016-trends-we-expect-to-come-to-fruition-in-2017.html</link>
      
      <pubDate>Tue, 10 Jan 2017 15:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/01/10/five-2016-trends-we-expect-to-come-to-fruition-in-2017.html</guid>
      <description>The start of a new year is an excellent occasion for audacious extrapolation. Based on 2016 developments, what do we expect for 2017?
This blog post covers five prominent trends: Deep Learning Beyond Cats, Chat Bots - Take Two, All the News In The World - Turning Text Into Action, The Proliferation of Data Roles, and _What Are You Doing to My Data? _
(1) Deep Learning Beyond Cats In 2012, Google found cats on the internet using deep neural networks.</description>
    </item>
    
    <item>
      <title>Learning to Use React</title>
       
      <link>https://blog.fastforwardlabs.com/2017/01/03/learning-to-use-react.html</link>
      
      <pubDate>Tue, 03 Jan 2017 15:00:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2017/01/03/learning-to-use-react.html</guid>
      <description>React and Redux helped us keep application state manageable in our probabilistic programming prototypes For every topic we research at Fast Forward Labs, we create prototypes to show how the technology can be applied to make great products. Finite, stand-alone projects, our prototype web applications are great opportunities to experiment with new front-end tech.
In our latest report on probabilistic programming, I used the React javascript library to create the interface with Redux for managing the data state of UI components.</description>
    </item>
    
    <item>
      <title>Hilary Mason at Data Driven NYC</title>
       
      <link>https://blog.fastforwardlabs.com/2016/12/13/hilary-mason-at-data-driven-nyc.html</link>
      
      <pubDate>Tue, 13 Dec 2016 13:02:12 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/12/13/hilary-mason-at-data-driven-nyc.html</guid>
      <description>&lt;figure data-orig-width=&#34;1046&#34; data-orig-height=&#34;478&#34; class=&#34;tmblr-full&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/706a85da781f730b2aa59f0dc0e965c4/tumblr_inline_oi4x9i4yhD1ta78fg_540.png&#34; alt=&#34;image&#34; data-orig-width=&#34;1046&#34; data-orig-height=&#34;478&#34;/&gt;&lt;/figure&gt;&lt;p&gt;Hilary Mason, Fast Forward Labs Founder &amp;amp; CEO, gave &lt;a href=&#34;http://firstmarkcap.com/insights/a-process-for-discovery/&#34;&gt;a talk&lt;/a&gt; at November’s &lt;a href=&#34;http://www.meetup.com/DataDrivenNYC/&#34;&gt;Data Driven NYC&lt;/a&gt; Meetup. &lt;a href=&#34;http://firstmarkcap.com/insights/a-process-for-discovery/&#34;&gt;Check it out&lt;/a&gt; to hear our thoughts on:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Machines in Conversation</title>
       
      <link>https://blog.fastforwardlabs.com/2016/12/12/machines-in-conversation.html</link>
      
      <pubDate>Mon, 12 Dec 2016 17:47:22 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/12/12/machines-in-conversation.html</guid>
      <description>We at Fast Forward Labs have long been interested in speech recognition technologies. This year’s chatbot craze has seen growing interest in machines that interface with users in friendly, accessible language. Bots, however, only rarely understand the complexity of colloquial conversation: many practical customer service bots are trained on a very constrained set of queries (”I lost my password”). That’s why we’re excited to highlight Gridspace, a San Francisco-based startup that provides products, services, and an easy-to-use API focused on making human-to-human conversation analyzable by machines.</description>
    </item>
    
    <item>
      <title>Dimensionality Reduction and Intuition</title>
       
      <link>https://blog.fastforwardlabs.com/2016/12/08/dimensionality-reduction-and-intuition.html</link>
      
      <pubDate>Thu, 08 Dec 2016 17:22:58 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/12/08/dimensionality-reduction-and-intuition.html</guid>
      <description>&lt;figure data-orig-width=&#34;651&#34; data-orig-height=&#34;513&#34; class=&#34;tmblr-full&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/a97d30b7577d5fb32955853cb2ec7b17/tumblr_inline_ohvie7chq81ta78fg_540.png&#34; alt=&#34;image&#34; data-orig-width=&#34;651&#34; data-orig-height=&#34;513&#34;/&gt;&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;“I call our world Flatland, not because we call it so, but to make its nature clearer to you, my happy readers, who are privileged to live in Space.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So reads the first sentence of Edwin Abbott Abbott’s 1884 work of science fiction and social satire, &lt;i&gt;&lt;a href=&#34;http://www.gutenberg.org/ebooks/201&#34;&gt;Flatland: A Romance of Many Dimensions&lt;/a&gt;.&lt;/i&gt; At the time, Abbott used contemporary developments in the fields of geometry and topology (he was a contemporary of &lt;a href=&#34;https://en.wikipedia.org/wiki/Henri_Poincar%C3%A9&#34;&gt;Poincaré&lt;/a&gt;) to illustrate the rigid social hierarchies in Victorian England. A century later, with machine learning algorithms playing an increasingly prominent role in our daily lives, Abbott’s play on the conceptual leaps required to cross dimensions is relevant again. This time, however, the dimensionality shifts lie not between two human social classes, but between the domains of human reasoning and intuition and machine reasoning and computation.&lt;/p&gt;
&lt;p&gt;Much of the recent excitement around artificial intelligence stems from the fact that computers are newly able to process data historically too complex to analyze. At Fast Forward Labs, we’ve been excited by new capabilities to use computers to &lt;a href=&#34;http://pictograph.us&#34;&gt;perceive objects in images&lt;/a&gt;, extract the &lt;a href=&#34;http://fastforwardlabs.github.io/brief/&#34;&gt;most important sentences from long bodies of text&lt;/a&gt;, and &lt;a href=&#34;https://research.googleblog.com/2016/11/zero-shot-translation-with-googles.html&#34;&gt;translate between languages&lt;/a&gt;. But making complex data like images or text tractable for machines involves representing the data in high-dimensional vectors, long strings of numbers that encode the complexity of pixel clusters or relationships between words. The problem is these vectors become so large that it’s hard for humans to make sense of them: plotting them often requires a space of way more than the three dimensions we live in and perceive!&lt;/p&gt;&lt;p&gt;On the other hand, machine learning techniques that entirely remove humans from the loop, like &lt;a href=&#34;http://www.automl.org/&#34;&gt;automatic machine learning&lt;/a&gt; and unsupervised learning, are still active areas of research. For now, machines perform best when nudged by humans. And that means we need a way to reverse engineer the high-dimensionality vectors machines compute in back down to the two and three dimensional spaces our visual systems have evolved to make sense of. &lt;/p&gt;&lt;p&gt;What follows is a brief survey of some tools available to reduce and visualize high-dimensional data. Send us a note at contact@fastforwardlabs.com if you know of others!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Probabilistic Data Structure Showdown: Cuckoo Filters vs. Bloom Filters</title>
       
      <link>https://blog.fastforwardlabs.com/2016/11/23/probabilistic-data-structure-showdown-cuckoo-filters-vs.-bloom-filters.html</link>
      
      <pubDate>Wed, 23 Nov 2016 18:36:58 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/11/23/probabilistic-data-structure-showdown-cuckoo-filters-vs.-bloom-filters.html</guid>
      <description>Probabilistic data structures store data compactly with low memory and provide approximate answers to queries about stored data. They are designed to answer queries in a space-efficient manner, which can mean sacrificing accuracy. However, they typically provide guarantees and bounds on error rates depending on specifications of the data structure in question. Because they provide low memory footprints, probabilisitic data structures are particularly useful ink streaming and low power settings.</description>
    </item>
    
    <item>
      <title>Exploring Deep Learning on Satellite Data</title>
       
      <link>https://blog.fastforwardlabs.com/2016/08/26/exploring-deep-learning-on-satellite-data.html</link>
      
      <pubDate>Fri, 26 Aug 2016 17:43:24 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/08/26/exploring-deep-learning-on-satellite-data.html</guid>
      <description>This is a guest post featuring a project Patrick Doupe, now a Senior Data Analyst at Icahn School of Medicine at Mount Sinai, completed as a fellow in the Insight Data Science program. In our partnership with Insight, we occassionally advise fellows on month-long projects and how to build a career in data science. Machines are getting better at identifying objects in images. These technologies are used to do more than organise your photos or chat your family and friends with snappy augmented pictures and movies.</description>
    </item>
    
    <item>
      <title>New TensorFlow Code for Text Summarization</title>
       
      <link>https://blog.fastforwardlabs.com/2016/08/25/new-tensorflow-code-for-text-summarization.html</link>
      
      <pubDate>Thu, 25 Aug 2016 17:24:14 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/08/25/new-tensorflow-code-for-text-summarization.html</guid>
      <description>Yesterday, Google released new TensorFlow model code for text summarization, specifically for generating news headlines on the Annotated English Gigaword dataset. We’re excited to see others working on summarization, as we did in our last report: our ability to “digest large amounts of information in a compressed form” will only become more important as unstructured information grows. The TensorFlow release uses sequence-to-sequence learning to train models that write headlines for news articles.</description>
    </item>
    
    <item>
      <title>Next Economics: Interview with Jimi Crawford</title>
       
      <link>https://blog.fastforwardlabs.com/2016/08/24/next-economics-interview-with-jimi-crawford.html</link>
      
      <pubDate>Wed, 24 Aug 2016 16:18:12 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/08/24/next-economics-interview-with-jimi-crawford.html</guid>
      <description>&lt;figure class=&#34;tmblr-full&#34; data-orig-height=&#34;656&#34; data-orig-width=&#34;1629&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/de9fc93bc4bda585c1bb51a785fc7801/tumblr_inline_ocf6npUoHK1ta78fg_540.png&#34; data-orig-height=&#34;656&#34; data-orig-width=&#34;1629&#34;/&gt;&lt;/figure&gt;&lt;figure class=&#34;tmblr-full&#34; data-orig-height=&#34;923&#34; data-orig-width=&#34;2291&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/e65b096c51449dd8ad7ef4b2f0e4ad10/tumblr_inline_ocf6k5SHv61ta78fg_540.png&#34; data-orig-height=&#34;923&#34; data-orig-width=&#34;2291&#34;/&gt;&lt;/figure&gt;
&lt;h5 id=&#34;building-shadows-as-proxies-for-construction-rates-in-shanghai-photos-courtesy-of-orbital-insightdigital-globe&#34;&gt;Building shadows as proxies for construction rates in Shanghai. Photos courtesy of Orbital Insight/Digital Globe.&lt;/h5&gt;
&lt;p&gt;It’s no small feat to commercialize new technologies that arise from scientific and academic research. The useful is a small subset of the possible, and the features technology users (let alone corporate buyers) care about rarely align with the problems researchers want to solve. But it’s immensely exciting when it works. When the phase transition is complete. When the general public starts to appreciate how a bunch of mathematics can impact their business, their lives, and their understanding of how the world works. It’s why the Fast Forward Labs team wakes up every day. It’s why we love what we do. It drives us. And it’s why we’re always on the lookout for people who are doing it well. &lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://orbitalinsight.com/&#34;&gt;Orbital Insight&lt;/a&gt; is an excellent example of a company that is successfully commercializing deep learning technologies. 2015 saw a &lt;a href=&#34;http://www.nytimes.com/2015/12/11/science/an-advance-in-artificial-intelligence-rivals-human-vision-abilities.html?_r=0&#34;&gt;series of improvements&lt;/a&gt; in the performance of object recognition and computer vision systems. The technology is being applied across domains, to &lt;a href=&#34;http://www.enlitic.com/&#34;&gt;improve medical diagnosis&lt;/a&gt;, gain &lt;a href=&#34;https://www.clarifai.com/&#34;&gt;brand insights&lt;/a&gt;, or update our &lt;a href=&#34;http://pictograph.us&#34;&gt;social media experience&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;Building on his experience at The Climate Corporation, Orbital Insight CEO  &amp;amp; Founder &lt;a href=&#34;https://www.linkedin.com/in/jmcrawfordjr&#34;&gt;Jimi Crawford&lt;/a&gt; decided to aim big and apply the latest in computer vision to satellite imagery. His team focused their first commercial offering on the financial services industry, honing their tools to count cars in parking lots to infer company performance and, transitively, stock market behavior. But hedge funds are just the beginning. Crawford’s long-term ambition (as that of &lt;a href=&#34;http://www.featurex.ai/&#34;&gt;FeatureX&lt;/a&gt;) is to reform macroeconomics, to replace government reports with quantified observations about the physical world. &lt;a href=&#34;https://techcrunch.com/2016/06/27/orbital-insight-lands-20-million-from-investors-led-by-gv/&#34;&gt;Investors have taken notice&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;We interviewed Jimi, discussing what he learned in the past, what he does in the present, and what he envisions for the future. Read on for highlights. &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Under the Hood of the Variational Autoencoder (in Prose and Code)</title>
       
      <link>https://blog.fastforwardlabs.com/2016/08/22/under-the-hood-of-the-variational-autoencoder-in-prose-and-code.html</link>
      
      <pubDate>Mon, 22 Aug 2016 18:02:08 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/08/22/under-the-hood-of-the-variational-autoencoder-in-prose-and-code.html</guid>
      <description>&lt;h5 id=&#34;the-a-hrefhttpsarxivorgabs13126114variationala-a-hrefhttpsarxivorgabs14014082autoencodera-vae-neatly-synthesizes-unsupervised-deep-learning-and-variational-bayesian-methods-into-one-sleek-package-in-a-hrefhttpblogfastforwardlabscom20160812introducing-variational-autoencoders-in-prose-andhtmlpart-ia-of-this-series-we-introduced-the-theory-and-intuition-behind-the-vae-an-exciting-development-in-machine-learning-for-combined-generative-modeling-and-inferencea-hrefhttpshakirmcomslidesdlsummerschool_aug2016_compresspdfmachines-that-imagine-and-reasona&#34;&gt;The &lt;a href=&#34;https://arxiv.org/abs/1312.6114&#34;&gt;Variational&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/1401.4082&#34;&gt;Autoencoder&lt;/a&gt; (VAE) neatly synthesizes unsupervised deep learning and variational Bayesian methods into one sleek package. In &lt;a href=&#34;http://blog.fastforwardlabs.com/2016/08/12/introducing-variational-autoencoders-in-prose-and.html&#34;&gt;Part I&lt;/a&gt; of this series, we introduced the theory and intuition behind the VAE, an exciting development in machine learning for combined generative modeling and inference—&lt;a href=&#34;http://shakirm.com/slides/DLSummerSchool_Aug2016_compress.pdf&#34;&gt;“machines that imagine and reason.”&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;To recap: VAEs put a probabilistic spin on the basic autoencoder paradigm—treating their inputs, hidden representations, and reconstructed outputs as probabilistic random variables within a directed graphical model. With this &lt;a href=&#34;https://xkcd.com/1236/&#34;&gt;Bayesian&lt;/a&gt; perspective, the encoder becomes a variational &lt;em&gt;inference network&lt;/em&gt;, mapping observed inputs to (approximate) posterior distributions over latent space, and the decoder becomes a &lt;em&gt;generative network&lt;/em&gt;, capable of mapping arbitrary latent coordinates back to distributions over the original data space.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://fastforwardlabs.github.io/blog-images/miriam/imgs_code/vae.4.png&#34; title=&#34;A variational autoencoder&#34; style=&#34;width:80.0%&#34;/&gt;&lt;/div&gt;
&lt;p&gt;The beauty of this setup is that we can take a principled Bayesian approach toward building systems with a rich internal “mental model” of the observed world, all by training a single, cleverly-designed deep neural network.&lt;/p&gt;
&lt;p&gt;These benefits derive from an enriched understanding of data as merely the tip of the iceberg—the observed result of an underlying causative probabilistic process.&lt;/p&gt;
&lt;p&gt;The power of the resulting model is captured by Feynman’s famous &lt;a href=&#34;http://archives-dc.library.caltech.edu/islandora/object/ct1:483&#34;&gt;chalkboard quote&lt;/a&gt;: “What I cannot create, I do not understand.” When trained on MNIST handwritten digits, our VAE model can parse the information spread thinly over the high-dimensional observed world of pixels, and condense the most meaningful features into a structured distribution over reduced latent dimensions.&lt;/p&gt;
&lt;p&gt;Having recovered the latent manifold and assigned it a coordinate system, it becomes trivial to walk from one point to another along the manifold, creatively generating realistic digits all the while:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://fastforwardlabs.github.io/blog-images/miriam/imgs_code/160816_1754_reloaded_latent_784_500_500_50_round_65536_morph_4730816952.gif&#34; title=&#34;Generatively morphing digits&#34;/&gt;&lt;/div&gt;
&lt;p&gt;In this post, we’ll take a look under the hood at the math and technical details that allow us to optimize the VAE model we sketched in &lt;a href=&#34;http://fastforwardlabs.github.io/2016/08/12/introducing-variational-autoencoders-in-prose-and.html&#34;&gt;Part I&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Along the way, we’ll show how to implement a VAE in &lt;a href=&#34;http://tensorflow.org/&#34;&gt;TensorFlow&lt;/a&gt;—a library for efficient numerical computation using data flow graphs, with key features like &lt;a href=&#34;http://alexey.radul.name/ideas/2013/introduction-to-automatic-differentiation/&#34;&gt;automatic differentiation&lt;/a&gt; and parallelizability (across clusters, CPUs, GPUs…and &lt;a href=&#34;https://cloudplatform.googleblog.com/2016/05/Google-supercharges-machine-learning-tasks-with-custom-chip.html&#34;&gt;TPUs&lt;/a&gt; if you’re lucky). You can find (and tinker with!) the full implementation &lt;a href=&#34;https://github.com/fastforwardlabs/vae-tf/tree/master&#34;&gt;here&lt;/a&gt;, along with a couple &lt;a href=&#34;https://github.com/fastforwardlabs/vae-tf/tree/master/out&#34;&gt;pre-trained models&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Giving Speech a Voice in the Home</title>
       
      <link>https://blog.fastforwardlabs.com/2016/08/18/giving-speech-a-voice-in-the-home.html</link>
      
      <pubDate>Thu, 18 Aug 2016 16:09:29 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/08/18/giving-speech-a-voice-in-the-home.html</guid>
      <description>This is a guest post by Sean Lorenz, the Founder &amp;amp; CEO of SENTER, a Boston-based startup using sensors and data science to support healthcare in the home. Sean explains how techniques from computational neuroscience can help make the smart home smarter and describes the speech recognition hurdles developers have to overcome to realize smart home potential. Consumer IoT pundits rave about the “smart home,” where our lights, shades, sprinklers and coffeemakers do what we want them to do automatically as they learn about our behaviors and habits.</description>
    </item>
    
    <item>
      <title>Introducing Variational Autoencoders (in Prose and Code)</title>
       
      <link>https://blog.fastforwardlabs.com/2016/08/12/introducing-variational-autoencoders-in-prose-and-code.html</link>
      
      <pubDate>Fri, 12 Aug 2016 17:09:50 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/08/12/introducing-variational-autoencoders-in-prose-and-code.html</guid>
      <description>Effective machine learning means building expressive models that sift out signal from noise—that simplify the complexity of real-world data, yet accurately intuit and capture its subtle underlying patterns.
Whatever the downstream application, a primary challenge often boils down to this: How do we represent, or even synthesize, complex data in the context of a tractable model?
This challenge is compounded when working in a limited data setting—especially when samples are in the form of richly-structured, high-dimensional observations like natural images, audio waveforms, or gene expression data.</description>
    </item>
    
    <item>
      <title>Late Summer Reading List</title>
       
      <link>https://blog.fastforwardlabs.com/2016/07/27/late-summer-reading-list.html</link>
      
      <pubDate>Wed, 27 Jul 2016 16:45:52 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/07/27/late-summer-reading-list.html</guid>
      <description>It’s July 27. 89 degrees with high humidity on the sweltering New York City streets. We’re hard at work on our probabilistic programming report and prototype and looking forward to some good reads on the beach. Here are our recommendations for the dog days of summer!
Homage to a great physicist, in a textbook: David MacKay, a Cambridge physicist, influenced the machine learning field by fusing together Bayesian methods with artificial neural nets.</description>
    </item>
    
    <item>
      <title>Two Talks in NYC this Saturday</title>
       
      <link>https://blog.fastforwardlabs.com/2016/07/11/two-talks-in-nyc-this-saturday.html</link>
      
      <pubDate>Mon, 11 Jul 2016 12:46:23 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/07/11/two-talks-in-nyc-this-saturday.html</guid>
      <description>If you’re a New Yorker looking for something interesting to do Saturday afternoon, Fast Forward Labs will be speaking at two events. PyGotham | July 16 | 2:15 pm Mike Williams on Text Summarization PyGotham is a conference for the Python developer community. Mike’s talk focuses on text summarization: taking some text in and returning a shorter document that contains the same information. Mike will use libraries like scikit-learn and keras, explain how to choose between different approaches, and show working prototypes.</description>
    </item>
    
    <item>
      <title>What We Liked at AINow</title>
       
      <link>https://blog.fastforwardlabs.com/2016/07/08/what-we-liked-at-ainow.html</link>
      
      <pubDate>Fri, 08 Jul 2016 17:29:52 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/07/08/what-we-liked-at-ainow.html</guid>
      <description>Over the past two months, the White House ran a series of workshops geared to “prepare for the future of artificial intelligence.” At each session, technologists, academics, policy makers, and social scientists discussed social and economic issues stemming from data technologies, which will feed into the development of a public report later this year (likely similar to the FTC’s January report on the social impact of big data). The last session took place yesterday in NYC.</description>
    </item>
    
    <item>
      <title>Machine Listening: Interview with Juan Pablo Bello</title>
       
      <link>https://blog.fastforwardlabs.com/2016/06/10/machine-listening-interview-with-juan-pablo-bello.html</link>
      
      <pubDate>Fri, 10 Jun 2016 14:23:11 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/06/10/machine-listening-interview-with-juan-pablo-bello.html</guid>
      <description>&lt;figure data-orig-width=&#34;360&#34; data-orig-height=&#34;270&#34; class=&#34;tmblr-full&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/5cfcc6b830aa09f63bf9e9b2333524a1/tumblr_inline_o8k7rqJDmA1ta78fg_540.png&#34; alt=&#34;image&#34; data-orig-width=&#34;360&#34; data-orig-height=&#34;270&#34;/&gt;&lt;/figure&gt;
&lt;h5 id=&#34;a-probabilistic-latent-component-analysis-of-a-pitch-class-sequence-for-the-beatles-good-day-sunshine-the-top-layer-shows-the-original-representation-time-vs-pitch-class-subsequent-layers-show-latent-components&#34;&gt;A probabilistic latent component analysis of a pitch class sequence for The Beatles’ Good Day Sunshine. The top layer shows the original representation (time vs pitch class). Subsequent layers show latent components.&lt;/h5&gt;
&lt;p&gt;What is music? Or rather, what differentiates music from noise?&lt;b&gt;&lt;br/&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;If you ask John Cage, “everything we do is music.” Forced to &lt;a href=&#34;https://www.youtube.com/watch?v=JTEFKFiXSx4&#34;&gt;sit silently for 4’33”&lt;/a&gt;, we masters of &lt;a href=&#34;https://en.wikipedia.org/wiki/Apophenia&#34;&gt;apophenia&lt;/a&gt; end up hearing music in noise (or just squirm in discomfort&amp;hellip;), perceiving order and meaning in sounds that normally escape notice. For Cage, music is in the ears of the listener. To study it is to study how we perceive.&lt;/p&gt;&lt;p&gt;But Cage wrote 4’33” at time when many artists were challenging inherited notions of art. Others, dating back to Pythagoras (who defined harmony in terms of &lt;a href=&#34;https://en.wikipedia.org/wiki/Pythagorean_hammers#cite_note-8&#34;&gt;ratios and proportions&lt;/a&gt;), have defined music through the structural properties that make music &lt;i&gt;music, &lt;/i&gt;and separate different musical styles. &lt;/p&gt;&lt;p&gt;The latest efforts to understand music lie in the field of machine listening, where researchers use computers to analyze audio data to identify meaning and structure in it like humans do. Some machine listening researchers analyze urban and environmental sounds, as at &lt;a href=&#34;https://wp.nyu.edu/sonyc/&#34;&gt;SONYC&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;This August in NYC, researchers in machine listening and related fields will convene at the &lt;a href=&#34;https://wp.nyu.edu/ismir2016/&#34;&gt;International Society for Music Information Retrieval (ISMIR) conference&lt;/a&gt;. The conference is of interest to anyone working in data or digital media, offering practical workshops and hackathons for the NYC data community. &lt;/p&gt;&lt;p&gt;We interviewed NYU Steinhardt Professor &lt;a href=&#34;http://steinhardt.nyu.edu/marl/people/bello&#34;&gt;Juan Pablo Bello&lt;/a&gt;, an organizer of ISMIR 2016 working in machine listening, to learn more about the conference and the latest developments in the field. Keep reading for highlights!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Welcome, Friederike!</title>
       
      <link>https://blog.fastforwardlabs.com/2016/06/02/welcome-friederike.html</link>
      
      <pubDate>Thu, 02 Jun 2016 13:53:20 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/06/02/welcome-friederike.html</guid>
      <description>Alongside our research on emerging machine learning techniques, Fast Forward Labs advises organizations on data science issues like technical architecture, new product development, models and algorithms, and even hiring the right talent. Our client base is growing fast, and we’re excited to support many large financial services, insurance, and media companies, as well as startups building cutting-edge AI products. We work closely with our clients to identify opportunities where they can apply our research and take our prototypes the next mile by implementing them on their own data.</description>
    </item>
    
    <item>
      <title>Online Talk: Summarization Algorithms</title>
       
      <link>https://blog.fastforwardlabs.com/2016/05/27/online-talk-summarization-algorithms.html</link>
      
      <pubDate>Fri, 27 May 2016 17:16:23 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/05/27/online-talk-summarization-algorithms.html</guid>
      <description>This week, we had the pleasure of hosting an online talk with the team from Agolo, an NYC-based startup with a great product that automatically summarizes articles (and can even generate new titles for a group of summarized articles). During the event, we discussed:
why summarization has suddenly become interestinghow different industries are applying the technologyhow algorithms work (LDA, word2vec, and recurrent neural networks)related applications (realtime translation, speech recognition)what’s next in the fieldCheck out the recording below!</description>
    </item>
    
    <item>
      <title>Human-Machine Algorithms: Interview with Eric Colson</title>
       
      <link>https://blog.fastforwardlabs.com/2016/05/25/human-machine-algorithms-interview-with-eric-colson.html</link>
      
      <pubDate>Wed, 25 May 2016 12:37:15 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/05/25/human-machine-algorithms-interview-with-eric-colson.html</guid>
      <description>&lt;figure data-orig-width=&#34;780&#34; data-orig-height=&#34;396&#34; class=&#34;tmblr-full&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/dd7cacb29c66b0d8ac385f37fde89d2d/tumblr_inline_o7qmpkkMD11qcg73w_540.png&#34; alt=&#34;image&#34; data-orig-width=&#34;780&#34; data-orig-height=&#34;396&#34;/&gt;&lt;/figure&gt;
&lt;blockquote&gt;Therefore render unto Caesar the things that are Caesar&amp;rsquo;s, and unto God the things that are God&amp;rsquo;s.&lt;/blockquote&gt;
&lt;h5 id=&#34;ndash-matthew-2221&#34;&gt;– Matthew, 22:21&lt;/h5&gt;
&lt;p&gt;We tend to think that recommender systems are old hat. ECommerce platforms like Amazon have been using techniques like &lt;a href=&#34;https://www.quora.com/How-does-Amazons-collaborative-filtering-recommendation-engine-work&#34;&gt;collaborative filtering&lt;/a&gt; for years to help shoppers navigate vast catalogues by inferring consumer taste from past behavior. And yet, we’ve all experienced the limitations of these approaches (that time you bought a toilet bowl plunger to subsequently be flooded by recommendations for strange bathroom accessories). This may be a nuisance for consumers, but it doesn’t jeopardize eCommerce business models: only 35% of Amazon’s sales, for example, are driven from recommendations.&lt;br/&gt;&lt;/p&gt;&lt;p&gt;But what would happen if the stakes were higher? What kind of algorithmic creativity would it take to build a company with revenue based entirely on recommendations?&lt;/p&gt;&lt;p&gt;This is the challenge faced by &lt;a href=&#34;https://twitter.com/ericcolson&#34;&gt;Eric Colson&lt;/a&gt;’s team at &lt;a href=&#34;http://multithreaded.stitchfix.com/&#34;&gt;Stitch Fix&lt;/a&gt;. Stitch Fix is an online “personal styling service” that selects clothing apparel for customers (they also have a &lt;a href=&#34;http://multithreaded.stitchfix.com/blog/&#34;&gt;solid technical blog&lt;/a&gt;). Instead of recommending items for shoppers to choose from, Stitch Fix goes a step further and simply ships items its algorithms and stylists choose to customers. The key to making this work, according to Colson, is to optimize the division of labor between human and machine.&lt;/p&gt;&lt;p&gt;Colson &lt;a href=&#34;https://www.youtube.com/watch?v=-rId42xabXY&#34;&gt;spoke about&lt;/a&gt; the value of human-machine collaborations at the March &lt;a href=&#34;http://datadrivennyc.com/&#34;&gt;Data Driven NYC Meetup&lt;/a&gt;. We interviewed him to learn more about the approach.&lt;br/&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Evaluating Summarization Systems</title>
       
      <link>https://blog.fastforwardlabs.com/2016/05/23/evaluating-summarization-systems.html</link>
      
      <pubDate>Mon, 23 May 2016 17:09:13 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/05/23/evaluating-summarization-systems.html</guid>
      <description>We’re excited for tomorrow’s online discussion about automatic text summarization! You can register here. During the event, Mohamed AlTantawy, CTO of Agolo, will explain the tech behind their summarization tool. In this blog post, he describes how his team evaluates the quality of summaries produced by their system. Join us tomorrow to learn more and ask questions about the approach! Evaluating the quality of algorithmically created summaries is a very hard task.</description>
    </item>
    
    <item>
      <title>May 24 Online Event: Text Summarization</title>
       
      <link>https://blog.fastforwardlabs.com/2016/05/12/may-24-online-event-text-summarization.html</link>
      
      <pubDate>Thu, 12 May 2016 07:36:14 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/05/12/may-24-online-event-text-summarization.html</guid>
      <description>Please join us May 24 at 1:00 pm EST/10:00 am PST for an online discussion about how recent breakthroughs in deep learning allow us to extract and process meaning from text. This opens up a vast range of applications: summarization, instant translation, semantic search, document clustering, and even speech recognition. We’ll be joined by Agolo, a startup focused on text summarization. You can register here. Making language computable has been a goal of computer science research for decades.</description>
    </item>
    
    <item>
      <title>Probabilistic Programming for Anomaly Detection</title>
       
      <link>https://blog.fastforwardlabs.com/2016/05/03/probabilistic-programming-for-anomaly-detection.html</link>
      
      <pubDate>Tue, 03 May 2016 14:51:14 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/05/03/probabilistic-programming-for-anomaly-detection.html</guid>
      <description>The Fast Forward Labs research team is developing our next prototype, which will demonstrate an application of probabilistic programming. Probabilistic programming languages are a set of high-level languages that lower the barrier to entry for Bayesian data analysis.
Bayesian data analysis is often seen as the best approach to machine learning. Models derived by this process are highly interpretable, in contrast to other modern models like neural networks and support vector machines.</description>
    </item>
    
    <item>
      <title>Making a Case for Machine Learning to Legal Departments</title>
       
      <link>https://blog.fastforwardlabs.com/2016/04/26/making-a-case-for-machine-learning-to-legal-departments.html</link>
      
      <pubDate>Tue, 26 Apr 2016 18:41:17 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/04/26/making-a-case-for-machine-learning-to-legal-departments.html</guid>
      <description>Since we released our text summarization resources, the legal technology community has shown interest in leveraging summarization technology to support litigation document review, deposition digests, and contract analysis. Data scientist interest to use machine learning to mine legal document corpuses and support legal strategy was also one factor motivating our summarization research. The time is therefore ripe for data scientists to apply new text analytics capabilities for legal use cases.</description>
    </item>
    
    <item>
      <title>Active Learning in the Law</title>
       
      <link>https://blog.fastforwardlabs.com/2016/04/25/active-learning-in-the-law.html</link>
      
      <pubDate>Mon, 25 Apr 2016 17:41:02 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/04/25/active-learning-in-the-law.html</guid>
      <description>&lt;figure data-orig-width=&#34;782&#34; data-orig-height=&#34;438&#34; class=&#34;tmblr-full&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/5419ce3290efb6634bdefc85748cbe2e/tumblr_inline_o67a1hhHOk1ta78fg_540.gif&#34; alt=&#34;image&#34; data-orig-width=&#34;782&#34; data-orig-height=&#34;438&#34;/&gt;&lt;/figure&gt;
&lt;h5 id=&#34;the-nist-text-retrieval-conference-trec-logo&#34;&gt;The NIST Text Retrieval Conference (TREC) Logo&lt;/h5&gt;
&lt;p&gt;We &lt;a href=&#34;http://blog.fastforwardlabs.com/2016/04/11/new-tools-to-summarize-text.html&#34;&gt;recently released research&lt;/a&gt; on neural network methods to summarize text. Systems like &lt;a href=&#34;http://fastforwardlabs.github.io/brief/&#34;&gt;Brief&lt;/a&gt;, our summarization prototype, are poised to modify how we consume text. Content systems were historically designed to help humans find, read, and research documents. But as electronically stored information continues to proliferate, systems will inevitably evolve to support research conducted by a man-machine partnership: machines will take the first pass to synthesize vast amounts of information, and humans will step in to derive more nuanced conclusions. &lt;/p&gt;&lt;p&gt;One field where this man-machine partnership is increasingly important is electronic discovery (eDiscovery), the processes lawyers use to discover information relevant for litigation. The traditional model, where armies of associates and paralegals read documents to find relevant information, does not scale to contemporary information volumes. To promote efficiency and curb the exorbitant costs of review, judges have issued opinions permitting the use of software that automatically finds evidence (&lt;a href=&#34;http://www.insidecounsel.com/2012/04/30/da-silva-moore-sets-stage-for-predictive-codings-a&#34;&gt;&lt;i&gt;Da Silva Moore&lt;/i&gt; v. &lt;i&gt;Publicis Groupe&lt;/i&gt;&lt;/a&gt;) and sometimes even requiring it (&lt;a href=&#34;https://e-discoveryteam.com/2012/10/25/news-flash-surprise-ruling-by-delaware-judge-orders-both-sides-to-use-predictive-coding/&#34;&gt;&lt;i&gt;EORHB Inc.&lt;/i&gt; v. &lt;i&gt;HOA Holdings LLC&lt;/i&gt;&lt;/a&gt;). &lt;/p&gt;&lt;p&gt;This shift in practice has created a large market for “technology-assisted review” (TAR) or “predictive coding” software. The industry is quite large, and vendors use TAR to refer to products that use rules-based Boolean logic as well as more advanced machine learning techniques. &lt;/p&gt;&lt;p&gt;But two voices in the space remain clear and constant. Since they met at the 2009 &lt;a href=&#34;http://trec.nist.gov/&#34;&gt;Text Retrieval Conference&lt;/a&gt; (TREC), &lt;a href=&#34;http://plg.uwaterloo.ca/~gvcormac/&#34;&gt;Gordon Cormack&lt;/a&gt; and &lt;a href=&#34;http://www.wlrk.com/MRGrossman/&#34;&gt;Maura Grossman&lt;/a&gt; have &lt;a href=&#34;http://cormack.uwaterloo.ca/cormack/calstudy/&#34;&gt;published multiple papers&lt;/a&gt; demonstrating that continuous active learning (CAL) is the most accurate and effective technique to find “substantially all” relevant information in a document collection. As they write in their recent &lt;a href=&#34;http://us.practicallaw.com/w-001-8253&#34;&gt;&lt;i&gt;Practical Law &lt;/i&gt;article&lt;/a&gt; on the topic:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;CAL is a method for finding substantially all relevant information on a particular subject within a vast sea of electronically stored information (ESI). At the outset, CAL resembles a web search engine, presenting first the documents that are most likely to be of interest, followed by those that are somewhat less likely to be of interest. Unlike a typical search engine, however, CAL repeatedly refines its understanding about which of the remaining documents are most likely to be of interest, based on the user’s feedback regarding the documents already presented. CAL continues to present documents, learning from user feedback, until none of the documents presented are of interest.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;We met with Cormack and Grossman to learn more about why they believe CAL is the best approach for eDiscovery and how the landscape is evolving. Keep reading for highlights.  &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Summarization as a Gateway to Computable Language</title>
       
      <link>https://blog.fastforwardlabs.com/2016/04/21/summarization-as-a-gateway-to-computable-language.html</link>
      
      <pubDate>Thu, 21 Apr 2016 19:38:37 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/04/21/summarization-as-a-gateway-to-computable-language.html</guid>
      <description>Analyzing unstructured text data such as news, emails, chats, narrative prose, legal documents, or transcribed speech is an extremely tough problem. Thanks to massive leaps in data engineering, we can just about store and retrieve this torrent of information. But we can&amp;rsquo;t yet conduct the kind of rich and fast analyses that we take for granted with structured, quantitative data.
Our newly released summarization report is a response to this problem in two senses.</description>
    </item>
    
    <item>
      <title>Assessing Data Science Maturity</title>
       
      <link>https://blog.fastforwardlabs.com/2016/04/15/assessing-data-science-maturity.html</link>
      
      <pubDate>Fri, 15 Apr 2016 18:07:33 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/04/15/assessing-data-science-maturity.html</guid>
      <description>We’re getting excited for our Data Leadership Conference, which is set for April 28 in New York City! The conference will feature an expert panel where Haile Owusu (Mashable), Claudia Perlich (Dstillery), and Kirk Borne (Booz Allen Hamilton) will share practical insights on how to build data capabilities within complex organizations. We’ll discuss questions like: What skills should organizations combine in data science teams?Why should organizations invest in experiments that may fail?</description>
    </item>
    
    <item>
      <title>New Tools to Summarize Text</title>
       
      <link>https://blog.fastforwardlabs.com/2016/04/11/new-tools-to-summarize-text.html</link>
      
      <pubDate>Mon, 11 Apr 2016 11:25:26 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/04/11/new-tools-to-summarize-text.html</guid>
      <description>We’re excited to introduce the latest report and prototype from our machine intelligence R&amp;amp;D group! In this iteration, we explore summarization, or neural network techniques for making unstructured text data computable.
Making language computable has been a goal of computer science research for decades. Historically, it has been a challenge to merely collect and store data. But it’s now so cheap to store data that we often have the opposite problem: once we’ve data, how should we analyze it to find meaning and insights?</description>
    </item>
    
    <item>
      <title>Where Do You Put Your Data Scientists?</title>
       
      <link>https://blog.fastforwardlabs.com/2016/04/06/where-do-you-put-your-data-scientists.html</link>
      
      <pubDate>Wed, 06 Apr 2016 15:41:33 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/04/06/where-do-you-put-your-data-scientists.html</guid>
      <description>This is a guest post by Daniel Tunkelang, a data scientist and engineering executive, to preview the keynote he’ll deliver at our April 28 Data Leadership Conference in New York City! In 2012, Harvard Business Review proclaimed that &amp;ldquo;data scientist&amp;rdquo; was the sexiest job of the 21st century. That&amp;rsquo;s pretty amazing, considering that the job title was less than five years old at the time.
The Rapid Rise of Data Science DJ Patil and Jeff Hammerbacher, then at LinkedIn and Facebook, coined the term in 2008 to capture the intersection of analytics, engineering, and product skills that was becoming critical to their companies.</description>
    </item>
    
    <item>
      <title>Shivon Zilis on the Machine Intelligence Landscape</title>
       
      <link>https://blog.fastforwardlabs.com/2016/03/30/shivon-zilis-on-the-machine-intelligence-landscape.html</link>
      
      <pubDate>Wed, 30 Mar 2016 15:00:03 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/03/30/shivon-zilis-on-the-machine-intelligence-landscape.html</guid>
      <description>&lt;figure data-orig-width=&#34;727&#34; data-orig-height=&#34;541&#34; class=&#34;tmblr-full&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/74ea2d9c04837d672993d887788dcd76/tumblr_inline_o4tb0bi8wF1ta78fg_540.png&#34; alt=&#34;image&#34; data-orig-width=&#34;727&#34; data-orig-height=&#34;541&#34;/&gt;&lt;/figure&gt;&lt;p&gt;2016 is shaping up to be a big year for machine intelligence. Achievements like &lt;a href=&#34;http://t.umblr.com/redirect?z=https%3A%2F%2Fdeepmind.com%2Falpha-go.html&amp;amp;t=OWFhNGUyZTg5YTZhMDA5YzMxMjhkZTcwZTA3ZjVkNTlkMTM5OWI5OCxXelB4R3dDVA%3D%3D&#34;&gt;DeepMind’s AlphaGo&lt;/a&gt; are making headlines in the popular press, large tech companies have started a “&lt;a href=&#34;http://t.umblr.com/redirect?z=http%3A%2F%2Fwww.nytimes.com%2F2016%2F03%2F26%2Ftechnology%2Fthe-race-is-on-to-control-artificial-intelligence-and-techs-future.html%3Faction%3Dclick%26contentCollection%3DTechnology%26module%3DRelatedCoverage%26region%3DEndOfArticle%26pgtype%3Darticle&amp;amp;t=NzFhYjEzNjJiMmY1NmNhZDFiMjQ2MWY0ZDQxYTc2MGUxODA2ZmZjNixXelB4R3dDVA%3D%3D&#34;&gt;platform war&lt;/a&gt;” to become the go-to company for A.I., and entrepreneurs are increasingly building machine learning products that have the potential to &lt;a href=&#34;http://t.umblr.com/redirect?z=http%3A%2F%2Ftechcrunch.com%2F2016%2F03%2F19%2Fhow-real-businesses-are-using-machine-learning%2F&amp;amp;t=MzRiYTE2MmVjYjBiN2UyZjQ3Zjc4YWRjOGI0ZjVmYmViODViZGY2OSxXelB4R3dDVA%3D%3D&#34;&gt;transform how companies operate&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;Exciting though the hype may be, the commercial potential of machine intelligence won’t be realized unless entrepreneurs and data scientists can clearly communicate the business value of new tools to non-technical executives. And the first step to communicating clearly is to define a vocabulary to think through what machine intelligence is, how the different algorithms work, and, most importantly, what practical benefits they can provide across verticals and industries. &lt;/p&gt;&lt;p&gt;&lt;a href=&#34;http://t.umblr.com/redirect?z=http%3A%2F%2Fwww.shivonzilis.com%2F&amp;amp;t=Y2UyNGUzNGI0YTMxYmNhYzFlNzQxMWJmMDMwZGI4YjBlN2ZmMTI3NCxXelB4R3dDVA%3D%3D&#34;&gt;Shivon Zilis&lt;/a&gt;, a partner and founding member of &lt;a href=&#34;http://t.umblr.com/redirect?z=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FBloomberg_Beta&amp;amp;t=NTM4NjA1ZDJkZWU2M2FjZTgzNDk0Nzk0NzkwZDdlNjdiYWQwYjRkNCxXelB4R3dDVA%3D%3D&#34;&gt;Bloomberg Beta&lt;/a&gt;, is doing just that. She has spent the past few years focused exclusively on machine intelligence, building out a vocabulary and taxonomy to help the community understand activity in the field and communicate new developments clearly and effectively. &lt;/p&gt;&lt;p&gt;We interviewed Zilis to learn her views on the past, present, and future of machine intelligence. Keep reading for highlights!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Fast Forward Labs Data Leadership Conference</title>
       
      <link>https://blog.fastforwardlabs.com/2016/03/28/fast-forward-labs-data-leadership-conference.html</link>
      
      <pubDate>Mon, 28 Mar 2016 16:26:11 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/03/28/fast-forward-labs-data-leadership-conference.html</guid>
      <description>We’re excited to announce the first Fast Forward Labs Data Leadership Conference! Join us April 28 in New York City as we discuss how to build strong data teams in complex organizations. You can register here. In a recent Computerworld article, our CEO &amp;amp; Founder Hilary Mason explained common pitfalls organizations face when trying to build a data-driven organization: they over-invest in technical architecture, hire the wrong people, or lack the leadership focus required to cut across political silos, and foster innovation and change.</description>
    </item>
    
    <item>
      <title>H.P. Luhn and the Heuristic Value of Simplicity</title>
       
      <link>https://blog.fastforwardlabs.com/2016/03/25/h.p.-luhn-and-the-heuristic-value-of-simplicity.html</link>
      
      <pubDate>Fri, 25 Mar 2016 16:25:43 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/03/25/h.p.-luhn-and-the-heuristic-value-of-simplicity.html</guid>
      <description>&lt;figure data-orig-width=&#34;600&#34; data-orig-height=&#34;385&#34; class=&#34;tmblr-full&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/8ad092dc44ae1ad68bad697a55274b30/tumblr_inline_o4lon7By6N1ta78fg_540.gif&#34; alt=&#34;image&#34; data-orig-width=&#34;600&#34; data-orig-height=&#34;385&#34;/&gt;&lt;/figure&gt;&lt;p&gt;The Fast Forward Labs team is putting final touches on our &lt;i&gt;Summarization &lt;/i&gt;research, which explains approaches to making text quantifiable and computable. Stay tuned for a series of resources on the topic, including an online talk May 24 where we’ll cover technical details and survey use cases for financial services, media, and professional services with &lt;a href=&#34;http://www.agolo.com/&#34;&gt;Agolo&lt;/a&gt;. Sign up &lt;a href=&#34;https://textsummarizationwebinar.splashthat.com/&#34;&gt;here&lt;/a&gt;! &lt;br/&gt;&lt;/p&gt;&lt;p&gt;In writing our reports, we try not only to inform readers about the libraries, math, and techniques they can use to put a system into production today, but also the lessons they can learn from historical approaches to a given topic. Turning a retrospective eye towards past work can be particularly helpful when using an algorithm like a &lt;a href=&#34;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&#34;&gt;recurrent neural network&lt;/a&gt;. That’s because neural networks are notoriously hard to interpret: feature engineering is left to the algorithm, and involves a &lt;a href=&#34;http://blog.fastforwardlabs.com/2015/09/24/how-do-neural-networks-learn.html&#34;&gt;complex interplay&lt;/a&gt; between the weight of individual computing nodes and the connections that link them together. In the face of this complexity, it can be helpful to keep former, more simple techniques in mind as a heuristic guide - or model - to shape our intuition about how contemporary algorithms work. &lt;/p&gt;&lt;p&gt;For example, we found that &lt;a href=&#34;https://en.wikipedia.org/wiki/Hans_Peter_Luhn&#34;&gt;H.P. Luhn&lt;/a&gt;’s 1958 paper &lt;i&gt;&lt;a href=&#34;http://courses.ischool.berkeley.edu/i256/f06/papers/luhn58.pdf&#34;&gt;The Automatic Creation of Literary Abstracts&lt;/a&gt; &lt;/i&gt;provided a simple heuristic to help wrap our heads around the basic steps that go into probabilistic models for summarizing text. (For those interested in history, Luhn also wrote &lt;a href=&#34;http://altaplana.com/ibmrd0204H.pdf&#34;&gt;a paper about business intelligence&lt;/a&gt; in 1958 that feels like it could have been written today, as it highlights the growing need to automate information retrieval to manage an unwieldy amount of information.) Our design lead, &lt;a href=&#34;https://twitter.com/GrantCuster&#34;&gt;Grant Custer&lt;/a&gt;, designed a prototype you can play with to walk through Luhn’s method. &lt;/p&gt;&lt;p&gt;&lt;a href=&#34;http://fastforwardlabs.github.io/luhn/&#34;&gt;Here’s the link&lt;/a&gt; to access the live demo. Feel free to use the suggested text, or to play around your own (and share results on Twitter!). &lt;br/&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>&#34;Hello world&#34; in Keras (or, Scikit-learn versus Keras)</title>
       
      <link>https://blog.fastforwardlabs.com/2016/02/24/hello-world-in-keras-or-scikit-learn-versus-keras.html</link>
      
      <pubDate>Wed, 24 Feb 2016 18:58:10 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/02/24/hello-world-in-keras-or-scikit-learn-versus-keras.html</guid>
      <description>This article is available as a notebook on Github. Please refer to that notebook for a more detailed discussion and code fixes and updates. Despite all the recent excitement around deep learning, neural networks have a reputation among non-specialists as complicated to build and difficult to interpret.
And while interpretability remains an issue, there are now high-level neural network libraries that enable developers to quickly build neural network models without worrying about the numerical details of floating point operations and linear algebra.</description>
    </item>
    
    <item>
      <title>NeuralTalk with Kyle McDonald</title>
       
      <link>https://blog.fastforwardlabs.com/2016/02/18/neuraltalk-with-kyle-mcdonald.html</link>
      
      <pubDate>Thu, 18 Feb 2016 15:09:51 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/02/18/neuraltalk-with-kyle-mcdonald.html</guid>
      <description>&lt;figure data-orig-width=&#34;640&#34; data-orig-height=&#34;427&#34; class=&#34;tmblr-full&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/806e99a04e9f647a198d0d887942562b/tumblr_inline_o2pgqgwhP01ta78fg_540.jpg&#34; alt=&#34;image&#34; data-orig-width=&#34;640&#34; data-orig-height=&#34;427&#34;/&gt;&lt;/figure&gt;
&lt;h5 id=&#34;image-from-a-hrefhttpsvimeocom90547410social-soula-an-immersive-experience-of-being-inside-a-social-media-stream-by-a-hrefhttplauren-mccarthycomlauren-mccarthya-and-kyle-mcdonald&#34;&gt;Image from &lt;a href=&#34;https://vimeo.com/90547410&#34;&gt;Social Soul&lt;/a&gt;, an immersive experience of being inside a social media stream, by &lt;a href=&#34;http://lauren-mccarthy.com/&#34;&gt;Lauren McCarthy&lt;/a&gt; and Kyle McDonald&lt;/h5&gt;
&lt;p&gt;A few weeks ago, &lt;a href=&#34;http://siliconangle.tv/&#34;&gt;theCUBE&lt;/a&gt; stopped by the Fast Forward Labs offices to &lt;a href=&#34;http://siliconangle.tv/innovation-day-fast-forward-labs/&#34;&gt;interview us &lt;/a&gt;about our approach to innovation. In the interview, we highlighted that &lt;a href=&#34;http://blog.fastforwardlabs.com/2016/02/16/machines-and-metaphors.html&#34;&gt;artists have an important role to play&lt;/a&gt; in shaping the future of machine intelligence. Unconstrained by market demands and product management requirements, artists are free to probe the potential of new technologies. And by optimizing for intuitive power or emotional resonance over theoretical accuracy or usability, they open channels to understand how machine intelligence is always, at its essence, a study of our own humanity.&lt;br/&gt;&lt;/p&gt;&lt;p&gt;One provocative artist exploring the creative potential of new machine learning tools is &lt;a href=&#34;http://kylemcdonald.net/&#34;&gt;Kyle McDonald&lt;/a&gt;. McDonald has seized the deep learning moment, undertaking projects that use neural networks to document a stroll down the Amsterdam canals, &lt;a href=&#34;https://medium.com/@kcimc/comparing-artificial-artists-7d889428fce4#.ltnl33b5p&#34;&gt;recreate images&lt;/a&gt; in the style of famous painters, or challenge our awareness of what we hold to be reality. &lt;/p&gt;&lt;p&gt;We interviewed Kyle to understand how he understands his work. Keep reading for highlights:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Machines and Metaphors</title>
       
      <link>https://blog.fastforwardlabs.com/2016/02/16/machines-and-metaphors.html</link>
      
      <pubDate>Tue, 16 Feb 2016 16:35:11 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/02/16/machines-and-metaphors.html</guid>
      <description>&lt;figure data-orig-width=&#34;703&#34; data-orig-height=&#34;376&#34; class=&#34;tmblr-full&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/2cf1b0924a1bfc66737d96f4797ef6f7/tumblr_inline_o2neravFOu1ta78fg_540.png&#34; alt=&#34;image&#34; data-orig-width=&#34;703&#34; data-orig-height=&#34;376&#34;/&gt;&lt;/figure&gt;
&lt;h5 id=&#34;this-is-a-guest-post-by-a-hrefhttpwwwgenekogancomgene-kogana-an-artist-and-programmer-who-applies-emerging-technology-into-artistic-and-expressive-contexts-and-teaches-courses-and-workshops-on-topics-related-to-code-and-art&#34;&gt;This is a guest post by &lt;a href=&#34;http://www.genekogan.com/&#34;&gt;Gene Kogan&lt;/a&gt;, an artist and programmer who applies emerging technology into artistic and expressive contexts, and teaches courses and workshops on topics related to code and art.&lt;/h5&gt;
&lt;p&gt;Recent advances in deep learning research have renewed popular interest in machine intelligence. With new benchmarks set in tough problems (e.g., image classification and speech recognition), researchers are exploring unexpected and exciting applications, and eliciting public engagement and private investment. These recent breakthroughs have captured the attention of many for whom AI was previously obscure, as new capabilities spur applications of interest to wider public audiences.&lt;br/&gt;&lt;/p&gt;&lt;p&gt;But these advances have captured more than just our attention; they&amp;rsquo;ve captured our  imagination. Artists have been quick to apply these new techniques for novel creations, exploring the uncharted territories of machine creativity, slyly provoking questions of greater importance. What is creativity anyway? How do machines perceive, learn, and imitate?&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>What History Teaches Us About Data Science</title>
       
      <link>https://blog.fastforwardlabs.com/2016/02/03/what-history-teaches-us-about-data-science.html</link>
      
      <pubDate>Wed, 03 Feb 2016 22:05:44 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/02/03/what-history-teaches-us-about-data-science.html</guid>
      <description>The FFL team at the New York Historical Society&amp;rsquo;s Silicon City exhibit  Study the past if you would define the future. — Confucius
 Until April 17, 2016, the New York Historical Society is featuring an exhibition called Silicon City: Computer History Made in New York. The Fast Forward Labs team took a field trip to the museum back in December to augment our perspective on our current machine intelligence research (and, of course, to geek out and have fun).</description>
    </item>
    
    <item>
      <title>Eli Pariser on the Ethics of Algorithmic Filtering</title>
       
      <link>https://blog.fastforwardlabs.com/2016/01/22/eli-pariser-on-the-ethics-of-algorithmic-filtering.html</link>
      
      <pubDate>Fri, 22 Jan 2016 14:06:46 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/01/22/eli-pariser-on-the-ethics-of-algorithmic-filtering.html</guid>
      <description>&lt;figure data-orig-width=&#34;1442&#34; data-orig-height=&#34;1440&#34; class=&#34;tmblr-full&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/3172e6d87c1d99d12058845989f9041f/tumblr_inline_o1bgwg1jOS1ta78fg_540.jpg&#34; alt=&#34;image&#34; data-orig-width=&#34;1442&#34; data-orig-height=&#34;1440&#34;/&gt;&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;Chiefly his reflection, of which the portrait / Is the reflection, of which the portrait / Is the reflection once removed. / The glass chose to reflect only what he saw / Which was enough for his purpose: his image / Glazed, embalmed, projected at a 180-degree angle.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h5 id=&#34;-parmagiano-and-john-ashbery---self-portrait-in-a-convex-mirror&#34;&gt;— Parmagiano and John Ashbery - Self-Portrait in a Convex Mirror&lt;/h5&gt;
&lt;p&gt;Personalizing experiences and recommendations for consumers is the goal of many data science efforts, so much so that the NYC Media Lab created &lt;a href=&#34;http://www.nycmedialab.org/personalizationpalooza-16/&#34;&gt;Personalizationpalooza&lt;/a&gt; to unite technicians and media leadership around the topic. And while &lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1321962&#34;&gt;studies have shown&lt;/a&gt; that recommendation algorithms promote happy consumers, it’s questionable that also they promote healthy citizens.&lt;b&gt;&lt;br/&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;http://technosociology.org/&#34;&gt;Zeynep Tufecki&lt;/a&gt;, for example, recently wrote a &lt;a href=&#34;https://medium.com/message/how-facebook-s-tyranny-of-the-like-and-engagement-can-be-an-obstacle-to-an-open-and-connected-dddc03a0d39b#.2tc1yluvx&#34;&gt;Medium post &lt;/a&gt;explaining how Facebook’s focus on “engagement” as a metric for deciding which content to share on user news feeds can happen “to the detriment of the substantive experiences and interactions” she (and others) wants on Facebook. Zufecki illustrates her point with an instance where users felt morally unable to “like” a video with child refugees surrounded by dead bodies. She argues that the platform needs different mechanisms and metrics to ensure that content worth viewing can compete with the content we’re more apt to “like.” &lt;/p&gt;&lt;p&gt;Tufecki is not alone in her concerns. In 2011, &lt;a href=&#34;https://en.wikipedia.org/wiki/Eli_Pariser&#34;&gt;Eli Pariser&lt;/a&gt;, CEO of &lt;a href=&#34;https://www.upworthy.com/&#34;&gt;Upworthy&lt;/a&gt;, wrote the first monograph about the risks algorithmic filtering poses to citizenship and society. &lt;a href=&#34;http://www.amazon.com/The-Filter-Bubble-Personalized-Changing/dp/0143121235&#34;&gt;The Filter Bubble&lt;/a&gt; is still worth reading today, as it provides a framework for understanding how filtering algorithms work and what personal, social, and political consequences they may have. It’s critical we understand these tools now that the 2016 election season is in full swing. &lt;/p&gt;&lt;p&gt;We recently interviewed Pariser to see how his thinking has evolved in light of recent algorithmic progress. Keep reading for highlights. &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>New NLG Resources: Sale and January 26 Webinar</title>
       
      <link>https://blog.fastforwardlabs.com/2016/01/12/new-nlg-resources-sale-and-january-26-webinar.html</link>
      
      <pubDate>Tue, 12 Jan 2016 13:11:44 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2016/01/12/new-nlg-resources-sale-and-january-26-webinar.html</guid>
      <description>We’re excited to start 2016 by putting our Natural Language Generation (NLG) report and prototype on sale! We’re offering multiple packages, including a short report focused on business value for C-level executives. NLG is a technology that allows software systems to write articles and reports. Our RoboRealtor prototype shows how to use NLG to generate real estate ads from a few apartment attributes (e.g., number of bedrooms and bathrooms, location, onsite gym facilities, etc.</description>
    </item>
    
    <item>
      <title>Five Good Books* of 2015</title>
       
      <link>https://blog.fastforwardlabs.com/2015/12/21/five-good-books-of-2015.html</link>
      
      <pubDate>Mon, 21 Dec 2015 17:13:49 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/12/21/five-good-books-of-2015.html</guid>
      <description>* Plus a few papers, videos, and other fun things. We love books, and it feels fitting to end 2015 with a round-up of a few that we thought were particularly excellent this year. Our favorite book for data science beginners: Data Science from Scratch: First Principles with Python by Joel Grus (and if you order from O’Reilly directly, use discount code PCBW and get 50% off most ebooks/videos and 40% off most print books).</description>
    </item>
    
    <item>
      <title>NextView Ventures Podcast</title>
       
      <link>https://blog.fastforwardlabs.com/2015/12/15/nextview-ventures-podcast.html</link>
      
      <pubDate>Tue, 15 Dec 2015 16:15:34 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/12/15/nextview-ventures-podcast.html</guid>
      <description>Boston-based NextView Ventures runs a podcast series called Traction that features interviews with exciting new startups. Their latest podcast features an interview with our own Hilary Mason. Some highlights:
 In its essence, data science is the practice of learning insights from a data set and building a product based upon these learnings.
  The Fast Forward Labs team starts every data advisory engagement, be this with small startup or large enterprise clients, with a data census comprising three questions:</description>
    </item>
    
    <item>
      <title>Fashion Goes Deep: Data Science at Lyst</title>
       
      <link>https://blog.fastforwardlabs.com/2015/12/09/fashion-goes-deep-data-science-at-lyst.html</link>
      
      <pubDate>Wed, 09 Dec 2015 17:44:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/12/09/fashion-goes-deep-data-science-at-lyst.html</guid>
      <description>&lt;figure data-orig-width=&#34;694&#34; data-orig-height=&#34;376&#34; class=&#34;tmblr-full&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/993f6c53159f5715d9c127b790bb4bf5/tumblr_inline_nz3b9rPB5w1ta78fg_540.png&#34; alt=&#34;image&#34; data-orig-width=&#34;694&#34; data-orig-height=&#34;376&#34;/&gt;&lt;/figure&gt;&lt;p&gt;On November 16, 2015, Lyst, an online fashion marketplace based in the United Kingdom, &lt;a href=&#34;https://www.linkedin.com/pulse/lyst-ad-campaign-fashion-meets-data-science-flavia-young&#34;&gt;launched its first advertising campaign&lt;/a&gt;. Featuring a series of ironic headlines (one simply says “Rip-off”) etched over beautiful images, the campaign emphasizes the company’s identity as a “challenger brand,” whose success “has been driven by marrying insights from data science with the emotional nature of fashion.” (CEO Chris Morton) &lt;/p&gt;&lt;p&gt;&lt;a href=&#34;http://www.lyst.com&#34;&gt;Lyst&lt;/a&gt; provides fashion consumers with a central platform where they can mix and match millions of products from 11,500 different brands. In this context, data science serves as a virtual personal shopper, recommending products to users based upon insights from their behavior using the site. One might think these recommendations are powered by &lt;a href=&#34;https://en.wikipedia.org/wiki/Collaborative_filtering&#34;&gt;collaborative filtering&lt;/a&gt;, but the world of fashion is far too transient and fickle to support data models matching similar users. Matrices are sparse and inventory drains rapidly (consider flash sales sites like &lt;a href=&#34;http://www.gilt.com/&#34;&gt;Gilt&lt;/a&gt;). Instead, recommendation algorithms align consumer behavior with product features. And as the world of fashion is one dominated by image and appearance, fashion data science has a lot to do with image analysis. &lt;/p&gt;&lt;p&gt;We interviewed &lt;a href=&#34;https://twitter.com/ejlbell&#34;&gt;Eddie Bell&lt;/a&gt;, Lyst’s lead data scientist, about his team’s current efforts to use deep learning to analyze images and personalize recommendations to consumers. We talked about his past, his team’s present, and the fashion industry’s future. &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>When Dog Is Enough: Using Hypernyms To Improve Neural Network Predictions</title>
       
      <link>https://blog.fastforwardlabs.com/2015/11/17/when-dog-is-enough-using-hypernyms-to-improve-neural-network-predictions.html</link>
      
      <pubDate>Tue, 17 Nov 2015 16:35:14 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/11/17/when-dog-is-enough-using-hypernyms-to-improve-neural-network-predictions.html</guid>
      <description>&lt;p&gt;Possibly true statement: the Fast Forward Labs dog is the cutest dog in the world. &lt;br/&gt;&lt;/p&gt;&lt;figure data-orig-width=&#34;399&#34; data-orig-height=&#34;288&#34; class=&#34;tmblr-full&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/7df25fbff433a17f0a2d414666eff600/tumblr_inline_nxxgbxBEMd1ta78fg_540.png&#34; alt=&#34;image&#34; data-orig-width=&#34;399&#34; data-orig-height=&#34;288&#34;/&gt;&lt;/figure&gt;&lt;p&gt;Our General Counsel Ryan picked up the puppy a month ago and we’ve yet to name him. Ryan likes Renfield, which, as Bram Stoker fans know, evokes slightly different thoughts than “super cute,” particularly when &lt;a href=&#34;https://www.youtube.com/watch?v=WaVZmo8CsGQ&#34;&gt;played by&lt;/a&gt; the ever-guttural Tom Waits. But the fact that we’re in no rush to name him tells us something about how we label and identify things. We know he’s a dog, we love him for his dogness, and thus far that’s been just fine. I personally tend to forget what breed he is, as my knowledge of dog breeds is shamefully sparse. &lt;/p&gt;&lt;p&gt;Pictograph, in contrast, does an excellent job recognizing that our puppy is in fact a blenheim spaniel. Pictograph is the public app we built to illustrate how neural nets identify objects in images. &lt;a href=&#34;http://pictograph.us&#34;&gt;Try it&lt;/a&gt; on your personal Instagram feed!&lt;/p&gt;&lt;figure data-orig-width=&#34;658&#34; data-orig-height=&#34;216&#34; class=&#34;tmblr-full&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/36ad21fb7fbc61e892ca66af5f684fe4/tumblr_inline_nxxgkiSgKa1ta78fg_540.png&#34; alt=&#34;image&#34; data-orig-width=&#34;658&#34; data-orig-height=&#34;216&#34;/&gt;&lt;/figure&gt;&lt;p&gt;A 97% confidence rate in the accuracy of the prediction is a dream for automated classification. Here, the confidence is so high for two reasons. &lt;/p&gt;&lt;p&gt;First, the &lt;a href=&#34;http://www.image-net.org/&#34;&gt;ImageNet &lt;/a&gt;database used to train the Pictograph neural network has a lot of pictures of blenheim spaniels (971&amp;hellip;and yep, it’s prime). This labelled data informs the network what a correct classification &lt;i&gt;should&lt;/i&gt; look like. The learning mechanism (called &lt;a href=&#34;http://blog.fastforwardlabs.com/2015/09/24/how-do-neural-networks-learn.html&#34;&gt;backpropagation&lt;/a&gt;) then steps in and learns the network to predict the label “blenheim spaniel” when presented with new images that have similar features.&lt;br/&gt;&lt;/p&gt;&lt;p&gt;Second, the images in Ryan’s Instagram feed aren’t noisy. Note how the two images with 97% confidence rates show our puppy alone and facing the camera. This pose is similar to the stock images available on ImageNet, rendering it easier for the neural net to detect similarities. The confidence rate in the right-hand image including the human face drops to 62% because the data is noisier. It would likely drop further when presented an image of our puppy unconsciously playing &lt;a href=&#34;https://en.wikipedia.org/wiki/Ouroboros&#34;&gt;Ouroboros&lt;/a&gt; (the mythical snake that eats its own tail).&lt;/p&gt;&lt;figure data-orig-width=&#34;462&#34; data-orig-height=&#34;328&#34; class=&#34;tmblr-full&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/bac3509687c5af7c21fd2fc853bf0501/tumblr_inline_nxxhlkZ4zi1ta78fg_540.png&#34; alt=&#34;image&#34; data-orig-width=&#34;462&#34; data-orig-height=&#34;328&#34;/&gt;&lt;/figure&gt;&lt;p&gt;But Instagram, like most data in the wild, rarely contains clean data that maps neatly to a model’s parameters or the stock photos in a training set like ImageNet. In turn, classification systems can yield confidence rates as low as 20% or 30% (or lower), generating doubts as to whether it’s worth using the technology at all. One way to improve unsatisfying results from a machine learning tool is to adopt a “&lt;a href=&#34;https://medium.com/the-wtf-economy/artificial-intelligence-and-the-future-of-work-a0eaabea7c41&#34;&gt;human in the loop&lt;/a&gt;” approach, where humans step in and manually label images technology misclassifies or classifies with low confidence rates. But we decided to adopt a different approach.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>FFL in the Bay Area, Chicago, and DC</title>
       
      <link>https://blog.fastforwardlabs.com/2015/11/03/ffl-in-the-bay-area-chicago-and-dc.html</link>
      
      <pubDate>Tue, 03 Nov 2015 19:00:20 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/11/03/ffl-in-the-bay-area-chicago-and-dc.html</guid>
      <description>We’ve got three events on the horizon. Join us, or contact us with questions if you’d like to learn more but are unable to attend!
Wednesday, November 11 | Mountain View, CA Hilary Mason will give a keynote at H2O World, where participants discuss how to use machine learning to build intelligent applications. Hilary will explain how Fast Forward Labs helps companies discover and build exciting new products from their existing data assets.</description>
    </item>
    
    <item>
      <title>Interview with Pedro Domingos</title>
       
      <link>https://blog.fastforwardlabs.com/2015/10/29/interview-with-pedro-domingos.html</link>
      
      <pubDate>Thu, 29 Oct 2015 13:13:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/10/29/interview-with-pedro-domingos.html</guid>
      <description>&lt;figure data-orig-width=&#34;1000&#34; data-orig-height=&#34;614&#34; class=&#34;tmblr-full&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/4886047f032d47b832630a0ad0a72841/tumblr_inline_nwzgbcEPkQ1ta78fg_540.jpg&#34; alt=&#34;image&#34; data-orig-width=&#34;1000&#34; data-orig-height=&#34;614&#34;/&gt;&lt;/figure&gt;&lt;p&gt;Machine learning technologies increasingly shape our sense of reality and the choices we make in our daily lives. They power Amazon’s product recommendations. They classify documents relevant for a lawsuit. They enable computers to play chess like the masters.&lt;b&gt;&lt;br/&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;As machine learning applications expand to influence our civic, professional and private lives, it’s important that we all have a basic understanding of how they work and what potential they offer. &lt;a href=&#34;https://homes.cs.washington.edu/~pedrod/&#34;&gt;Pedro Domingos&lt;/a&gt; undertook the challenge of providing the first comprehensive, nontechnical overview of machine learning in his new book, &lt;i&gt;The Master Algorithm&lt;/i&gt;. &lt;/p&gt;&lt;p&gt;In &lt;i&gt;The Master Algorithm, &lt;/i&gt;Domingos divides the field into five different “tribes” &amp;ndash; symbolism, connectionism (neural networks), evolutionary algorithms, Bayesian networks, and analogical reasoning &amp;ndash; which he challenges his readers to unify in one future “master algorithm” capable of learning nearly anything. This will towards universality informs his book’s bold central hypothesis: “all knowledge - past, present, and future - can be derived from data by a single, universal learning algorithm.” It’s up to us to find it. &lt;/p&gt;&lt;p&gt;We had the pleasure of interviewing Domingos last week. Keep reading to see the highlights. &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Hello Deep Learning</title>
       
      <link>https://blog.fastforwardlabs.com/2015/10/26/hello-deep-learning.html</link>
      
      <pubDate>Mon, 26 Oct 2015 16:20:07 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/10/26/hello-deep-learning.html</guid>
      <description>Deep learning is a hot and fascinating research area, particularly when applied to classifying images. While researching the Fast Forward Labs Deep Learning: Image Analysis report, we played with a lot of very cool technology. In this blog post, we offer a guide to getting started with deep learning by using APIs from some of the most interesting deep-learning-as-a-service startups.
These APIs accept images and/or video, and quickly classify objects, ideas, and items shown in the images and video.</description>
    </item>
    
    <item>
      <title>Flip the Paradigm</title>
       
      <link>https://blog.fastforwardlabs.com/2015/10/20/flip-the-paradigm.html</link>
      
      <pubDate>Tue, 20 Oct 2015 19:14:08 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/10/20/flip-the-paradigm.html</guid>
      <description>Last week, Hilary, Fast Forward Labs founder and CEO, gave the opening keynote at the Grace Hopper Celebration of Women in Computing in Houston, TX. Her talk inspired an audience of over 12,000 women to embrace the unimaginable possibilities that will shape the careers of future technologists. Sure, aspiring engineers and data scientists have to endure the angst that they can no longer chart their future by emulating role models from their past.</description>
    </item>
    
    <item>
      <title>Grace Hopper Opening Keynote</title>
       
      <link>https://blog.fastforwardlabs.com/2015/10/13/grace-hopper-opening-keynote.html</link>
      
      <pubDate>Tue, 13 Oct 2015 20:23:46 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/10/13/grace-hopper-opening-keynote.html</guid>
      <description>We’re super excited that our CEO &amp;amp; Founder, Hilary Mason, will deliver the opening keynote tomorrow at the Grace Hopper conference in Houston, TX. You can catch the keynote live at 9:30 EDT / 8:30 CDT on the conference livestream website. As a brief preview, Hilary’s talk will cover: how and why machine intelligence is revolutionizing computing; what today’s tech ecosystem looks like and how aspiring female technologists have to maneuver to succeed (be that landing the right job or starting a company); and how Fast Forward Labs is an example of a “hack” of the system, rethinking the future of applied research.</description>
    </item>
    
    <item>
      <title>Breaking Down Memes</title>
       
      <link>https://blog.fastforwardlabs.com/2015/10/06/breaking-down-memes.html</link>
      
      <pubDate>Tue, 06 Oct 2015 21:12:57 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/10/06/breaking-down-memes.html</guid>
      <description>Last week, Hilary spoke about opportunities for mid-sized companies to use data at Accelerating America’s Middle Market, hosted by the Wall Street Journal. This morning, I spoke about opportunities for large, established companies to use data at a Corporate Longevity Leadership Briefing, hosted by the Financial Times. Here are two key takeaways from the panels: Memes generate excitement; excitement generates hype; hype generates confusion. In her panel, Hilary aimed to demystify some of the sorcery surrounding ‘big data’ that risks rendering data projects intimidating to mid-sized companies with limited resources.</description>
    </item>
    
    <item>
      <title>Machine Learning in Retail: Consumer Privacy Implications</title>
       
      <link>https://blog.fastforwardlabs.com/2015/09/29/machine-learning-in-retail-consumer-privacy-implications.html</link>
      
      <pubDate>Tue, 29 Sep 2015 18:13:00 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/09/29/machine-learning-in-retail-consumer-privacy-implications.html</guid>
      <description>&lt;figure data-orig-width=&#34;1401&#34; data-orig-height=&#34;956&#34; class=&#34;tmblr-full&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/7404e8cb508c2bc469e5a9887f5ac5e4/tumblr_inline_nvgbviEbKS1ta78fg_540.jpg&#34; alt=&#34;image&#34; data-orig-width=&#34;1401&#34; data-orig-height=&#34;956&#34;/&gt;&lt;/figure&gt;&lt;p&gt;In our &lt;a href=&#34;http://blog.fastforwardlabs.com/2015/08/19/machine-learning-applications-in-fashion-retail.html&#34;&gt;last retail post&lt;/a&gt;, we explained how emerging sensor technologies are changing data science for brick and mortar. Many companies are working to retro-fit physical stores with capabilities originally developed for ecommerce. Without adding any new sensors or tags, image recognition company &lt;a href=&#34;https://blippar.com/en/&#34;&gt;Blippar &lt;/a&gt;provides APIs that digitize physical products to create an in-app experience around them. &lt;a href=&#34;http://www.businessoffashion.com/articles/bof-exclusive/farfetch-e-commerce-fashion-brands-omnichannel&#34;&gt;Farfetch&lt;/a&gt;, which initially brought an exclusive network of luxury brick &amp;amp; mortar shops online, recently announced that it will soon provide in-store analytics.&lt;b&gt;&lt;br/&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;In parallel, digital companies continue to seek out physical spaces to augment their customer experience. Warby Parker and Rent the Runway have physical stores, and Gilt and Etsy are exploring new models to engage consumers offline (Gilt has a &lt;a href=&#34;http://fashionista.com/2015/08/gilt-by-appointment&#34;&gt;private retail space&lt;/a&gt; in their corporate headquarters and Etsy &lt;a href=&#34;http://venturebeat.com/2015/08/25/etsy-squeezes-into-brick-and-mortar-stores-with-latest-shop-local-feature/&#34;&gt;launched an app&lt;/a&gt; to inform shoppers of sellers’ nearby products). These digital companies were built upon testing &lt;a href=&#34;http://pathtoperf.com/2015/04/07/01-with-lara-hogan.html&#34;&gt;every detail&lt;/a&gt; affecting their website performance. As tracking migrates to physical retail, it’s important that retailers consider the consumer experience risks associated with new technologies as they explore the benefits. &lt;/p&gt;&lt;p&gt;&amp;ldquo;When you look at something on the web, you get ads that pop up and follow you around - companies like that have much better advantage over brick-and-mortar retailers, and so they&amp;rsquo;re under more pressure to equalise the playing field.&amp;rdquo; &lt;a href=&#34;http://www.theguardian.com/news/datablog/2013/oct/03/analytics-amazon-retailers-physical-cookies-high-street&#34;&gt;Kevin Kearns&lt;/a&gt;, ShopperTrak&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>How do neural networks learn?</title>
       
      <link>https://blog.fastforwardlabs.com/2015/09/24/how-do-neural-networks-learn.html</link>
      
      <pubDate>Thu, 24 Sep 2015 18:56:09 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/09/24/how-do-neural-networks-learn.html</guid>
      <description>Neural networks are generating a lot of excitement, as they are quickly proving to be a promising and practical form of machine intelligence. At Fast Forward Labs, we just finished a project researching and building systems that use neural networks for image analysis, as shown in our toy applicationPictograph. Our companion deep learning report explains this technology in depth and explores applications and opportunities across industries.
As we built Pictograph, we came to appreciate just how challenging it is to understand how neural networks work.</description>
    </item>
    
    <item>
      <title>Fast Forward Labs Interviews Clarifai about Deep Learning</title>
       
      <link>https://blog.fastforwardlabs.com/2015/09/22/fast-forward-labs-interviews-clarifai-about-deep-learning.html</link>
      
      <pubDate>Tue, 22 Sep 2015 18:58:45 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/09/22/fast-forward-labs-interviews-clarifai-about-deep-learning.html</guid>
      <description>Last Thursday Hilary and I headed to Clarifai’s offices in the Flatiron District to ask CEO Matt Zeiler about using deep learning for image analysis. A few highlights from the interview:
1) The success of a deep learning project depends on the quality of the initial training data set. Deep learning algorithms start by scanning massive data sets to identify features (inputs) that can be correlated with categories (outputs) to make sense of the data.</description>
    </item>
    
    <item>
      <title>Pictograph: Unlock Your Images</title>
       
      <link>https://blog.fastforwardlabs.com/2015/09/15/pictograph-unlock-your-images.html</link>
      
      <pubDate>Tue, 15 Sep 2015 19:16:19 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/09/15/pictograph-unlock-your-images.html</guid>
      <description>Have you ever wondered what your photos say about how you look at the world and who you are? Your images won’t say much about what types of things you tend to post unless you routinely tag them. Our new toy application, Pictograph, catalogs the objects that make up your Instagram identity. Pictograph analyzes your Instagram photos and creates a visualization, or pictograph, of what you like to photograph. It’s a fun way to play with new deep learning algorithms for image analysis, and it makes some pretty hilarious mistakes.</description>
    </item>
    
    <item>
      <title>D’Alembert’s Deep Dream: Bees and Nonlinear Transformations</title>
       
      <link>https://blog.fastforwardlabs.com/2015/09/02/dalemberts-deep-dream-bees-and-nonlinear-transformations.html</link>
      
      <pubDate>Wed, 02 Sep 2015 14:08:33 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/09/02/dalemberts-deep-dream-bees-and-nonlinear-transformations.html</guid>
      <description>&lt;figure data-orig-width=&#34;284&#34; data-orig-height=&#34;178&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/5e441cb1ef3adcd99999f7b29da5ee5e/tumblr_inline_nu1ujeo4cc1ta78fg_540.jpg&#34; alt=&#34;image&#34; data-orig-width=&#34;284&#34; data-orig-height=&#34;178&#34;/&gt;&lt;/figure&gt;&lt;p&gt;Hold your hats! In the next couple of weeks we’re launching an arsenal of deep learning resources, including a feature report, a public prototype that will classify your Instagram identity and a webinar exploring the past, present and future of deep learning. Sign up &lt;a href=&#34;https://deeplearningwebinar.splashthat.com/&#34;&gt;here&lt;/a&gt;!&lt;/p&gt;&lt;p&gt;As a philosophical prelude to the upcoming report, we wanted to invite you to think about the emergent properties of neural nets. Let’s explore what 18th-century philosopher Denis Diderot can teach us about artificial intelligence.&lt;/p&gt;&lt;p&gt;Diderot was not your average Enlightenment philosopher. A &lt;i&gt;philosophe, &lt;/i&gt;he grappled with the colossal inheritance of mechanical philosophy that dominated 18th-century intellectual circles. Spearheaded by Descartes and Newton, the mechanical view held that the material world was composed of complicated machines governed and determined by immutable laws. &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Machine Learning Applications in Fashion Retail</title>
       
      <link>https://blog.fastforwardlabs.com/2015/08/19/machine-learning-applications-in-fashion-retail.html</link>
      
      <pubDate>Wed, 19 Aug 2015 18:08:14 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/08/19/machine-learning-applications-in-fashion-retail.html</guid>
      <description>&lt;figure data-orig-width=&#34;1080&#34; data-orig-height=&#34;572&#34; class=&#34;tmblr-full&#34;&gt;&lt;img src=&#34;http://68.media.tumblr.com/1ea7d218b3ec206decfb85436ead953d/tumblr_inline_ntcf1hms2i1ta78fg_540.jpg&#34; data-orig-width=&#34;1080&#34; data-orig-height=&#34;572&#34;/&gt;&lt;/figure&gt;
&lt;h5 id=&#34;this-is-the-first-of-two-articles-about-recent-developments-in-fashion-technology-part-two-will-focus-on-implications-for-consumer-privacy&#34;&gt;This is the first of two articles about recent developments in fashion technology. Part two will focus on implications for consumer privacy.&lt;/h5&gt;
&lt;p&gt;The next frontier for recommender systems is the retail store. We’re used to associating machine learning with ecommerce giants like Gilt and Lyst, but can data science transform physical stores like Rebecca Minkoff and Zara? The impact would be significant, as more than &lt;a href=&#34;http://www2.deloitte.com/us/en/pages/about-deloitte/articles/press-releases/digital-influences-in-retail-store-sales.html&#34;&gt;90% of retail sales still occur in brick-and-mortar stores&lt;/a&gt;, which could use data to redefine in-store personalization.&lt;/p&gt;&lt;p&gt;The introduction of data collection in the retail space does not always require new tools; rather, retailers can gather granular data on customers and products by repurposing existing hardware like security cameras and clothing sensors in new ways. Developments in machine learning have made it possible to auto-process existing video feeds to track customers, from simple movement analytics to full body profiles. Smart mirrors, security cameras (CCTV) and radio-frequency identification (RFID) sensors which create rich datasets that make automated personalization in bricks &amp;amp; mortar scalable.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Why Now? Some Preconditions for Technology Innovations</title>
       
      <link>https://blog.fastforwardlabs.com/2015/08/14/why-now-some-preconditions-for-technology-innovations.html</link>
      
      <pubDate>Fri, 14 Aug 2015 18:57:31 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/08/14/why-now-some-preconditions-for-technology-innovations.html</guid>
      <description>We like to hold fast to the myth of the individual creative genius as the source of the world’s most impactful scientific revolutions or disruptive innovations. But it’s consoling to recall how Isaac Newton consoled his rival Robert Hooke: “If I’ve seen further than others, it was by standing on the shoulders of giants.”
 This is French how painter Nicolas Poussin represents Cedalion providing sight to the blind Orion, a mythological pair associated with each generation’s progress over its predecessors.</description>
    </item>
    
    <item>
      <title>On Stirling Engines and Orchids: A Prelude to Deep Learning</title>
       
      <link>https://blog.fastforwardlabs.com/2015/08/07/on-stirling-engines-and-orchids-a-prelude-to-deep-learning.html</link>
      
      <pubDate>Fri, 07 Aug 2015 19:08:04 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/08/07/on-stirling-engines-and-orchids-a-prelude-to-deep-learning.html</guid>
      <description>Today’s post is inspired by a slow-motion recording we captured of a Stirling engine that Ryan, Fast Forward’s General Counsel, just so happened to have lying around our New York City offices. For the non-mechanics among us, a Stirling engine is a heat engine that operates by cyclic compression and expansion of air and other gas at different temperatures; the temperature differential translates heat into mechanical work. The slowmo in the video renders a hypnotic locomotive sound that differs greatly from the mechanic buzz of realtime.</description>
    </item>
    
    <item>
      <title>A Flying Machine from New York to Paris</title>
       
      <link>https://blog.fastforwardlabs.com/2015/08/05/a-flying-machine-from-new-york-to-paris.html</link>
      
      <pubDate>Wed, 05 Aug 2015 16:28:34 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/08/05/a-flying-machine-from-new-york-to-paris.html</guid>
      <description>On March 25, 1909, Wilbur Wright (of the Wright brothers) told a reporter at the Cairo, Illinois bulletin that “no airship will ever fly from New York to Paris.” As with most quotes inherited from the past, people often misinterpret Wright’s quote as reactionary because they read it out of context. He continues: “What limits the flight is the motor. No known motor can run at the requisite speed for four days without stopping, and you can’t be sure of finding the proper winds for soaring&amp;hellip;But the history of civilization has usually shown that every new invention has brought in its train new needs it can satisfy, and so what the airship will eventually be used for is probably what we can least predict at the present.</description>
    </item>
    
    <item>
      <title>Fast Forward Labs at the Science Fair</title>
       
      <link>https://blog.fastforwardlabs.com/2015/07/16/fast-forward-labs-at-the-science-fair.html</link>
      
      <pubDate>Thu, 16 Jul 2015 19:43:26 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/07/16/fast-forward-labs-at-the-science-fair.html</guid>
      <description>Last night was the New York Times&amp;rsquo; Open Source Science Fair. It kicked off with a keynote speech by our own Hilary Mason.
 Photo by Chrys Wu: http://t.co/m7G7G3g1ih After a dinner meet-and-greet session, the attendees and exhibitors got down to business, showing off their open source software projects, science fair style.
Fast Forward Labs was an exhibitor. Micha Gorelick impressed the crowd with our realtime stream analysis prototype, CliqueStream.</description>
    </item>
    
    <item>
      <title>Celebrating Our One Year Anniversary</title>
       
      <link>https://blog.fastforwardlabs.com/2015/06/23/celebrating-our-one-year-anniversary.html</link>
      
      <pubDate>Tue, 23 Jun 2015 19:24:50 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/06/23/celebrating-our-one-year-anniversary.html</guid>
      <description>Celebrating our one year anniversary!</description>
    </item>
    
    <item>
      <title>MongoDB World Keynote</title>
       
      <link>https://blog.fastforwardlabs.com/2015/06/19/mongodb-world-keynote.html</link>
      
      <pubDate>Fri, 19 Jun 2015 19:42:33 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/06/19/mongodb-world-keynote.html</guid>
      <description>Fast Forward Labs’ founder and CEO gave a keynote speech at MongoDB World in New York earlier this month. Check out the video here.</description>
    </item>
    
    <item>
      <title>Hilary on Science Friday</title>
       
      <link>https://blog.fastforwardlabs.com/2015/06/16/hilary-on-science-friday.html</link>
      
      <pubDate>Tue, 16 Jun 2015 14:40:47 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/06/16/hilary-on-science-friday.html</guid>
      <description>Last Friday, Hilary joined Ira Flatow on Science Friday in the studio to discuss the question:Will Our Smartphones Know Us Better Than Ourselves?
We love imagining what our future may look like with the technology we are building.</description>
    </item>
    
    <item>
      <title>Coffee Thermoregulation</title>
       
      <link>https://blog.fastforwardlabs.com/2015/06/04/coffee-thermoregulation.html</link>
      
      <pubDate>Thu, 04 Jun 2015 22:59:36 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/06/04/coffee-thermoregulation.html</guid>
      <description>We take coffee pretty seriously here at Fast Forward Labs. In addition to our machine learning research, we have also explored both hot and cold coffee brewing technologies.
Research reveals that no one wants to drink a mug of hot brew once it settles to room temperature. As a slow sipper, I end up pouring out most of the coffee in my mug because it cools off too quickly. I set up this solution in a few minutes this afternoon using an off-the-shelf immersion heater and a microcontroller temperature regulator.</description>
    </item>
    
    <item>
      <title>Designing the Tech Graph</title>
       
      <link>https://blog.fastforwardlabs.com/2015/05/19/designing-the-tech-graph.html</link>
      
      <pubDate>Tue, 19 May 2015 16:25:01 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/05/19/designing-the-tech-graph.html</guid>
      <description>We launched a redesign of the Fast Forward Labs website today. There are lots of parts of the design I’d like to write someday about but for this post I want to focus on the thinking behind the “Tech Graph” force-directed graph display in the Intro section.
Dramatizing Information Work The response I want the intro section to provoke is something like: “Wow, there is some really interesting work being done here that I would like to be involved with it.</description>
    </item>
    
    <item>
      <title>Hilary on Bloomberg TV</title>
       
      <link>https://blog.fastforwardlabs.com/2015/04/30/hilary-on-bloomberg-tv.html</link>
      
      <pubDate>Thu, 30 Apr 2015 21:33:25 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/04/30/hilary-on-bloomberg-tv.html</guid>
      <description>Our fearless leader Hilary Mason appeared yesterday on Bloomberg TV to discuss Selerity’s early acquisition of Twitter’s earnings data. The interview video is available here.</description>
    </item>
    
    <item>
      <title>Bytecode Hacking for Great Justice</title>
       
      <link>https://blog.fastforwardlabs.com/2015/04/23/bytecode-hacking-for-great-justice.html</link>
      
      <pubDate>Thu, 23 Apr 2015 15:16:56 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/04/23/bytecode-hacking-for-great-justice.html</guid>
      <description>DO NOT TRY THIS AT HOME! NO PYTHONS WERE HURT IN THE CREATION OF THIS BLOG POST!
Check out the code at code at github.com/mynameisfiber/pytailcall
As an exercise into learning more about python 2.7 bytecode, I wanted to implement the thing that pythonistas love to hate - tail call optimization! This isn&amp;rsquo;t novel at all, but I chose to implement this only using the standard library so that I could understand more about generating and modifying bytecode.</description>
    </item>
    
    <item>
      <title>Blog Design Refresh</title>
       
      <link>https://blog.fastforwardlabs.com/2015/04/06/blog-design-refresh.html</link>
      
      <pubDate>Mon, 06 Apr 2015 20:12:24 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/04/06/blog-design-refresh.html</guid>
      <description>It&amp;rsquo;s restyled blog day here at Fast Forward Labs! Having wrapped up our second report, I (Grant, Fast Forward Labs designer-dev) am now spending some time putting our web presence in order. The real big piece of that is a revamped Fast Forward website, which should be out next week, but as a warm-up today I worked on getting our blog into shape.
I wanted to keep things clean and simple (to go with the grain).</description>
    </item>
    
    <item>
      <title>Probabilistic Methods for Realtime Streams</title>
       
      <link>https://blog.fastforwardlabs.com/2015/04/01/probabilistic-methods-for-realtime-streams.html</link>
      
      <pubDate>Wed, 01 Apr 2015 20:24:20 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/04/01/probabilistic-methods-for-realtime-streams.html</guid>
      <description>Our second R&amp;amp;D Report, Probabilistic Methods for Realtime Streams, has gone out! In this report, we explored probabilistic algorithms for machine learning on potentially large realtime streams of data with efficient CPU and memory usage.
Our prototype demonstrated these algorithms running on large amounts of social conversation data. More on that soon.</description>
    </item>
    
    <item>
      <title>The Hoff</title>
       
      <link>https://blog.fastforwardlabs.com/2015/03/31/the-hoff.html</link>
      
      <pubDate>Tue, 31 Mar 2015 18:44:43 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2015/03/31/the-hoff.html</guid>
      <description>The Hoff stopped by Fast Forward Labs!</description>
    </item>
    
    <item>
      <title>Our Next Topic: Realtime Stream Analysis</title>
       
      <link>https://blog.fastforwardlabs.com/2014/12/19/our-next-topic-realtime-stream-analysis.html</link>
      
      <pubDate>Fri, 19 Dec 2014 21:17:09 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2014/12/19/our-next-topic-realtime-stream-analysis.html</guid>
      <description>We&amp;rsquo;re very pleased to announce our second research report topic will be realtime stream analysis, with a focus on probabilistic data structures. Using these techniques, we&amp;rsquo;re able to build systems that enable extremely fast and memory efficient computation over very large data sets. For example, imagine being able to do comparisons between two sets of billions of items in milliseconds using only a few megabytes of memory.
We&amp;rsquo;re exploring applications with partners in marketing, healthcare, and science.</description>
    </item>
    
    <item>
      <title>The Natural Language Generation Report is Out</title>
       
      <link>https://blog.fastforwardlabs.com/2014/10/17/the-natural-language-generation-report-is-out.html</link>
      
      <pubDate>Fri, 17 Oct 2014 20:49:48 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2014/10/17/the-natural-language-generation-report-is-out.html</guid>
      <description>The Fast Forward Labs report on Natural Language Generation will be arriving soon to a bookshelf near you!</description>
    </item>
    
    <item>
      <title>We Built a Table</title>
       
      <link>https://blog.fastforwardlabs.com/2014/08/21/we-built-a-table.html</link>
      
      <pubDate>Thu, 21 Aug 2014 19:28:14 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2014/08/21/we-built-a-table.html</guid>
      <description>At Fast Forward Labs we’re thoughtful about the kind of work space we want to spend our time in. We decided that we preferred a shared large table instead of individual desks, to make it easier to collaborate and offer the flexibility to spread out when necessary, and to allow space for other people to drop in and work with us.
We also love to build things, and find an hour of working with our hands to be a welcome respite from hours of math, programming, reading, and writing, so we decided to build it ourselves!</description>
    </item>
    
    <item>
      <title>Hello, Fast Forward Labs!</title>
       
      <link>https://blog.fastforwardlabs.com/2014/07/21/hello-fast-forward-labs.html</link>
      
      <pubDate>Mon, 21 Jul 2014 12:01:01 +0000</pubDate>
      
      <guid>https://blog.fastforwardlabs.com/2014/07/21/hello-fast-forward-labs.html</guid>
      <description>I’m very pleased to introduce Fast Forward Labs.
Fast Forward Labs is an independent data technology research lab. We focus on taking technologies that are just becoming possible, and making them useful.
We believe that the existing research structures are failing in 2014. We offer companies a new approach to applied research that helps them find those product and business opportunities that exist at the intersection of their existing business and new data and technology capabilities.</description>
    </item>
    
  </channel>
</rss>
