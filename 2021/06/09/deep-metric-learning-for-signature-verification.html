<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />
    
<title>Deep Metric Learning for Signature Verification</title>
<meta property="og:title" content="Deep Metric Learning for Signature Verification">
<meta property="description" content="By Victor and Andrew.
TLDR; This post provides an overview of metric learning loss functions (constrastive, triplet, quadruplet, and group loss), and results from applying contrastive and triplet loss to the task of signature verification. A complete list of the posts in this series is outlined below: Pretrained Models as Baselines for Signature Verification  --   Part 1: Deep Learning for Automatic Offline Signature Verification: An Introduction    Part 2: Pretrained Models as Baselines for Signature Verification    Part 3: Deep Metric Learning for Signature Verification      In our previous blog post, we discussed how pretrained models can serve as strong baselines for the task of signature verification.">
<meta property="og:description" content="By Victor and Andrew.
TLDR; This post provides an overview of metric learning loss functions (constrastive, triplet, quadruplet, and group loss), and results from applying contrastive and triplet loss to the task of signature verification. A complete list of the posts in this series is outlined below: Pretrained Models as Baselines for Signature Verification  --   Part 1: Deep Learning for Automatic Offline Signature Verification: An Introduction    Part 2: Pretrained Models as Baselines for Signature Verification    Part 3: Deep Metric Learning for Signature Verification      In our previous blog post, we discussed how pretrained models can serve as strong baselines for the task of signature verification.">
<meta property="og:image" content="https://blog.fastforwardlabs.com/images/hugo/metricblog/onlinetraining.png">
<meta property="og:url" content="https://blog.fastforwardlabs.com/2021/06/09/deep-metric-learning-for-signature-verification.html">
<meta property="twitter:card" content="summary_large_image">
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <link rel="stylesheet" type="text/css" href="/style.css" />
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-53030428-5', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

  </head>
  <body>
      <div class="container">
        <div class="spacer"></div>
        <div style="height: 2.5rem; padding-top: 0.35rem;">
          <a target="_blank" href="https://www.cloudera.com/products/fast-forward-labs-research.html">
            <img style="height: 2rem;" src="/images/cloudera-fast-forward-logo.png" />
          </a>
        </div>
        <div class="spacer"></div>
      </div>
      <main id="main">
        
<div class="container">
  <div>
    <h3 class="clear"><a href="/">Blog</a></h3>
  </div>
  <div class="spacer"></div>
  <div class="post">
    <h5 class="clear">
      <span>Jun 9, 2021</span> &middot;
      <span style="text-transform: capitalize;">
        
      </span>
    </h5>
    <h1>Deep Metric Learning for Signature Verification</h1>
    <p>By <em><a href="https://twitter.com/vykthur">Victor</a></em> and <em><a href="https://www.linkedin.com/in/andrew-r-reed/">Andrew.</a></em></p>
<div  class="tldr"> 
  <span class="textbold">TLDR;</span> This post provides an overview of metric learning loss functions (constrastive, triplet, quadruplet, and group loss), and results from applying contrastive and triplet loss to the task of signature verification. A complete list of the posts in this series is outlined below:
  <div style="margin-top:10px; border-top: 1px dashed grey"> 
    <!-- <a href="/2021/05/27/pre-trained-models-as-a-strong-baseline-for-automatic-signature-verification.html" class="postlink"> Pretrained Models as Baselines for Signature Verification 
    </a> -->
    <ul>
    <li> <a href="/2021/05/26/deep-learning-for-automatic-offline-signature-verification-an-introduction.html" class=""> Part 1: Deep Learning for Automatic Offline Signature Verification: An Introduction </a> </li>
      <li> <a href="/2021/05/27/pre-trained-models-as-a-strong-baseline-for-automatic-signature-verification.html" class=""> Part 2: Pretrained Models as Baselines for Signature Verification </a> </li>
      <li> <a href="/2021/06/09/deep-metric-learning-for-signature-verification.html" class=""> Part 3: Deep Metric Learning for Signature Verification </a> </li>
    </ul>
  </div>
</div>
<p>In our <a href="/2021/05/27/pre-trained-models-as-a-strong-baseline-for-automatic-signature-verification.html">previous blog post</a>, we discussed how pretrained models can serve as strong baselines for the task of signature verification. Essentially, using representations learned by models trained on the ImageNet task allowed us to obtain competitive performance when attempting to correctly classify signature pairs as genuine or forgeries (with ResNet50: 69.3% accuracy on skilled forgeries, 86.7% on unskilled forgeries).</p>
<p><img src="/images/hugo/metricblog/classificationmetric.png" alt=""></p>
<h5 id="figure-1-metric-learning-allows-us-to-learn-a-discriminative-embedding-space-that-both-maximizes-inter-class-distance-and-minimizes-intra-class-distance">Figure 1. Metric learning allows us to learn a discriminative embedding space that both maximizes inter-class distance and minimizes intra-class distance.</h5>
<p>However, several intuitions suggest that we might be able to improve on this. First, given that the pretrained model was trained on data (natural images), which does differ from our task data(signature images), it makes sense that some sort of fine-tuning would be useful in <em><strong>adapting</strong></em> the pretrained model weights to our data distribution. Second, the pretrained model is based on a classification objective (cross-entropy loss) - i.e., correctly classify one million images into 1000 different classes. It learns to maximize inter-class distances such that features before the softmax fully connected layer are linearly separable.</p>
<p>While this objective yields semantic representations that have proven to be useful, our primary task of verification does require something more precise. Essentially, we want an objective focused on capturing the similarities/dissimilarities between two data points; we want to learn discriminative features that not only <strong>maximize inter-class distance</strong> but also <strong>minimize intra-class distance</strong>. To achieve this goal, we can turn to metric learning: learning a distance metric designed to satisfy the objective of making representations for similar objects close and representations for dissimilar objects far apart, in some metric space.</p>
<h2 id="what-is-metric-learning">What is Metric Learning?</h2>
<p>A simple description of metric learning is &ldquo;any machine learning model structured to learn a distance measure over samples&rdquo;. The intuition here is that if the model is designed to learn a distance function for similar/dissimilar objects, we can use it for applications such as signature verification that rely on this property.</p>
<p>The metric learning objective can be formulated in several ways, depending on how the training dataset is structured.</p>
<p><img src="/images/hugo/metricblog/datapoints.png" alt=""></p>
<h5 id="figure-11-data-used-to-train-a-metric-learning-model-may-be-structured-as-pairs-triplets-or-quadruplets-this-in-turn-influences-how-the-loss-function-is-designed">Figure 1.1. Data used to train a metric learning model may be structured as pairs, triplets, or quadruplets. This, in turn, influences how the loss function is designed.</h5>
<p>Commonly, sets of training examples are structured as either pairs, triplets, or quadruplets, from which a distance function is learned. Despite the differences in problem formulation, the high-level metric learning workflow remains consistent:</p>
<ul>
<li>Define a dataset of train samples (2, 3, 4 data points in each sample, depending on loss function).</li>
<li>Extract representations for each datapoint in a given sample, using a trainable embedding model. (Note that the exact architecture of this embedding model might vary depending on the data - e.g., a CNN architecture might be used for metric learning on images, while an RNN architecture might be useful for sequences.)</li>
<li>Measure the similarity between data points in each sample using a distance function (e.g., Euclidean).</li>
<li>Calculate a loss based on the observed vs. expected distances between data points. (For example, for data points we know to be similar - i.e., expected small distance - we have very little or no loss if their observed distance is <em>indeed</em> small.)</li>
<li>Backpropagate the loss through the embedding network, so the model learns to produce similar representations for similar examples and distant representations for dissimilar examples.</li>
</ul>
<p>In the following sections, we will consider a few loss functions for implementing metric learning.</p>
<h2 id="contrastive-loss">Contrastive Loss</h2>
<p>Contrastive loss (also known as pairwise ranking loss) is a metric learning objective function where we learn from training data examples structured as pairs: positive pairs (examples that belong to the same class) and negative pairs (examples that belong to different classes).</p>
<div style="border-bottom: 1px dashed grey; background-color:#E5E5E5; padding: 10px; margin-bottom:10px"> 
In our signature verification task, we construct positive pairs consisting of an anchor signature and an alternative (genuine) signature from the same author. We then construct negative pairs composed of an anchor signature and a forged signature authored by a different individual. 
<p>Note that the forged signature may be unskilled (i.e., the forger knows nothing about the original) or skilled (i.e., the forger has access to what the original signature looks like and actively attempts to mimic it). Clearly, it is easier to detect unskilled forgeries compared to skilled forgeries, which are hard to detect (even for humans). As we construct training pairs for a model to learn from, we want it to be particularly good at solving the hard version of the problem: distinguishing skilled forgeries.</p>
<p>In the contrastive loss setting, negative pairs consisting of unskilled forgeries are termed easy negatives, while those consisting of skilled forgeries are termed hard negatives.</p>
</div>
<p>The contrastive loss function is set up such that <strong><em>we minimize the distance between embeddings for positive pairs</em></strong>, and <em><strong>maximize the distance between embeddings for negative pairs</strong></em>. After each pass through the network, we ideally want to update the weights of our embedding model such that the above condition is satisfied. Contrastive loss can be implemented using a Siamese network architecture for the embedding model. This model takes in two inputs and uses a shared trunk network that produces two embeddings for each input.</p>
<div style="border-bottom: 1px dashed grey; background-color:#E5E5E5; padding: 10px; margin-bottom:10px"> 
Note that a Siamese network is not required to implement metric learning with contrastive loss. In theory, you could design a custom training loop where a forward pass through the embedding model is used to get embeddings for each data point in the sample pair, loss is computed, and the model weights updated. The Siamese architecture (which avoids manually implementing multiple forward passes) simplifies implementation and is widely used.
</div>
<p><img src="/images/hugo/metricblog/contrastive_loss.png" alt=""></p>
<h5 id="figure-2-contrastive-loss-is-used-to-minimize-the-distance-between-genuine-signature-pairs-top-and-maximize-distance-between-negative-pairs-bottom">Figure 2. Contrastive loss is used to minimize the distance between genuine signature pairs (top) and maximize distance between negative pairs (bottom)</h5>
<p><img src="/images/hugo/metricblog/contrastive_train.png" alt=""></p>
<h5 id="figure-3-during-training-we-expect-that-loss-updates-will-cause-the-model-to-update-its-weights-such-that-positive-pairs-are-closer-and-negative-pairs-are-farther-apart">Figure 3. During training, we expect that loss updates will cause the model to update its weights such that positive pairs are closer and negative pairs are farther apart.</h5>
<p>Contrastive loss is defined as:</p>
<div style="text-align:center">
<p><code>$ Loss = \begin{cases} D    \text{  ...............................  if pair is positive}\\ max(0, m-D) \text{ .........  if pair is negative }\\  \end{cases}    $</code></p>
</div>
<p>where <code>$D$</code> is the calculated distance between embeddings for each datapoint in the pair, and <code>$m$</code> is a constant value of margin.</p>
<p>In the case of positive pairs, the loss becomes positive only when we have a positive distance between vector representations. In the case of negative pairs, the loss is positive only when the distance between vectors is less than the margin. The margin value is a hyperparameter that serves as an upper threshold to constrain the amount of loss attributed to “easy to classify” pairs. A simple way to think about the function of the margin is that if the network produces representations that are distant “enough” for a given pair, there is no need to continue focusing training efforts on that example. <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p>While contrastive loss is useful, it has a limitation. For points in a negative pair, contrastive loss will push them far apart without any knowledge of the broader embedding space. For example: imagine we have 10 classes, and each time we see class 1 and 2, we want to push them far apart; a result of this is that 1 might now become farther from 2 on the average, but might overlap with other classes (such as class 3, 4, 5, etc.).</p>
<p>Triplet loss addresses this issue by using triplets in each training sample: an anchor, a positive (same class), and a negative (different class) data point. This way, with each gradient update, we learn to minimize anchor + positive distance while also ensuring that the anchor is far from the negative point.</p>
<h2 id="triplet-loss">Triplet Loss</h2>
<p><img src="/images/hugo/metricblog/triplet_loss.png" alt=""></p>
<h5 id="figure-4-a-triplet-model-is-trained-with-triplets-such-that-the-distance-between-the-anchor-and-positive-is-minimized-while-the-distance-between-the-anchor-and-negative-is-maximized">Figure 4. A triplet model is trained with triplets such that the distance between the anchor and positive is minimized while the distance between the anchor and negative is maximized.</h5>
<p><img src="/images/hugo/metricblog/triplet_train.png" alt=""></p>
<h5 id="figure-5-at-each-train-step-we-expect-that-triplet-loss-updates-will-simultaneously-minimize-the-distance-between--anchor-and-positive-while-maximizing-distance-between-anchor-and-negative-samples">Figure 5. At each train step, we expect that triplet loss updates will simultaneously minimize the distance between  anchor and positive while maximizing distance between anchor and negative samples.</h5>
<p>The triplet loss function aims to learn a distance between representations such that the anchor-to-positive distance is less than the anchor-to-negative distance. Similar to contrastive loss, a margin value is imposed on the anchor-to-negative distance so that once negative representations have enough distance between them, no further effort is taken to increase distance between them. With this understanding, triplet loss is formally defined as:</p>
<div style="text-align:center">
<p><code>$ Loss = max (0, m + D_p - D_n) $</code></p>
</div>
<p>where <code>$D_p$</code> is the anchor-to-positive distance and <code>$D_n$</code> is the anchor-to-negative distance and <code>$m$</code> is the margin. With this loss formulation, we can create three different types of triplet combinations based on how we sample:</p>
<ul>
<li>Easy triplets: result when <code>$D_n$</code> &gt; <code>$D_p$</code> + <code>$m$</code>. Here, the sampled anchor-to-negative distance is already large enough so loss is 0, and the network has nothing to learn from.</li>
<li>Hard triplets: result when <code>$D_n$</code> &lt; <code>$D_p$</code>. In this case, the anchor-to-negative distance is less than the anchor-to-positive distance, meaning high loss to backpropagation through the network.</li>
<li>Semi-hard triplets: result when <code>$D_p$</code> &lt; <code>$D_n$</code> &lt; <code>$D_p$</code> + <code>$m$</code>. Semi-hard triplets occur when the negative example is more distant to the anchor than the positive example, but the distance is not greater than the margin. This, therefore, results in a positive loss (i.e., the negative is far &hellip; but not far enough.) <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></li>
</ul>
<p>Based on the above, we can see that different approaches for selecting triplets (also called <strong>triplet mining</strong> approaches) are not equally informative. More importantly, if we prescriptively mine the right types of triplets that are informative, we can increase the overall efficiency of learning. Ideally, we want to focus on hard and semi-hard triplets - a luxury we were not afforded in the contrastive, pairwise learning setup.</p>
<h3 id="from-offline-to-online-triplet-mining">From Offline to Online Triplet Mining</h3>
<p>To implement any of the triplet strategies, we need to obtain embeddings for our samples, so as to determine their distances and how best to construct our triplet. An initial offline approach will be to obtain embeddings for our entire training set, and then select only hard or semi-hard triplets.</p>
<p>Assuming we have a training set of size <code>$T$</code>:</p>
<ul>
<li>Get all embeddings for all points in <code>$T$</code></li>
<li>Get a list of all possible <code>$T^3$</code> triplets in <code>$T$</code>, and identify valid triplets (hard or semi-hard), based on embeddings and class labels</li>
<li>Compute loss and update model weights using valid triplets</li>
</ul>
<p>This approach is inefficient, as it requires that we extract embeddings for the entire training set to generate valid triplets. It also assumes we have an embedding model that yields good embeddings on the train dataset prior to training.</p>
<p><img src="/images/hugo/metricblog/onlinetraining.png" alt=""></p>
<h5 id="figure-6-in-online-triplet-mining-triplets-are-constructed-during-training">Figure 6. In online triplet mining, triplets are constructed during training.</h5>
<p>We can be more efficient about this by adopting online triplet mining. <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> Here, we select our triplets during training (within each training batch), as opposed to precomputing triplets prior to training.</p>
<p>Assuming we have a batch of size <code>$B$</code>:</p>
<ul>
<li>Get embeddings for all <code>$B$</code> data points in our batch</li>
<li>Get all possible triplets <code>$B^3$</code> in <code>$B$</code> and identify valid triplets (hard or semi hard) based on embeddings and class labels</li>
<li>Compute loss and update model weights using valid triplets</li>
</ul>
<p>As we will see later on, the ability to sample intelligently and introduce relative knowledge between three examples helps improve training efficiency, and results in more contextually robust feature embeddings.</p>
<h2 id="other-losses-quadruplet-loss-group-loss">Other Losses: Quadruplet Loss, Group Loss</h2>
<p><img src="/images/hugo/metricblog/quadruplet_train.png" alt=""></p>
<h5 id="figure-7-quadruplet-loss">Figure 7: Quadruplet loss</h5>
<p>At this point, it is clear that a good loss function should attempt to learn similarity using information from each training sample. Contrastive loss learns from pairs of images (positive, negative). Triplet loss achieves even better performance by simultaneously learning from carefully selected image triplets (anchor, positive, negative) within a batch. But what if we could simultaneously learn from even more samples within a batch? Existing research has explored this.</p>
<p>In their work, Chen et al. <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> introduce <strong>quadruplet loss</strong>, where each training sample consists of 4 data points (anchor, positive, negative1, negative2), where negative2 is dissimilar to all other data points. They minimize the distance between anchor and positive, while simultaneously maximizing distance between anchor and negative1 <em>as well as</em> negative1 and negative2.</p>
<p>Elezi et al <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> take this one step further and propose <strong>group loss</strong>, which aims to simultaneously learn from all samples within a minibatch (as opposed to a pair, triplet or quadruplet). To create the mini-batch, they sample from a fixed number of classes, with samples coming from a class forming a group. Thus, each mini-batch consists of several randomly chosen groups, and each group has a fixed number of samples. An iterative, fully-differentiable label propagation algorithm is then used to build feature embeddings, which are similar for samples belonging to the same group, and dissimilar otherwise. The overall effect of this group loss formulation is to enforce embedding similarity across all samples of a group, while simultaneously promoting low-density regions amongst data points belonging to different groups.</p>
<p>One trend we can observe from these loss functions is that the more data points from multiple classes we use to update our gradients, the better our model is able to capture the global structure of the embedding space. In theory, quadruplet loss and group loss represent the state of the art in metric learning loss functions, but they do introduce multiple parameters that can be challenging to implement and tune. Even when this is done correctly, the expected increase in performance (~1 - 4%) may be hard to reproduce, especially due to the stochastic nature of DNN training. In practice (as we will see in the experiment section below), triplet loss (with online learning) is efficient and high-performing; hence, it is the recommended approach.</p>
<h2 id="our-experimentation">Our Experimentation</h2>
<h3 id="dataset">Dataset</h3>
<p>To evaluate a metric learning approach on our signature verification task, we experimented with contrastive and triplet loss. We first constructed an experimental setup consistent with the design from our <a href="/2021/05/27/pre-trained-models-as-a-strong-baseline-for-automatic-signature-verification.html">pretrained baseline experiment</a>. Using the CEDAR dataset, we withheld signatures for 11 of the 55 authors as a test set, and used the other 44 as a training set.</p>
<p><img src="/images/hugo/metricblog/cedardataset.png" alt=""></p>
<h5 id="figure-8-the-cedar-dataset-used-for-our-signature-verification-experiments">Figure 8. The CEDAR dataset used for our signature verification experiments.</h5>
<p>Note that depending on the loss function, the strategy for constructing the eventual training dataset from which the network learns might differ. In our contrastive loss experiments, we generated a total of 36,432 pairs. For our triplet loss experiments - given that we used the online triplet mining approach - we structured the dataset as a classification problem; each signer and each forgery was treated as a class, making for a total of 88 classes with 24 samples (2112 total), each in the training set. Amongst other limitations, the CEDAR dataset is small; as we&rsquo;ll see later on, this makes it challenging to train a performant neural network from scratch.</p>
<p>To evaluate each model, we constructed a set of pairs of positives and negatives (see Figures 9 and 10 for examples of skilled and unskilled forgeries in our test set) and report maximum accuracy and equal error rate. Except where mentioned, the performance scores we report are based on a test set of positives and skilled forgeries (hard negatives). (Additional discussion on evaluation metrics is provided in a <a href="/2021/05/27/pre-trained-models-as-a-strong-baseline-for-automatic-signature-verification.html">previous post</a> in this series.)</p>
<p><img src="/images/hugo/metricblog/hardnegatives.jpg" alt=""></p>
<h5 id="figure-9-examples-of-skilled-forgeries-where-the-forger-has-access-to-the-genuine-signature-and-attempts-to-replicate-it-we-refer-to-such-pairs-as-hard-negatives-all-performance-scores-reported-are-based-on-a-test-set-containing-this-type-of-negatives">Figure 9. Examples of skilled forgeries where the forger has access to the genuine signature and attempts to replicate it. We refer to such pairs as hard negatives. All performance scores reported are based on a test set containing this type of negatives.</h5>
<p><img src="/images/hugo/metricblog/easynegatives.jpg" alt=""></p>
<h5 id="figure-10-examples-of-unskilled-forgeries-where-the-forger-does-not-have-access-to-the-genuine-signature-and-may-provide-a-random-signature-instead-we-refer-to-such-pairs-as-easy-negatives">Figure 10. Examples of unskilled forgeries where the forger does not have access to the genuine signature and may provide a random signature instead. We refer to such pairs as easy negatives.</h5>
<h3 id="contrastive-loss-training-and-evaluation">Contrastive Loss Training and Evaluation</h3>
<p>For contrastive loss, a Siamese network was assembled, using an embedding model composed of a frozen ResNet50 backbone with a series of 3 trainable layers appended to the network head (1 GlobalAveragePooling 2D layer and 2 Dense layers with 128 activations each). This approach allowed us to fine-tune the already effective pre-trained ResNet50 feature extractor on our signature verification dataset, using contrastive loss.</p>
<p>Through several training iterations, we found that introducing intermediate dropout and L2 regularization to our custom network head allowed us to achieve 72.8% max accuracy on the test set - an improvement over the pretrained ResNet50 baseline (69.3%)!</p>
<p>While these results are promising, there are several considerations to take into account when using contrastive loss. First, contrastive loss requires us to construct and train on an exhaustive list of genuine/forged examples. For our dataset, that meant training on 36,432 sample pairs each epoch, despite the fact that many of those pairs may not actually contribute any loss (and therefore learning) to the network because they are “easy” positives or negatives.</p>
<p><img src="/images/hugo/metricblog/umap_contrastive.jpg" alt=""></p>
<h5 id="figure-11-a-3d-plot-of-embeddings-produced-by-a-network-trained-with-contrastive-loss-for-three-authors-signatures-genuine-vs-forged-examples-are-appropriately-placed-into-separable-regions-as-seen-by-groupings-of-circles-vs-diamonds">Figure 11. A 3D plot of embeddings produced by a network trained with contrastive loss, for three authors&rsquo; signatures. Genuine vs. forged examples are appropriately placed into separable regions, as seen by groupings of circles vs. diamonds.</h5>
<p>Second, contrastive loss is limited in its ability to learn global, contextual feature representations. Figure 11 above depicts embeddings generated from the trained Siamese network for genuine and forged signature examples from three authors. We observe that the network has correctly separated genuine vs. forged examples into separate regions, which is precisely what contrastive loss aims to accomplish. However, the model has not captured the relative similarity of signatures from the same author (which can hurt generalization to new signature datasets). Ideally, we desire a model that produces embeddings that are separable across authors and discriminative between genuine/forged classes. For example, we would like to see that all signatures from Author 3 appear in the same general region, and within that region, signatures maintain separation between genuine and forged. Because contrastive loss is formulated by looking only at pairs of images, it cannot preserve a contextual understanding of author classes, and simply just learns discriminative features between genuine and forged. This shortcoming is solved by triplet loss (as we will see), and therefore the remainder of our exploration focused on this superior loss function.</p>
<h3 id="triplet-loss-training-and-evaluation">Triplet Loss Training and Evaluation</h3>
<p>We constructed several model architectures (see Table 1 below) and trained them using the Triplet Semi-Hard loss implemented in the Tensorflow addons library. [5]  Each model had a similar set of final layers (two conv2D layers, followed by a Dense layer and an L2 Normalization layer with size 256). We found that adding an L2 Normalization layer after our output embeddings (as suggested in [2]) was useful in constraining the embeddings to a hypersphere conducive for cosine similarity. Each model was trained with the Adam optimizer (lr=0.001), with a learning rate decay of 0.7 ever 10 steps, and trained for 25 epochs.</p>
<div class="metrictable">
<table>
<thead>
<tr>
<th>Model</th>
<th align="center">Description</th>
<th align="center">Max Acc</th>
<th align="center">EER</th>
<th align="center"># Params (Million)</th>
<th align="center">Size (MB)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Small ResNet50 + Skip</td>
<td align="center">Initialized with weights from a pretrained ResNet50 model at layer conv4_block6_add.</td>
<td align="center">81.8</td>
<td align="center">0.184</td>
<td align="center">9.27</td>
<td align="center">35.72</td>
</tr>
<tr>
<td>Small ResNet50</td>
<td align="center">Initialized with weights from a pretrained ResNet50 model at layer conv4_block6_3_conv.</td>
<td align="center">76.6</td>
<td align="center">0.236</td>
<td align="center">9.27</td>
<td align="center">35.7</td>
</tr>
<tr>
<td>ResNet50</td>
<td align="center">Initialized with weights from a pretrained ResNet50 model.</td>
<td align="center">76.4</td>
<td align="center">0.242</td>
<td align="center">24.79</td>
<td align="center">95.01</td>
</tr>
<tr>
<td>Smaller ResNet50 + Skip</td>
<td align="center">Initialized with weights from a pretrained ResNet50 model at layer conv3_block4_add.</td>
<td align="center">76.1</td>
<td align="center">0.24</td>
<td align="center">2.18</td>
<td align="center">8.5</td>
</tr>
<tr>
<td>Small ResNet50 NONORM</td>
<td align="center">Initialized with weights from a pretrained ResNet50 model at layer conv4_block6_3_conv. Embedding is not L2 normalized</td>
<td align="center">75.2</td>
<td align="center">0.249</td>
<td align="center">9.27</td>
<td align="center">35.7</td>
</tr>
<tr>
<td>ResNet50 NONORM</td>
<td align="center">Initialized with weights from a pretrained ResNet50 model. Embedding is not L2 normalized.</td>
<td align="center">75</td>
<td align="center">0.252</td>
<td align="center">24.79</td>
<td align="center">95.01</td>
</tr>
<tr>
<td>Pretrained VGG16 Baseline</td>
<td align="center">Pretrained VGG16 without any metric learning fine tuning.</td>
<td align="center">74.3</td>
<td align="center">0.268</td>
<td align="center">14.71</td>
<td align="center">56.18</td>
</tr>
<tr>
<td>Small VGG16</td>
<td align="center">Initialized with weights from a pretrained VGG16 at layer block3_conv1</td>
<td align="center">72.1</td>
<td align="center">0.282</td>
<td align="center">2.33</td>
<td align="center">8.91</td>
</tr>
<tr>
<td>UNet ResNet50</td>
<td align="center">UNet like model initialized with weights from a pretrained ResNet50 model at layer conv4_block6_add.</td>
<td align="center">72</td>
<td align="center">0.281</td>
<td align="center">9.47</td>
<td align="center">36.61</td>
</tr>
<tr>
<td>Base UNet</td>
<td align="center">UNet like model.</td>
<td align="center">71.5</td>
<td align="center">0.293</td>
<td align="center">1.29</td>
<td align="center">5.06</td>
</tr>
<tr>
<td>Pretrained ResNet50 Baseline</td>
<td align="center">Pretrained ResNet50 without any metric learning fine tuning.</td>
<td align="center">69.3</td>
<td align="center">0.315</td>
<td align="center">23.59</td>
<td align="center">90.39</td>
</tr>
<tr>
<td>Base CNN NONORM</td>
<td align="center">CNN baseline. Embedding is not L2 normalized</td>
<td align="center">68.1</td>
<td align="center">0.327</td>
<td align="center">1.68</td>
<td align="center">6.44</td>
</tr>
<tr>
<td>Base CNN</td>
<td align="center">CNN baseline.</td>
<td align="center">68</td>
<td align="center">0.327</td>
<td align="center">1.68</td>
<td align="center">6.44</td>
</tr>
<tr>
<td>VGG16</td>
<td align="center">Initialized with weights from a pretrained VGG16 model.</td>
<td align="center">67.4</td>
<td align="center">0.328</td>
<td align="center">15.04</td>
<td align="center">57.42</td>
</tr>
</tbody>
</table>
</div>
<h5 id="table-1-performance-for-multiple-models-evaluated-on-a-test-set-with-hard-negative-pairs-all-models-are-trained-with-the-triplet-loss-objective-except-the-pretrained-vgg16-and-resnet50-baseline">Table 1. Performance for multiple models evaluated on a test set with hard negative pairs. All models are trained with the triplet loss objective (except the pretrained VGG16 and ResNet50 baseline).</h5>
<p><img src="/images/hugo/metricblog/tripletmodel_summary.jpg" alt=""></p>
<h5 id="figure-12-maximum-accuracy-vs-model-size-for-multiple-model-architectures-evaluated-on-the-signature-verification-task">Figure 12. Maximum accuracy vs. model size for multiple model architectures, evaluated on the signature verification task.</h5>
<h4 id="effect-of-triplet-model-choices">Effect of Triplet Model Choices</h4>
<p>Training multiple models allowed us to explore the impact of the design choices a data sciencist must navigate when applying metric learning in practice.</p>
<p><strong>Impact of a Metric Loss Formulation:</strong>
We see that fine-tuning a pretrained model using the triplet loss leads to a 81.8% maximum accuracy for our best model. This is a significant improvement from a pretrained ResNet50 baseline performance of 74.3%.</p>
<p><strong>Impact of Pretrained Features:</strong>
In our previous experiments, we saw that a pretrained model could be a strong baseline without any finetuning. To evaluate how much of the performance increase we see in our metric learning experiments is attributable to the use of pretrained features (vs. the triplet loss function), we trained a vanilla baseline CNN, as well as models constructed from intermediate layers of a pretrained ResNet and VGG16 model.</p>
<ul>
<li>Base CNN - 68%, 1.68 million parameters</li>
<li>Base UNet - 71.5%, 1.29 million parameters</li>
<li>Small ResNet50 Skip initialized at skip connection layer conv4_block6_add - 81.8%, 9.2 million parameters. (best performance)</li>
<li>Smaller ResNet50 Skip initialized at skip connection layer conv3_block4_add - 76.1%, 2.18 million parameters</li>
<li>Full ResNet50 fine-tuned on the triplet metric learning objective - 76.4%  24.79 million</li>
</ul>
<p>Overall, we found the following to be useful insights:</p>
<ul>
<li>Fine-tuning with pretrained features (e.g., Smaller ResNet50 vs Baseline CNN and Baseline UNet) yields better results compared to training a model from scratch.</li>
<li>A UNet-like architecture for models of comparable size yields better performance for our task (Base UNet vs Base CNN).</li>
</ul>
<p><strong>Impact of Skip Connections:</strong>
Skip connections in CNNs have been shown to improve the loss surface <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>  for deep networks, making them easier to train and yielding performance.</p>
<ul>
<li>Fine-tuning a full ResNet50 model (which has skip connections) achieves better performance (76.4%) compared to fine tuning a VGG16 model (67.4%).</li>
</ul>
<p><strong>Intermediate model vs Full Model:</strong>
When applying transfer learning, the data scientist must decide how much of the pretrained model is useful to their task - i.e., what features to include, what features to freeze, and what features to finetune.</p>
<ul>
<li>We found that we got the best performance when we constructed an intermediate model from ResNet50 (Small ResNet50 skip, 35MB) and fine-tuned it, vs. fine-tuning the entire ResNet50.</li>
<li>Furthermore, we found that when we constructed this intermediate model using the output of a skip connection vs. a conv2D layer, the results were better. Our intuition is that the later layers in a pretrained ResNet50 model contain high level features (e.g., eyes, wheels, doors) that are not relevant to our task and dataset (which are mostly lines and texture in a signature) and can introduce noise. More importantly, an intermediate model is <strong><em>faster to train, significantly smaller, and hence easier to deploy</em></strong> (e.g., as a microservice with limits on file sizes). Note that Smaller ResNet50 is competitive (76.1% on hard negatives, 95% on easy negatives), but only <strong><em>8.5mb</em></strong> in file size.</li>
</ul>
<p>Notes on training with triplet loss:</p>
<ul>
<li>Ensure a large enough batch size is selected when training with the online triplet loss (on the minimum, batch size should be greater than the number of classes). This ensures each batch contains enough samples such that a valid semi-hard triplet can be sampled. A NaN loss value during training might indicate that your current batch size is too small.</li>
<li>Ensure the distance metric used in the loss function is the same distance metric that will be used during evaluation/test time. (E.g., if cosine distance is used in the loss function during training, it should also be used when comparing the similarity of two signatures at test time. For example, a model trained with an L2 distance function will show reduced accuracy when evaluated with cosine distance.)</li>
<li>Modifying the margin parameter can be used to improve the performance of each model. (For example, if loss does not decrease during training, it might indicate that it is challenging to find good triplets (e.g., negatives are too hard and yield similar embeddings); in this case, decreasing the margin parameter can be useful.)</li>
</ul>
<h2 id="debugging-a-metric-learning-model---does-it-do-what-we-think-it-does">Debugging a Metric Learning Model - Does It Do What We Think It Does?</h2>
<p>So far, we have built several models with competitive results. However, it is important to verify that the model is in fact achieving its goals (similar signatures close together, dissimilar signatures far apart), and that it is doing this for the right reasons.</p>
<p>To this end, we explored several sanity check approaches to help us build trust and confidence in the model’s behaviour. Sanity checks help us confirm expected behaviours and question any unexpected or unusual observations.</p>
<h3 id="dimensionality-reduction--visualization">Dimensionality Reduction + Visualization</h3>
<p>First, we have used dimensionality reduction techniques (UMAP) to visualize embeddings for each signature in our test set. We expect that signatures from the same author are clustered together; we also expect that skilled forgeries are close to originals, but separated from the cluster of the associated genuine signature.</p>
<p><img src="/images/hugo/metricblog/smallresnetembedding.jpg" alt=""></p>
<h5 id="figure-13-visualization-of-umap-embeddings-2-dimensions-for-signatures-in-our-test-set-in-general-we-see-that-embeddings-for-forgeries-are-in-the-same-region-as-their-corresponding-genuine-signatures-but-still-separated">Figure 13. Visualization of UMAP embeddings (2 dimensions) for signatures in our test set. In general, we see that embeddings for forgeries are in the same region as their corresponding genuine signatures, but still separated.</h5>
<h3 id="visualization-of-distance-metrics">Visualization of distance metrics</h3>
<p>In this sanity check, we construct image pairs (positive and negative pairs) and compute the distance between embeddings produced by our model for each pair. We expect that the density of distances between positive pairs is close to zero (with some variation to account for user error), but more spread out toward 1 for negative pairs.</p>
<p><img src="/images/hugo/metricblog/hard_density.png" alt=""></p>
<h5 id="figure-14-density-plot-of-the-cosine-distances-between-embeddings-produced-by-multiple-models-for-positive-and-hard-negative-pairs-in-our-test-set">Figure 14. Density plot of the cosine distances between embeddings (produced by multiple models) for positive and hard negative pairs in our test set.</h5>
<p><img src="/images/hugo/metricblog/easy_density.png" alt=""></p>
<h5 id="figure-15-density-plot-of-the-cosine-distances-between-embeddings-produced-by-multiple-models-for-positive-and-easy-negative-pairs-in-our-test-set">Figure 15. Density plot of the cosine distances between embeddings (produced by multiple models) for positive and easy negative pairs in our test set.</h5>
<h3 id="gradient-visualization-of-class-activation-maps">Gradient Visualization of Class Activation Maps</h3>
<p>We can also utilize gradient based approaches <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>  to inspect “what aspects of the input are most influential/relevant to the output.” These approaches have been used in debugging classification models to identify what pixels in the input image drive a specific class prediction. For example, we want to see that pixels around the head and ears of a husky dog are the relevant regions, as opposed to a snow background, when predicting the husky class. A well-known approach in this area is GradCam, <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>  which visualizes the gradient of the class score (logit) with respect to the feature map of the last convolutional unit of a DNN model.
We can adapt it to our use case by visualizing the gradient of the entire output embedding with respect to the feature map of a preselected convolutional layer in our metric learning model. What this visualization gives us is some intuition on the region within the input signature that our layer finds influential while producing embeddings. Ideally, we want to see a concentration around the actual lines and strokes of the signature, and possible focus on parts of written letters that might have high variance (e.g., attention to how users might round their g’s in a unique way). Our implementation of GradCam is based on the Keras GradCam example <a href="https://keras.io/examples/vision/grad_cam/">here</a>.<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup></p>
<p>Note, that methods like GradCam are not exactly principled, but subject to interpretation based on domain knowledge. The reader is encouraged to explore GradCam visualizations to confirm that the pixels which the model finds influential make sense, based on their knowledge of the problem space.</p>
<p><img src="/images/hugo/metricblog/signaturegradcam.jpg" alt=""></p>
<h5 id="figure-16-gradcam-visualization-for-the-last-4-convolutional-layers-for-signatures-from-our-test-set">Figure 16: GradCam visualization for the last 4 convolutional layers for signatures from our test set.</h5>
<h2 id="limitations">Limitations</h2>
<p>In this work, we&rsquo;ve shown that fine-tuning a pretrained model on a metric learning loss (contrastive and triplet loss) can improve performance. However, there are a few limitations with our experiment&rsquo;s setup that are worth noting:</p>
<ul>
<li>
<p><strong>Dataset Limitations</strong>: While our training setup is designed such that we evaluated the models on signatures from individuals not represented in the training set, we recognize that the CEDAR dataset is small, and does not cover properties of signatures (e.g., different writing styles, languages, etc.) that may occur in real world documents, but are not covered in the CEDAR dataset. Training and evaluation on additional datasets is strongly recommended prior to production use.</p>
</li>
<li>
<p><strong>Limitations of Triplet Loss</strong>: While triplet loss is great, its application is limited to scenarios where labels exist. To implement intelligent construction of triplets (especially semi-hard negatives), we need labels for each class. On the other hand, we can still apply contrastive loss to unlabelled datasets by leveraging pseudo-labels (e.g., existing research <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>  shows that k-means assignments can be used as pseudo-labels to learn visual representations).</p>
</li>
<li>
<p><strong>Data Augmentation</strong>: We used the CEDAR dataset as is, without exploring augmentations or transformations. For example, it may be useful to experiment with scale or rotation transforms, to ensure the model is invariant to these changes.</p>
</li>
<li>
<p><strong>Reproducibility</strong>: Deep Neural Networks can have multiple sources of randomness, which can make it challenging to reproduce the exact results we report. These may include hardware properties, weight initialization, etc. In our experiments, we attempted to minimize this by fixing the seeds for numpy and tensorflow. We still observed slight variations across each run (~3 percentage points) - however, the relative order of results stayed consistent.</p>
</li>
<li>
<p><strong>Model Tuning</strong>: In our experiments, while we explored the impact of metric loss finetuning, pretrained features, and skip connections over a small set of presets, there are still many parameters that can be tuned. These include margin parameter, learning rate, model architecture (e.g., use of models designed to learn multiscale features), etc.</p>
</li>
<li>
<p><strong>Other Loss Function</strong>s: While we have focused on contrastive and triplet loss, there are other valid (and perhaps less complicated) ways of fine-tuning a pretrained model that yield good distance metrics. For example, we could explore a cross-entropy loss that learns to predict a class (genuine, skilled forgery, unskilled forgery), given a pair of images. (The reader is encouraged to review the work of Boudiaf, Malik, et al. for further insight on how cross-entropy loss compares with other metric learning losses.) <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup></p>
</li>
</ul>
<p>Naturally, these limitations are opportunities for future work and experiments.</p>
<h2 id="summary-and-conclusion">Summary and Conclusion</h2>
<p>Metric learning helps us train a model that yields representations of similarity adapted to our task and dataset. In this post, we have covered several loss functions for metric learning (contrastive loss, triplet loss, quadruplet loss, and group loss). We also reported on a set of experiments training a model for signature verification on the CEDAR dataset.</p>
<p>We learned that:</p>
<ul>
<li>Pretrained models can be strong baselines for the signature verification task. When the problem is easy (i.e., unskilled forgery), we see that a pretrained VGG16 model yields 89.5% accuracy and 74.3% when the problem is hard (skilled forgeries).</li>
<li>Triplet loss achieves better performance compared to contrastive loss, and learns a better overall structure of the embedding space.</li>
<li>Triplet loss (with online triplet mining) is more efficient to train, compared to contrastive loss (i.e., training time to reach maximum accuracy). It does not require the manual construction of training triplets (as required in contrastive loss) and uses intelligent sampling to select informative triplets. This is very useful when running experiments to explore a large hyperparameter space. As an example, we found that finetuning a ResNet50 model with triplet loss was completed in three minutes, while a contrastive loss model needed eight hours to reach max accuracy.</li>
<li>With respect to implementation, we found the Tensorflow addons implementation of triplet loss <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup> to be useful.</li>
<li>Fine-tuning an intermediate model can not only result in a reduced model size, but also improved accuracy. Our experiments suggest that for the task of signature verification, fine-tuning a subset of the ResNet50 model resulted in a smaller model size, compared to fine-tuning the entire ResNet50 model.</li>
<li>A good practice is to use the same distance metric across training and test. L2 distance is more computationally efficient compared to cosine distance, making it the better choice in production. L2 similarity measures are also well-supported by libraries designed for fast approximate nearest neighbor search (e.g., Annoy, FAISS, ScaNN). Also note that computing L2 distance on normalized embeddings (recall our L2 normalization layer earlier) yields the equivalent of cosine distance.</li>
</ul>
<h2 id="references">References</h2>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Understanding Ranking Loss, Contrastive Loss, Margin Loss, Triplet Loss, Hinge Loss and all those confusing names. <a href="https://gombru.github.io/2019/04/03/ranking_loss/">https://gombru.github.io/2019/04/03/ranking_loss/</a> <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>Schroff, Florian, Dmitry Kalenichenko, and James Philbin. &ldquo;Facenet: A unified embedding for face recognition and clustering.&rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2015. <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>Chen, Weihua, et al. &ldquo;Beyond triplet loss: a deep quadruplet network for person re-identification.&rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2017. <a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>Elezi, Ismail, et al. &ldquo;The group loss for deep metric learning.&rdquo; European Conference on Computer Vision. Springer, Cham, 2020. <a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p>Li H, Xu Z, Taylor G, Studer C, Goldstein T. Visualizing the loss landscape of neural nets. arXiv preprint arXiv:1712.09913. 2017 Dec 28. <a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6" role="doc-endnote">
<p>Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M., &amp; Kim, B. (2018). Sanity checks for saliency maps. arXiv preprint arXiv:1810.03292. <a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7" role="doc-endnote">
<p>Selvaraju, Ramprasaath R., et al. &ldquo;Grad-cam: Visual explanations from deep networks via gradient-based localization.&rdquo; Proceedings of the IEEE international conference on computer vision. 2017. <a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8" role="doc-endnote">
<p>Grad-CAM class activation visualization <a href="https://keras.io/examples/vision/grad_cam/">https://keras.io/examples/vision/grad_cam/</a> <a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9" role="doc-endnote">
<p>Caron, Mathilde, et al. &ldquo;Deep clustering for unsupervised learning of visual features.&rdquo; Proceedings of the European Conference on Computer Vision (ECCV). 2018. <a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10" role="doc-endnote">
<p>Boudiaf, Malik, et al. &ldquo;A unifying mutual information view of metric learning: cross-entropy vs. pairwise losses.&rdquo; European Conference on Computer Vision. Springer, Cham, 2020. <a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11" role="doc-endnote">
<p>TensorFlow Addons Losses: TripletSemiHardLoss <a href="https://www.tensorflow.org/addons/tutorials/losses_triplet">https://www.tensorflow.org/addons/tutorials/losses_triplet</a> <a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

    <div class="spacer"></div>
    <div>
      <div style="width: 100%; height: 2px; background: #ccc; margin-top: -1px; margin-bottom: -1px;"></div>
    </div>
    <div class="spacer"></div>
  </div>
</div>


<div class="container">
  <div class="spacer"></div>
  <h2 class="clear">Read more</h2>
  <div class="spacer"></div>
  <div style="display: grid; grid-template-columns: 1fr 1fr; grid-column-gap: 2ch;">
    <div>
      
      <div class="small">Newer</div>
      <div>
  <h5 style="margin-bottom: 0px;">
    <span>Jul 7, 2021</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2021/07/07/exploring-multi-objective-hyperparameter-optimization.html"><strong>Exploring Multi-Objective Hyperparameter Optimization</strong></a></div>
  
  <div class="spacer"></div>
</div>


      
    </div>
    <div>
      
      <div class="small">Older</div>
      <div>
  <h5 style="margin-bottom: 0px;">
    <span>May 27, 2021</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2021/05/27/pretrained-models-as-a-strong-baseline-for-automatic-signature-verification.html"><strong>Pretrained Models as a Strong Baseline for Automatic Signature Verification</strong></a></div>
  
  <div class="spacer"></div>
</div>


      
    </div>
  </div>
</div>

<div class="container">
<div class="spacer"></div>
<div>
  <div style="width: 100%; height: 2px; background: #ccc; margin-top: -1px; margin-bottom: -1px;"></div>
</div>
<div class="spacer"></div>
<div class="spacer"></div>
</div>

<div class="container">
  

<h2 class="clear">Latest posts</h2>
<div class="spacer"></div>

<div id="posts-holder"> 
  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Jul 11, 2022</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
  <a href="/2022/07/11/automated-metrics-for-evaluating-text-style-transfer.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/image3-tst3.png" />
  </a>
  
  <div>
    
    <a href="/2022/07/11/automated-metrics-for-evaluating-text-style-transfer.html"
       ><h2 style="margin-bottom: 4px;">Automated Metrics for Evaluating Text Style Transfer</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by  Andrew &amp; Melanie &middot; 
    </span>
    By Andrew Reed and Melanie Beck
Blog Series This post serves as Part 3 of a three part blog series on the NLP task of Text Style Transfer. In this post, we expand our modeling efforts to a more challenging dataset and propose a set of custom evaluation metrics specific to our task.   Part 1: An Introduction to Text Style Transfer    Part 2: Neutralizing Subjectivity Bias with HuggingFace Transformers    Part 3: Automated Metrics for Evaluating Text Style Transfer      Automated Metrics for Evaluating Text Style Transfer In our previous blog post, we took an in-depth look at how to neutralize subjectivity bias in text using HuggingFace transformers.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/2022/07/11/automated-metrics-for-evaluating-text-style-transfer.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>May 5, 2022</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
  <a href="/2022/05/05/neutralizing-subjectivity-bias-with-huggingface-transformers.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/fig7-tst2.png" />
  </a>
  
  <div>
    
    <a href="/2022/05/05/neutralizing-subjectivity-bias-with-huggingface-transformers.html"
       ><h2 style="margin-bottom: 4px;">Neutralizing Subjectivity Bias with HuggingFace Transformers</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="https://twitter.com/andrewrreed">Andrew Reed</a>
        &middot; </span
      >
    </span>
    Blog Series This post serves as Part 2 of a three part blog series on the NLP task of Text Style Transfer. In this post, we introduce the applied use case through which we'll explore text style transfer and discuss our modeling approach.   Part 1: An Introduction to Text Style Transfer    Part 2: Neutralizing Subjectivity Bias with HuggingFace Transformers    Part 3: Automated Metrics for Evaluating Text Style Transfer      Subjective language is all around us &ndash; product advertisements, social marketing campaigns, personal opinion blogs, political propaganda, and news media, just to name a few examples.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/2022/05/05/neutralizing-subjectivity-bias-with-huggingface-transformers.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Mar 22, 2022</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
  <a href="/2022/03/22/an-introduction-to-text-style-transfer.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/parallel_nonparallel-1647959058.png" />
  </a>
  
  <div>
    
    <a href="/2022/03/22/an-introduction-to-text-style-transfer.html"
       ><h2 style="margin-bottom: 4px;">An Introduction to Text Style Transfer</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="https://twitter.com/andrewrreed/">Andrew Reed</a>
        &middot; </span
      >
    </span>
    Blog Series This post serves as Part 1 of a three part blog series on the NLP task of Text Style Transfer. In this post, we provide a broad introduction to the topic and tee up the direction of our research.   Part 1: An Introduction to Text Style Transfer    Part 2: Neutralizing Subjectivity Bias with HuggingFace Transformers    Part 3: Automated Metrics for Evaluating Text Style Transfer      Today’s world of natural language processing (NLP) is driven by powerful transformer-based models that can automatically caption images, answer open-ended questions, engage in free dialog, and summarize long-form bodies of text &ndash; of course, with varying degrees of success.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/2022/03/22/an-introduction-to-text-style-transfer.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Jan 31, 2022</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
  <a href="/2022/01/31/why-and-how-convolutions-work-for-video-classification.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/Fig_04_3D_conv_gray_video_kernel_2-1643658549.png" />
  </a>
  
  <div>
    
    <a href="/2022/01/31/why-and-how-convolutions-work-for-video-classification.html"
       ><h2 style="margin-bottom: 4px;">Why and How Convolutions Work for Video Classification</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="https://uk.linkedin.com/in/daniel-valdez-balderas-9051323b">Daniel Valdez-Balderas</a>
        &middot; </span
      >
    </span>
    Video classification is perhaps the simplest and most fundamental of the tasks in the field of video understanding. In this blog post, we’ll take a deep dive into why and how convolutions work for video classification. Our goal is to help the reader develop an intuition about the relationship between space (the image part of video) and time (the sequence part of video), and pave the way to a deep understanding of video classification algorithms.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/2022/01/31/why-and-how-convolutions-work-for-video-classification.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Dec 14, 2021</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
  <a href="/2021/12/14/an-introduction-to-video-understanding-capabilities-and-applications.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/video_classification-1639064585.png" />
  </a>
  
  <div>
    
    <a href="/2021/12/14/an-introduction-to-video-understanding-capabilities-and-applications.html"
       ><h2 style="margin-bottom: 4px;">An Introduction to Video Understanding: Capabilities and Applications</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="https://uk.linkedin.com/in/daniel-valdez-balderas-9051323b">Daniel Valdez Balderas</a>
        &middot; </span
      >
    </span>
    Video footage constitutes a significant portion of all data in the world. The 30 thousand hours of video uploaded to Youtube every hour is a part of that data; another portion is produced by 770 million surveillance cameras globally. In addition to being plentiful, video data has tremendous capacity to store useful information. Its vastness, richness, and applicability make the understanding of video a key activity within the field of computer vision.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/2021/12/14/an-introduction-to-video-understanding-capabilities-and-applications.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Sep 22, 2021</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
  <a href="/2021/09/22/automatic-summarization-from-textrank-to-transformers.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/summarize_blog/summarize_crop.png" />
  </a>
  
  <div>
    
    <a href="/2021/09/22/automatic-summarization-from-textrank-to-transformers.html"
       ><h2 style="margin-bottom: 4px;">Automatic Summarization from TextRank to Transformers</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="https://www.linkedin.com/in/melanierbeck/">Melanie Beck</a>
        &middot; </span
      >
    </span>
    Automatic summarization is a task in which a machine distills a large amount of data into a subset (the summary) that retains the most relevant and important information from the whole. While traditionally applied to text, automatic summarization can include other formats such as images or audio. In this article we’ll cover the main approaches to automatic text summarization, talk about what makes for a good summary, and introduce Summarize. &ndash; a summarization prototype we built that showcases several automatic summarization techniques.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/2021/09/22/automatic-summarization-from-textrank-to-transformers.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
</div>

<div>
  <button id="load_more" style="width: 100%;">load more</button>
</div>
<div class="spacer"></div>
<div class="spacer"></div>

<script>
  window.addEventListener('load', () => {
    let $posts_holder = document.getElementById('posts-holder')
    let $load_more = document.getElementById('load_more')
    let next_page = 2
    $load_more.addEventListener('click', () => {
      fetch(`/posts/page/${next_page}.html`).then(r =>r.text()).then(r => {
        let el = document.createElement('html')
        el.innerHTML = r
        next_page += 1
        let $posts = el.querySelector('#posts-holder').children
        for (let i=0; i< $posts.length; i++) {
          let $post = $posts[i].cloneNode(true)
          $posts_holder.appendChild($post)
        }
      })
    })
  })
</script>


  <h3 class="clear">Popular posts</h3>
<div class="spacer"></div>
<div>
  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Oct 30, 2019</span> &middot;
    <span style="text-transform: capitalize;">
      newsletter
    </span>
  </h5>
  
    <div><a href="/2019/10/30/exciting-applications-of-graph-neural-networks.html"><strong>Exciting Applications of Graph Neural Networks</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Nov 14, 2018</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2018/11/14/federated-learning-distributed-machine-learning-with-data-locality-and-privacy.html"><strong>Federated learning: distributed machine learning with data locality and privacy</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Apr 10, 2018</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2018/04/10/pytorch-for-recommenders-101.html"><strong>PyTorch for Recommenders 101</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Oct 4, 2017</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2017/10/04/first-look-using-three.js-for-2d-data-visualization.html"><strong>First Look: Using Three.js for 2D Data Visualization</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Aug 22, 2016</span> &middot;
    <span style="text-transform: capitalize;">
      whitepaper
    </span>
  </h5>
  
    <div><a href="/2016/08/22/under-the-hood-of-the-variational-autoencoder-in-prose-and-code.html"><strong>Under the Hood of the Variational Autoencoder (in Prose and Code)</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Feb 24, 2016</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2016/02/24/hello-world-in-keras-or-scikit-learn-versus-keras.html"><strong>&#34;Hello world&#34; in Keras (or, Scikit-learn versus Keras)</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
</div>

</div>

<div class="spacer"></div>
<div style="background: #efefef;">
  <div class="spacer"></div>
  <div class="spacer"></div>
  <div class="container">
  <h1 class="clear">Reports</h1>
  <div style="color: #444;">In-depth guides to specific machine learning capabilities</div>
</div>
<div class="spacer"></div>
<div style="max-width: 96ch; margin: 0 auto; padding-left: 1ch; padding-right: 1ch;">
  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF22</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://concept-drift.fastforwardlabs.com/" target="_blank">Inferring Concept Drift Without Labeled Data</a></h2>
  <a class="report-image" href="https://concept-drift.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff22-combo.png" />
  </a>
  <div class="small">Concept drift occurs when the statistical properties of a target domain change overtime causing model performance to degrade. Drift detection is generally achieved by monitoring a performance metric of interest and triggering a retraining pipeline when that metric falls below some designated threshold. However, this approach assumes ample labeled data is available at prediction time - an unrealistic constraint for many production systems. In this report, we explore various approaches for dealing with concept drift when labeled data is not readily accessible.</div>
  <div><a href="https://concept-drift.fastforwardlabs.com/" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF19</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://session-based-recommenders.fastforwardlabs.com/" target="_blank">Session-based Recommender Systems</a></h2>
  <a class="report-image" href="https://session-based-recommenders.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff19-combo.png" />
  </a>
  <div class="small">Being able to recommend an item of interest to a user (based on their past preferences) is a highly relevant problem in practice. A key trend over the past few years has been session-based recommendation algorithms that provide recommendations solely based on a user’s interactions in an ongoing session, and which do not require the existence of user profiles or their entire historical preferences. This report explores a simple, yet powerful, NLP-based approach (word2vec) to recommend a next item to a user. While NLP-based approaches are generally employed for linguistic tasks, here we exploit them to learn the structure induced by a user’s behavior or an item’s nature.</div>
  <div><a href="https://session-based-recommenders.fastforwardlabs.com/" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF18</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://few-shot-text-classification.fastforwardlabs.com/" target="_blank">Few-Shot Text Classification</a></h2>
  <a class="report-image" href="https://few-shot-text-classification.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff18-combo.png" />
  </a>
  <div class="small">Text classification can be used for sentiment analysis, topic assignment, document identification, article recommendation, and more. While dozens of techniques now exist for this fundamental task, many of them require massive amounts of labeled data in order to be useful. Collecting annotations for your use case is typically one of the most costly parts of any machine learning application. In this report, we explore how latent text embeddings can be used with few (or even zero) training examples and provide insights into best practices for implementing this method.</div>
  <div><a href="https://few-shot-text-classification.fastforwardlabs.com/" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF16</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://structural-time-series.fastforwardlabs.com" target="_blank">Structural Time Series</a></h2>
  <a class="report-image" href="https://structural-time-series.fastforwardlabs.com" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff16-combo.png" />
  </a>
  <div class="small">Time series data is ubiquitous. This report examines generalized additive models, which give us a simple, flexible, and interpretable means for modeling time series by decomposing them into structural components. We look at the benefits and trade-offs of taking a curve-fitting approach to time series, and demonstrate its use via Facebook’s Prophet library on a demand forecasting problem.</div>
  <div><a href="https://structural-time-series.fastforwardlabs.com" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF15</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://meta-learning.fastforwardlabs.com" target="_blank">Meta-Learning</a></h2>
  <a class="report-image" href="https://meta-learning.fastforwardlabs.com" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff15-combo.png" />
  </a>
  <div class="small">In contrast to how humans learn, deep learning algorithms need vast amounts of data and compute and may yet struggle to generalize. Humans are successful in adapting quickly because they leverage their knowledge acquired from prior experience when faced with new problems. In this report, we explain how meta-learning can leverage previous knowledge acquired from data to solve novel tasks quickly and more efficiently during test time</div>
  <div><a href="https://meta-learning.fastforwardlabs.com" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF14</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://qa.fastforwardlabs.com" target="_blank">Automated Question Answering</a></h2>
  <a class="report-image" href="https://qa.fastforwardlabs.com" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff14-combo.png" />
  </a>
  <div class="small">Automated question answering is a user-friendly way to extract information from data using natural language. Thanks to recent advances in natural language processing, question answering capabilities from unstructured text data have grown rapidly. This blog series offers a walk-through detailing the technical and practical aspects of building an end-to-end question answering system.</div>
  <div><a href="https://qa.fastforwardlabs.com" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF13</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://ff13.fastforwardlabs.com" target="_blank">Causality for Machine Learning</a></h2>
  <a class="report-image" href="https://ff13.fastforwardlabs.com" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff13-combo.png" />
  </a>
  <div class="small">The intersection of causal inference and machine learning is a rapidly expanding area of research that&#39;s already yielding capabilities to enable building more robust, reliable, and fair machine learning systems. This report offers an introduction to causal reasoning including causal graphs and invariant prediction and how to apply causal inference tools together with classic machine learning techniques in multiple use-cases.</div>
  <div><a href="https://ff13.fastforwardlabs.com" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF06-2020</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://ff06-2020.fastforwardlabs.com" target="_blank">Interpretability</a></h2>
  <a class="report-image" href="https://ff06-2020.fastforwardlabs.com" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff06-2020-combo.png" />
  </a>
  <div class="small">Interpretability, or the ability to explain why and how a system makes a decision, can help us improve models, satisfy regulations, and build better products. Black-box techniques like deep learning have delivered breakthrough capabilities at the cost of interpretability. In this report, recently updated to include techniques like SHAP, we show how to make models interpretable without sacrificing their capabilities or accuracy.</div>
  <div><a href="https://ff06-2020.fastforwardlabs.com" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF12</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://ff12.fastforwardlabs.com" target="_blank">Deep Learning for Anomaly Detection</a></h2>
  <a class="report-image" href="https://ff12.fastforwardlabs.com" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff12-combo.png" />
  </a>
  <div class="small">From fraud detection to flagging abnormalities in imaging data, there are countless applications for automatic identification of abnormal data. This process can be challenging, especially when working with large, complex data. This report explores deep learning approaches (sequence models, VAEs, GANs) for anomaly detection, when to use them, performance benchmarks, and product possibilities.</div>
  <div><a href="https://ff12.fastforwardlabs.com" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF11</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://transfer-learning.fastforwardlabs.com/" target="_blank">Transfer Learning for Natural Language Processing</a></h2>
  <a class="report-image" href="https://transfer-learning.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff11-combo.png" />
  </a>
  <div class="small">Natural language processing (NLP) technologies using deep learning can translate language, answer questions, and generate human-like text But these deep learning techniques require large, costly labeled datasets, expensive infrastructure, and scarce expertise. Transfer learning lifts these constraints by reusing and adapting a model’s understanding of language. Transfer learning is a good fit for any NLP application. In this report, we show how to use transfer learning to build high-performance NLP systems with minimal resources.</div>
  <div><a href="https://transfer-learning.fastforwardlabs.com/" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF10</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://lwlld.fastforwardlabs.com/" target="_blank">Learning with Limited Labeled Data</a></h2>
  <a class="report-image" href="https://lwlld.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff10-combo.png" />
  </a>
  <div class="small">Being able to learn with limited labeled data relaxes the stringent labeled data requirement for supervised machine learning. This report focuses on active learning, a technique that relies on collaboration between machines and humans to label smartly. Active learning reduces the number of labeled examples required to train a model, saving time and money while obtaining comparable performance to models trained with much more data. With active learning, enterprises can leverage their large pool of unlabeled data to open up new product possibilities.</div>
  <div><a href="https://lwlld.fastforwardlabs.com/" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF09</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://federated.fastforwardlabs.com/" target="_blank">Federated Learning</a></h2>
  <a class="report-image" href="https://federated.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff09-combo.png" />
  </a>
  <div class="small">Federated Learning makes it possible to build machine learning systems without direct access to training data. The data remains in its original location, which helps to ensure privacy and reduces communication costs. Federated learning is a great fit for smartphones and edge hardware, healthcare and other privacy-sensitive use cases, and industrial applications such as predictive maintenance.</div>
  <div><a href="https://federated.fastforwardlabs.com/" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF07</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://semantic-recommendations.fastforwardlabs.com/" target="_blank">Semantic Recommendations</a></h2>
  <a class="report-image" href="https://semantic-recommendations.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff07-combo.png" />
  </a>
  <div class="small">The internet has given us an avalanche of options for what to read, watch and buy. Because of this, recommendation algorithms, which find items that will interest a particular person, are more important than ever. In this report we explore recommendation systems that make use of the semantic content of items and users to deliver richer recommendations across multiple industries.</div>
  <div><a href="https://semantic-recommendations.fastforwardlabs.com/" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF04</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://summarization.fastforwardlabs.com/" target="_blank">Summarization</a></h2>
  <a class="report-image" href="https://summarization.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff04-combo.png" />
  </a>
  <div class="small">This report explores methods for extractive summarization, a capability that allows one to automatically summarize documents.  This technique has a wealth of applications: from the ability to distill thousands of product reviews, extract the most important content from long news articles, or automatically cluster customer bios into personas.</div>
  <div><a href="https://summarization.fastforwardlabs.com/" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF03-2019</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://deep-learning-image-analysis.fastforwardlabs.com/" target="_blank">Deep Learning for Image Analysis: 2019 Edition</a></h2>
  <a class="report-image" href="https://deep-learning-image-analysis.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff03-2019-combo.png" />
  </a>
  <div class="small">Convolutional Neural Networks (CNN) excel at learning meaningful representations of features and concepts within images. These capabilities make CNNs extremely valuable for solving problems in domains such as medical imaging, autonomous driving, manufacturing, robotics, and urban planning. In this report, we show how to select the right deep learning models for image analysis tasks and techniques for debugging deep learning models.</div>
  <div><a href="https://deep-learning-image-analysis.fastforwardlabs.com/" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF03</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://deep-learning-image-classic.fastforwardlabs.com/" target="_blank">Deep Learning: Image Analysis</a></h2>
  <a class="report-image" href="https://deep-learning-image-classic.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff03-classic-combo.png" />
  </a>
  <div class="small">Deep learning, or highly-connected neural networks, offers fascinating new capabilities for image analysis. Using deep learning, computers can now learn to identify objects in images. This report explores the history and current state of the field, predicts future developments, and explains how to apply deep learning today.</div>
  <div><a href="https://deep-learning-image-classic.fastforwardlabs.com/" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
</div>


</div>

<div class="spacer"></div>
<div class="spacer"></div>
 
<div class="container">
  <h1 class="clear">Prototypes</h1>
  <div style="color: #444;">Machine learning prototypes and interactive notebooks</div>
  <div class="spacer"></div>
</div>
<div id="prototypes-holder">
  
  <div style="display: flex; flex-direction: column; position: relative; padding-left: 1ch; padding-right: 1ch; height: 360px;">
  <div>
    <h5 style="margin-bottom: 0">Library</h5>
    <h2 style="padding-top: 0; margin-bottom: 0;"><a href="https://neuralqa.fastforwardlabs.com" target="_blank">NeuralQA</a></h2>
  </div>
  <div style="flex: 1 1 auto; position: relative;">
    <a href="https://neuralqa.fastforwardlabs.com" target="_blank" style="display: block; position: absolute; top: 0.375rem; width: 100%; height: calc(100% - 0.875rem); background-image: url('/images/hugo/neuralqa-1596123511.jpg'); background-size: contain; background-position: center left; background-repeat: no-repeat;"></a>
  </div>
  <div>
    <div class="small">A usable library for question answering on large datasets.</div>
    <div style="width: 100%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"><a href="https://neuralqa.fastforwardlabs.com" target="_blank">https://neuralqa.fastforwardlabs.com</a></div>
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
  <div style="display: flex; flex-direction: column; position: relative; padding-left: 1ch; padding-right: 1ch; height: 360px;">
  <div>
    <h5 style="margin-bottom: 0">Notebook</h5>
    <h2 style="padding-top: 0; margin-bottom: 0;"><a href="https://colab.research.google.com/drive/1tTiOgJ7xvy3sjfiFC9OozbjAX1ho8WN9?usp=sharing" target="_blank">Explain BERT for Question Answering Models</a></h2>
  </div>
  <div style="flex: 1 1 auto; position: relative;">
    <a href="https://colab.research.google.com/drive/1tTiOgJ7xvy3sjfiFC9OozbjAX1ho8WN9?usp=sharing" target="_blank" style="display: block; position: absolute; top: 0.375rem; width: 100%; height: calc(100% - 0.875rem); background-image: url('/images/hugo/distilexplanation-1592852137.jpg'); background-size: contain; background-position: center left; background-repeat: no-repeat;"></a>
  </div>
  <div>
    <div class="small">Tensorflow 2.0 notebook to explain and visualize a HuggingFace BERT for Question Answering model.</div>
    <div style="width: 100%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"><a href="https://colab.research.google.com/drive/1tTiOgJ7xvy3sjfiFC9OozbjAX1ho8WN9?usp=sharing" target="_blank">https://colab.research.google.com/drive/1tTiOgJ7xvy3sjfiFC9OozbjAX1ho8WN9?usp=sharing</a></div>
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
  <div style="display: flex; flex-direction: column; position: relative; padding-left: 1ch; padding-right: 1ch; height: 360px;">
  <div>
    <h5 style="margin-bottom: 0">Notebooks</h5>
    <h2 style="padding-top: 0; margin-bottom: 0;"><a href="https://qa.fastforwardlabs.com" target="_blank">NLP for Question Answering</a></h2>
  </div>
  <div style="flex: 1 1 auto; position: relative;">
    <a href="https://qa.fastforwardlabs.com" target="_blank" style="display: block; position: absolute; top: 0.375rem; width: 100%; height: calc(100% - 0.875rem); background-image: url('/images/uploads/qa.png'); background-size: contain; background-position: center left; background-repeat: no-repeat;"></a>
  </div>
  <div>
    <div class="small">Ongoing posts and code documenting the process of building a question answering model.</div>
    <div style="width: 100%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"><a href="https://qa.fastforwardlabs.com" target="_blank">https://qa.fastforwardlabs.com</a></div>
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
  <div style="display: flex; flex-direction: column; position: relative; padding-left: 1ch; padding-right: 1ch; height: 360px;">
  <div>
    <h5 style="margin-bottom: 0">Notebook</h5>
    <h2 style="padding-top: 0; margin-bottom: 0;"><a href="https://colab.research.google.com/drive/1pjPzsw_uZew-Zcz646JTkRDhF2GkPk0N" target="_blank">Interpretability Revisited: SHAP and LIME</a></h2>
  </div>
  <div style="flex: 1 1 auto; position: relative;">
    <a href="https://colab.research.google.com/drive/1pjPzsw_uZew-Zcz646JTkRDhF2GkPk0N" target="_blank" style="display: block; position: absolute; top: 0.375rem; width: 100%; height: calc(100% - 0.875rem); background-image: url('/images/uploads/shap-and-lime.png'); background-size: contain; background-position: center left; background-repeat: no-repeat;"></a>
  </div>
  <div>
    <div class="small">Explore how to use LIME and SHAP for interpretability.</div>
    <div style="width: 100%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"><a href="https://colab.research.google.com/drive/1pjPzsw_uZew-Zcz646JTkRDhF2GkPk0N" target="_blank">https://colab.research.google.com/drive/1pjPzsw_uZew-Zcz646JTkRDhF2GkPk0N</a></div>
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
</div>
<div class="container">
  <div ><button id="load_all_prototypes" style="width: 100%;">load all</button></div>
</div>

<script>
  
  window.addEventListener('load', () => {
    let $prototypes_holder = document.getElementById('prototypes-holder')
    let $load_more = document.getElementById('load_all_prototypes')
    $load_more.addEventListener('click', () => {
      fetch(`/prototypes.html`).then(r =>r.text()).then(r => {
        $load_more.remove()
        let el = document.createElement('html')
        el.innerHTML = r
        let $posts = el.querySelector('#prototypes-holder')
        $prototypes_holder.innerHTML = $posts.innerHTML
      })
    })
  })
</script>



<div class="spacer"></div>
<div class="spacer"></div>

<div class="container">
  <div>
    <h1 class="clear">Cloudera Fast Forward Labs</h1>
    <div>
      <i>Making the recently possible useful.</i><br />
      <p></p>
      <p>Cloudera Fast Forward Labs is an applied machine learning research group. Our mission is to empower enterprise data science practitioners to apply emergent academic research to production machine learning use cases in practical and socially responsible ways, while also driving innovation through the Cloudera ecosystem.  Our team brings thoughtful, creative, and diverse perspectives to deeply researched work. In this way, we strive to help organizations make the most of their ML investment as well as educate and inspire the broader machine learning and data science community.</p>
      <a
        href="https://www.cloudera.com/products/fast-forward-labs-research.html"
        >Cloudera</a
      >&nbsp;&nbsp;
      <a
        href="https://blog.fastforwardlabs.com"
        >Blog</a
      >&nbsp;&nbsp;
      <a href="https://twitter.com/fastforwardlabs">Twitter</a>
    </div>
  </div>
</div>



<div class="spacer"></div>
<div class="spacer"></div>


      </main>
 </body>
</html>
