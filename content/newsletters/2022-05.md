---
title: "CFFL May Newsletter"
date: 2022-05-18
preview_image: /images/hugo/fig4-tst2.png
post_type: Newsletter
# external_url: 
---

# May 2022

Welcome to the May edition of the Cloudera Fast Forward Labs newsletter.  This month we share the next chapter in our current research endeavor and chat about some great reads. 

---

## We’re hiring!

**[Research Engineer (US/UK/Canada - Remote)](https://cloudera.wd5.myworkdayjobs.com/External_Career/job/US-California-Remote/Research-Engineer--US-UK-Canada---Remote-_220616-1)**

As a Research Engineer on the Cloudera Fast Forwards Labs team, much of your time will be spent developing new applied research and sharing it with the community via reports and blog posts. You’ll also have the opportunity to hone your engineering skills by contributing to and supporting our catalog of applied machine learning prototypes. And as an ML advocate within Cloudera, you’ll be able to influence the shape of our product features to ensure their amenability for ML and data science practitioners. 

---

## New Research!

You’ve waited long enough — the next chapter of our investigation of Text Style Transfer is here! This time we’re applying the general technique to a specific task — neutralizing subjectivity bias. (If you missed it, check out our [Introduction to Text Style Transfer](https://blog.fastforwardlabs.com/2022/03/22/an-introduction-to-text-style-transfer.html) for some background.) 

### **[Neutralizing Subjectivity Bias with HuggingFace Transformers](https://blog.fastforwardlabs.com/2022/05/05/neutralizing-subjectivity-bias-with-huggingface-transformers.html)**

There are certain modes of communication today —  textbooks, encyclopedias, [some] news outlets, etc. — that strive for objectivity. However, subjectivity bias often creeps in. Subjectivity bias occurs when language that is intended to be neutral and fair is skewed by feeling, opinion, or taste. It can occur consciously or unconsciously and the underlying intent can be benevolent or malevolent.  Modeling how these biases can be removed from objective text has been the focus of our recent exploration of Text Style Transfer (TST), a natural language generation task that aims to automatically control the style attributes of text while preserving the content. 

The task of “neutralizing subjectivity bias” casts **subjectivity** as the style attribute. Given a subjective sentence, the goal is to generate a modified version of the sentence with the same semantic meaning, but in a neutral tone of voice. In the example below, we see that the source sentence uses the adjective “beautiful” when describing “Newark Academy’s campus”, which is a *subjective intensifier* that implies the author’s feelings about the topic at hand. This sentence can be “neutralized” simply by removing the subjective term as seen in the figures below.

![Subjectivity-Example](/images/hugo/fig1-11651776686.png)

![Subjectivity-Model](/images/hugo/fig4-tst2.png)

In this blog post, you can learn more about our approach, including the dataset we use and how the BART Transformer model can be leveraged for the task! 

---

## Fast Forward Live!

Check out replays of livestreams covering some of our research from last year.

[**Deep Learning for Automatic Offline Signature Verification**](https://youtu.be/7_MlFxyPYSg)

[**Session-based Recommender Systems**](https://www.youtube.com/watch?v=JoRx6udpnbI)

[**Few-Shot Text Classification**](https://youtu.be/oLFqTj5FcEA)

**[Representation Learning for Software Engineers](https://youtu.be/o4gQLVzIm5U)**

---

## Recommended Reading

CFFLers share their favorite reads of the month.

[Beyond Interpretability: Developing a language to shape our relationship with AI](https://www.youtube.com/watch?v=Ub45cGEcTB0)

As machine learning is used to make increasingly complex and important decisions, it becomes increasingly important that we find a way to understand why and how computers arrive at the decisions they do. In this superb keynote from ICLR 2022, [Been Kim](https://twitter.com/_beenkim?s=20&t=5WRIEballDR4PwBjrvrpWg) highlights the need for “a language” to help better align AI-driven outcomes with our human values and intentions. By mapping humans’ representational space to that of computers (which is fundamentally different), this language will reflect the nature of machines (like human languages do) and help expand human knowledge with concepts that can only be derived from a computer's representational space.

While this talk provokes more questions than it provides answers, the importance of these questions cannot be overstated. Been [summarizes the motivation](https://medium.com/@beenkim/beyond-interpretability-4bf03bbd9394) well: 

“AI is more than just a tool; we will be influenced by it, which will in return influence the next generation of AI. Without a language to meaningfully communicate with it, we don’t understand its decisions and, therefore, won’t know what we are creating. Building a language to communicate with AI isn’t going to be easy, but quite frankly, it’s the only way to gain control of the way we want to live. Languages shape the way we think. We have an opportunity to shape our own thinking and future machines.”  *— [Andrew](https://twitter.com/andrewrreed)*


[UNSUPERVISED SEMANTIC SEGMENTATION BY DISTILLING FEATURE CORRESPONDENCES](https://arxiv.org/abs/2203.08414)

No, the features aren’t sending quaint snail mail. Instead, this paper builds on prior art in the field of semantic image segmentation based on the idea of “feature correspondences” — a tensor that captures the cosine similarity of feature vectors at point locations between images. 

![feature_correspondences](/images/hugo/feature_correspondences-1652992227.png)

The blue cross represents a specific spatial query in the image (left) which is found to be semantically related to many other “sky” pixels found both within the original image (middle) and in other, similar images (right). The same can be said for the red cross, a pixel that is found to be semantically similar to other “motorcycle” pixels, etc. 

It turns out that these feature correspondences correlate with semantic segmentation labels, providing a strong signal on which to build an unsupervised image segmentation system. This paper goes into extraordinary detail on how they built that system (named STEGO), including how they developed a specialized loss function that helps “cluster” similar image pixels based on their feature vector. 

![STEGO_results](/images/hugo/STEGO_results-1652992220.png)

And their results are astounding! Above you can see some examples of how their model performs on images from the CoCo Stuff benchmark. Their STEGO model is able to capture fine details in images that, in some cases, outperform the ground truth labels. And they did it all without ever using those ground truth labels during training. Pretty impressive! — *[Melanie](https://www.linkedin.com/in/melanierbeck/)*

****[Efficient Classification of Long Documents Using Transformers](https://arxiv.org/abs/2203.11258)****

This paper is a short, breezy read with a provocative result. In it, the authors highlight the importance of **relevant baselines and benchmarks** when evaluating the results of a new model. They focus on the task of long document classification and point out that several recent advances in this field (Transformer-based models such as [Longformer](https://arxiv.org/abs/2004.05150), [ToBERT](https://arxiv.org/abs/1910.10781), and [CogLTX](https://proceedings.neurips.cc/paper/2020/hash/96671501524948bc3937b4b30d0e57b9-Abstract.html)) have each, in their own way, failed to consider reasonable baselines, either in terms of the datasets these models are benchmarked against or in terms of relevant models for comparison. This paper goes on to demonstrate that most of these more sophisticated models fail to outperform simpler BERT classifier baselines on a suite of five long-document classification datasets. This finding is even more impressive because some of these more sophisticated models take 12x longer in inference and double the GPU memory for training! All that for subpar performance... Remember this folks: baselines are IMPORTANT and often a simpler solution is sufficient.  — *[Melanie](https://www.linkedin.com/in/melanierbeck/)*