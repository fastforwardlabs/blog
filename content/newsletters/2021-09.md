---
date: 2021-09-21T12:40:00Z
---

Welcome to the September Cloudera Fast Forward Labs newsletter.

---

## New research!

We like to build things. In the past couple of months we've been hard at work on a new prototype, as well as wrapping up a few loose threads from various research directions that we've touched on so far this year. Our efforts have culminated in a fun new application and blog posts.

### [Summarize.](https://blog.fastforwardlabs.com/2021/09/22/automatic-summarization-from-textrank-to-transformers.html)

Automatic summarization is a task in which a machine distills a large amount of data into a subset (the summary) that retains the most relevant and important information from the whole. While traditionally applied to text, automatic summarization can include other formats such as images or audio. In this post we cover the main approaches to automatic text summarization, talk about what makes for a _good_ summary, and introduce **Summarize.** -- a summarization [prototype](https://github.com/fastforwardlabs/summarize.) we built that showcases both extractive and abstractive automatic summarization models.

![summarize_crop.png](/images/hugo/summarize_crop-1632401225.png)

### [Extractive Summarization with SentenceBERT](https://blog.fastforwardlabs.com/2021/09/21/extractive-summarization-with-sentence-bert.html)

In this post, we dive deeper into how we trained a SentenceBERT model to perform extractive summarization, from model architecture to considerations for training and inference. You can interact with this model in the Summarize. prototype!

### [How (and when) to enable early stopping for Gensim's Word2Vec](https://blog.fastforwardlabs.com/2021/09/20/how-and-when-to-enable-early-stopping-for-gensims-word2vec.html)

The [Gensim](https://radimrehurek.com/gensim/) library is a staple of the NLP stack and supports what is likely the best-known implementation of Word2Vec. In this post, we cover how to train Word2Vec for non-language use cases (like learning item embeddings) and explain when you should and **_shouldn't_** use early stopping.

---

## Fast Forward Live!

Check out replays of livestreams covering some of our research from this year.

[**Deep Learning for Automatic Offline Signature Verification**](https://youtu.be/7_MlFxyPYSg)

[**Session-based Recommender Systems**](https://www.youtube.com/watch?v=JoRx6udpnbI)

[**Few-Shot Text Classification**](https://youtu.be/oLFqTj5FcEA)

**[Representation Learning for Software Engineers](https://youtu.be/o4gQLVzIm5U)**

---

## Recommended reading

Our research engineers share their favourite reads of the month:

- [Why data scientists shouldn’t need to know Kubernetes](https://huyenchip.com/2021/09/13/data-science-infrastructure.html)

  In this post, Chip Huyen postulates that the ever-increasing job description of the “full stack” data scientist - from data to modeling to infrastructure - results from discordance between development and production environments that practitioners are expected to operate across. In today’s world, production ML requires knowledge of low-level infrastructure which is a very distinct skillset from traditional data science disciplines. Chip argues that rather than adding hats to the data scientists’ already extensive wardrobe, the solution involves proper infrastructure abstraction; a world where data scientists can own the ML lifecycle end-to-end without having to worry about infrastructure complexity. This post does a great job of summarizing the capability and direction of several tools that are helping to solve this issue. - [_Andrew_](https://www.linkedin.com/in/andrew-r-reed/)

- **Active learning for production**

  Since our very first research on [ML with limited labeled data](https://blog.cloudera.com/a-guide-to-learning-with-limited-labeled-data/), active learning (AL) and the like approaches have always been on my radar. After all labeled data is one of the most basic requirements for many ML applications. That said, as widely known many of these approaches, especially the ones based on deep learning are often not scalable for large datasets or are too slow in production environments. At times these could be too slow to train and/or vast amounts of data is needed for them to be performant. A [recent paper](https://arxiv.org/pdf/2006.09916.pdf) discusses a Bayesian approach to AL in a production-like setting. While it builds on the technique based on dropout layers being activated during test time to determine uncertainty (similar to what we had researched and experimented with), the researchers provide a nice comprehensive [library](https://github.com/ElementAI/baal) for practitioners to experiment with and confirm our observations on class imbalance issues. - [_Nisha_](https://twitter.com/NishaMuktewar)

- [Slot Machines: Discovering Winning Combinations of Random Weights in Neural Networks](https://arxiv.org/abs/2101.06475)

  As a former blackjack dealer (it was a _long_ time ago) and avid gambler, the title of this paper caught my eye and the content didn't disappoint! This provocative entry to ICML earlier this year rolls the dice on a new way to train neural networks: don't train them at all! (At least, not in the traditional way.) They present a method wherein the weights of a neural network are randomly instantiated and are **never** updated during training. Instead, each weight is allowed to pick a value from a fixed set of possible values. Each of these options has a score associated with it that corresponds to how likely that value is to contribute to a better neural network overall. Essentially, it is these _scores_ that are trained — not the model _weights_! They perform a slew of experiments comparing their method to randomly instantiated models, traditionally trained models, and more. It makes for both an entertaining and enlightening read. - [_Melanie_](http://www.linkedin.com/in/melanierbeck)

- [ViViT: A Video Vision Transformer](https://arxiv.org/abs/2103.15691)

  My previous work in compute-constrained Computer Vision (CV) restricted me not only to low-energy, low-memory consumption models — but also to convolutions as the main architectural building block. A look at recent literature, however, reveals a trend in CV towards the replacement of convolutions with attention modules. This trend accelerated after the publication, in 2017, of the seminal paper [Attention is All You Need](https://arxiv.org/abs/1706.03762), which introduced a pure-attention model for NLP. But it is only in 2021 that pure-attention models are achieving state-of-the-art in CV. An example of this is [ViViT](https://arxiv.org/abs/2103.15691), a family of pure-transformer based models for video classification. ViViT uses factorization to increase efficiency, and regularization and pre-training to reduce of the need of large training datasets. Execution times are presented for ViViT variants in the paper, but not compared against competing architectures, e.g., those based on convolutions, which would be particularly interesting to see. - [_Daniel_](https://uk.linkedin.com/in/daniel-valdez-balderas-9051323b)

- [Building a data team at a mid-stage startup: a short story](https://erikbern.com/2021/07/07/the-data-team-a-short-story.html)

  This parable of a new head of data transforming a company to be data driven is entirely fictional, but aspects of it will resonate with anyone who has worked in a data role. - _[Chris](https://twitter.com/_cjwallace)_

- Listening: [On the Metal with Jonathan Blow](https://oxide.computer/podcasts/jonathan-blow)

  The On the Metal podcast from Oxide computer company is the best kind of technical podcast: nerds being nerds about nerdy things. This nearly 3-hour episode with indie game developer Jonathan Blow is full of technical insights about game dev and programming languages, and also strong opinions on the state of the computing industry at large. - _[Chris](https://twitter.com/_cjwallace)_
