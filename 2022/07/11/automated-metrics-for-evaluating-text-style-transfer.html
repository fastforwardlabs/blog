<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />
    
<title>Automated Metrics for Evaluating Text Style Transfer</title>
<meta property="og:title" content="Automated Metrics for Evaluating Text Style Transfer">
<meta property="description" content="By Andrew Reed and Melanie Beck
Blog Series This post serves as Part 3 of a four part blog series on the NLP task of Text Style Transfer. In this post, we expand our modeling efforts to a more challenging dataset and propose a set of custom evaluation metrics specific to our task.   Part 1: An Introduction to Text Style Transfer    Part 2: Neutralizing Subjectivity Bias with HuggingFace Transformers    Part 3: Automated Metrics for Evaluating Text Style Transfer    Part 4: Ethical Considerations When Designing an NLG System      In our previous blog post, we took an in-depth look at how to neutralize subjectivity bias in text using HuggingFace transformers.">
<meta property="og:description" content="By Andrew Reed and Melanie Beck
Blog Series This post serves as Part 3 of a four part blog series on the NLP task of Text Style Transfer. In this post, we expand our modeling efforts to a more challenging dataset and propose a set of custom evaluation metrics specific to our task.   Part 1: An Introduction to Text Style Transfer    Part 2: Neutralizing Subjectivity Bias with HuggingFace Transformers    Part 3: Automated Metrics for Evaluating Text Style Transfer    Part 4: Ethical Considerations When Designing an NLG System      In our previous blog post, we took an in-depth look at how to neutralize subjectivity bias in text using HuggingFace transformers.">
<meta property="og:image" content="https://blog.fastforwardlabs.com/images/hugo/image3-tst3.png">
<meta property="og:url" content="https://blog.fastforwardlabs.com/2022/07/11/automated-metrics-for-evaluating-text-style-transfer.html">
<meta property="twitter:card" content="summary_large_image">
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <link rel="stylesheet" type="text/css" href="/style.css" />
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-53030428-5', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

  </head>
  <body>
      <div class="container">
        <div class="spacer"></div>
        <div style="height: 2.5rem; padding-top: 0.35rem;">
          <a target="_blank" href="https://www.cloudera.com/products/fast-forward-labs-research.html">
            <img style="height: 2rem;" src="/images/cloudera-fast-forward-logo.png" />
          </a>
        </div>
        <div class="spacer"></div>
      </div>
      <main id="main">
        
<div class="container">
  <div>
    <h3 class="clear"><a href="/">Blog</a></h3>
  </div>
  <div class="spacer"></div>
  <div class="post">
    <h5 class="clear">
      <span>Jul 11, 2022</span> &middot;
      <span style="text-transform: capitalize;">
        post
      </span>
    </h5>
    <h1>Automated Metrics for Evaluating Text Style Transfer</h1>
    <p>By <em><a href="https://twitter.com/andrewrreed">Andrew Reed</a></em> and <em><a href="https://www.linkedin.com/in/melanierbeck/">Melanie Beck</a></em></p>
<div  class="tldr"> 
  <span class="textbold">Blog Series</span>
  This post serves as Part 3 of a four part blog series on the NLP task of Text Style Transfer. In this post, we expand our modeling efforts to a more challenging dataset and propose a set of custom evaluation metrics specific to our task.
  <div style="margin-top:10px; border-top: 1px dashed grey"> 
    <ul>
        <li> <a href="/2022/03/22/an-introduction-to-text-style-transfer.html" class=""> Part 1: An Introduction to Text Style Transfer </a> 
        </li>
        <li> <a href="/2022/05/05/neutralizing-subjectivity-bias-with-huggingface-transformers.html" class="">  Part 2: Neutralizing Subjectivity Bias with HuggingFace Transformers </a>
        </li>
        <li> <a href="/2022/07/11/automated-metrics-for-evaluating-text-style-transfer.html" class=""> Part 3: Automated Metrics for Evaluating Text Style Transfer </a>
        </li>
        <li> <a href="/2022/07/29/ethical-considerations-when-designing-an-nlg-system.html" class=""> Part 4: Ethical Considerations When Designing an NLG System </a>
        </li>
    </ul>
  </div>
</div>
<p>In <a href="https://blog.fastforwardlabs.com/2022/05/05/neutralizing-subjectivity-bias-with-huggingface-transformers.html">our previous blog post</a>, we took an in-depth look at how to neutralize subjectivity bias in text using HuggingFace transformers. Specifically, we saw how to fine-tune a conditional language model (<a href="https://huggingface.co/facebook/bart-base">BART-base</a>) on the <a href="https://arxiv.org/pdf/1911.09709.pdf">Wiki Neutrality Corpus (WNC)</a> and benchmarked its performance against the WNC paper authors‚Äô modeling approach on a subset of the dataset which only included one-word edits.</p>
<p>In this post, we‚Äôve expanded our modeling efforts to the full dataset consisting of ~180,000 subjective-to-neutral sentence pairs. These pairs include the one-word edits that we used before, as well as all the sentence pairs with more than one-word edits &ndash; a materialy more difficult generative modeling task. We apply the same <a href="https://github.com/fastforwardlabs/text-style-transfer/blob/main/scripts/prepare_data.py">data preprocessing steps</a> and <a href="https://github.com/fastforwardlabs/text-style-transfer/blob/main/scripts/train/seq2seq/train_seq2seq.py">model training configuration</a> as discussed in the previous blog, and also propose a set of custom automated evaluation metrics aimed to better quantify the subtleties of text style transfer than traditional metrics.</p>
<h2 id="challenges-with-evaluating-text-style-transfer">Challenges with Evaluating Text Style Transfer</h2>
<p>Evaluating the quality of machine generated text is hard. Human evaluation is regarded as the best indicator of quality, but unfortunately is expensive, slow, and lacks reproducibility, making it a cumbersome approach for validating model performance. For this reason, NLP practitioners often rely on automated evaluation metrics to serve as a cheap and quick proxy for human judgment. Of course, this compromise comes with tradeoffs.</p>
<p>Traditional automated metrics like the <a href="https://aclanthology.org/P02-1040.pdf">BLEU score</a> &ndash; the most common metric for evaluating neural machine translation (NMT) &ndash; work by counting the lexical n-gram overlap between generated outputs and human-annotated, gold-standard references. As we saw in the previous blog post, BLEU is one of the metrics used by the <a href="https://arxiv.org/pdf/1911.09709.pdf">WNC paper authors</a> to benchmark their model performance against a set of references. Consider the task of comparing the following candidate sentence with the two references while evaluating for semantic equivalence.</p>
<p><strong>Candidate:</strong> He is a great singer.</p>
<p><strong>Reference #1:</strong> He sings really well.</p>
<p><strong>Reference #2:</strong> He is a great writer.</p>
<p>As humans, it&rsquo;s obvious that <em>Reference #1</em> means basically the same thing as the <em>Candidate</em>, while <em>Reference #2</em> changes the entire semantic meaning. However, because BLEU score only measures counts of identical n-grams, <em>Reference #2</em> actually scores higher than <em>Reference #1</em> by this metric. This highlights one of BLEU‚Äôs [many] shortcomings in that it fails to robustly match paraphrases, which leads to performance underestimation as semantically-correct phrases are penalized because they differ in lexical form<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<p>While it&rsquo;s clear that the BLEU metric itself is flawed, the broader ‚Äúcandidate-to-reference‚Äù based NMT evaluation strategy itself also poses issues for evaluating text style transfer. That‚Äôs because style transfer is a one-to-many task, which means that there are several suitable references for any one source sentence<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Therefore, a high-quality style transfer generation may have a low BLEU score towards a reference as we saw in the previous example. Rather than relying on gold references, <em>reference-free</em> evaluation methods have been found <a href="https://arxiv.org/pdf/2202.08479.pdf">to better align with human judgements</a> in the analogous task of paraphrase generation.</p>
<p>In this reference-free paradigm, we ignore ground-truth annotations and compare model output directly with model input. Let‚Äôs now consider the scenario wherein we feed the sentence ‚ÄúHe is a great singer.‚Äù to our text style transfer model to which it produces an output of ‚ÄúHe is a great writer.‚Äù The first thing we notice is the subjectivity in the sentence has not been neutralized (evidenced by the word &ldquo;great&rdquo; in both the input and output sentences) and, worse, the very <em>meaning</em> of the sentence has changed &ndash; singer and writer are not the same thing!</p>
<p>Unfortunately, BLEU was not designed to detect style, and as we already saw, it&rsquo;s not great at assessing semantics either. We&rsquo;d end up with a really high evaluation score for a really bad model! For text style transfer, a ‚Äúone size‚Äù score does <em>not</em> fit all. We need a comprehensive approach to evaluating TST.</p>
<p>As discussed in our <a href="https://blog.fastforwardlabs.com/2022/03/22/an-introduction-to-text-style-transfer.html">Part 1: Introduction to Text Style Transfer</a>, a comprehensive evaluation of quality for text style transfer output should consider three criteria.</p>
<ol>
<li><strong><em>Style strength</em></strong> - To what degree does the generated text achieve the target style?</li>
<li><strong><em>Content preservation</em></strong>- To what degree does the generated text retain the semantic meaning of the source text?</li>
<li><strong><em>Fluency</em></strong>- To what degree does the generated text appear as if it were produced naturally by a human?</li>
</ol>
<p>All three criteria are important in making a determination of quality. If our model transfers text from subjective to neutral tone, but omits or changes an important piece of information (e.g. a proper noun or subject), it fails to preserve the meaning of the original text. On the flip side, if the model reproduces the source text exactly as is, it would have perfect content preservation, but fail completely in style transfer. Finally, the text generation is useless if it contains all the expected tokens, but in an illegible sequence.</p>
<h2 id="automated-evaluation-metrics">Automated Evaluation Metrics</h2>
<p>In the following sections, we‚Äôll discuss reference-free, task-specific metrics aimed at tackling the first two of these criteria while also defining our implementation and design choices.</p>
<h3 id="style-strength">Style Strength</h3>
<p>A common automated method for evaluating transferred style strength involves training a classification model to distinguish between style attributes. At evaluation time, the classifier is used to determine if each style transfer output is in fact classified as the intended target style. Calculating the percentage of total text generations that achieve the target style provides a measure of style transfer strength.</p>
<p>While this approach serves as a strong foundation for assessing style transfer, its binary nature means that a quantifiable score only exists in aggregate. The authors of <a href="https://arxiv.org/pdf/1904.02295.pdf">Evaluating Style Transfer for Text</a> improve upon this idea with the realization that rather than count how many outputs achieve a target style, we can capture more nuanced differences between the style distributions of the input and output text using Earth Mover‚Äôs Distance (EMD)<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. The EMD metric calculates the minimum ‚Äúcost‚Äù to turn one distribution into the other. In this sense, we can interpret EMD between style class distributions (i.e. classifier output) as the intensity (or magnitude) of the style transfer. Ultimately, this metric called <em>Style Transfer Intensity (STI)</em> produces a score that holds meaning on a per-sample, as well as in-aggregate basis.</p>
<h4 id="implementation">Implementation</h4>
<p>Figure 1 below describes the logical workflow used in our implementation of Style Transfer Intensity.</p>
<p><img src="/images/hugo/image3-tst3.png" alt="">
<strong>Figure 1:</strong> Style Transfer Intensity metric using a BERT classification model.</p>
<p>First (1), a fine-tuned text style transfer model (BART) is used to generate neutralized text ( ùëãùëÅ ) from a subjective input ( ùëãùëÜ ). This forms the pair of text that we will be calculating the style transfer intensity between.</p>
<p>Then (2) both texts are passed through a fine-tuned, Transformer-based classification model (BERT) to produce a resulting style distribution for each text ( ùëëùëÜ , ùëëùëÅ ). These style distributions can be visualized at the bottom of Figure 1.</p>
<p>Finally (3), Earth Mover‚Äôs Distance is calculated on the two distributions to produce a resulting STI score. Note that like the original paper author‚Äôs, we penalize STI by negating the EMD score if the output text style distribution moves further away from the target style.</p>
<div  class="tldr"> 
 <span class="textbold">Fine-tuning the BERT Classifier</span>
<p>The BERT model from (2) has been fine-tuned on the same style classification task for which the style transfer model was also trained on. In this case, that means reformatting records in WNC from <em>source_text | target_text</em> pairs into <em>source_text: subjective; target_text: neutral</em> labels. In doing so, we maintain the same data splits (train/test/validation), but double the number of records in each split since each sentence pair record from the style transfer dataset becomes two independent examples in the classification dataset.</p>
<p>For training, we initialize <a href="https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSequenceClassification">HuggingFace‚Äôs AutoModelforSequenceClassification</a> with <a href="https://huggingface.co/bert-base-uncased">bert-base-uncased</a> pre-trained weights and perform a hyperparameter search over: batch size [16, 32], learning rate [3e-05, 3e-06, 3e-07], weight decay [0, 0.01, 0.1] and batch shuffling [True, False] while training for 15 epochs.</p>
</div>
<p>We monitor performance using accuracy as we have a perfectly balanced dataset and assign equal cost to false positives and false negatives. The best performing model produces an overall accuracy of 72.50% and <a href="https://huggingface.co/cffl/bert-base-styleclassification-subjective-neutral">has been published</a> to the HuggingFace model registry for experimental use &ndash; please reference our <a href="https://github.com/fastforwardlabs/text-style-transfer/blob/main/scripts/train/classifier/train_classifier.py">training script</a> and <a href="https://github.com/fastforwardlabs/text-style-transfer/blob/main/notebooks/WNC_full_style_classifier_evaluation.ipynb">classifier evaluation notebook</a> for further details.</p>
<h3 id="content-preservation">Content Preservation</h3>
<p>Measuring content preservation between input and output of a style transfer model is often likened to measuring document similarity. As we‚Äôve mentioned, there are numerous techniques used to quantify similarity between text including traditional lexical-based metrics (e.g. BLEU, METEOR, ROUGE) and newer embedding-based metrics (e.g. WMD, MoverScore, SBERT). However, content preservation in the context of reference-free text style transfer evaluation is uniquely challenging. That‚Äôs because these similarity metrics fail to account for the aim of style transfer modeling, which is to alter style by necessarily changing words. Therefore, intended differences (changes in style) between source and target text are often incorrectly penalized<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>.</p>
<p>To evaluate content preservation more precisely, attempts have been made to first distinguish between semantic and stylistic components of text, and then meaningfully quantify the similarity of just the semantic component alone. While there is open debate about whether it&rsquo;s possible to actually decouple style from content in free text, intuition leads us to believe that our style attribute of ‚Äúsubjectivity‚Äù is expressed, at least in part, through select words. For example, our <a href="https://github.com/fastforwardlabs/text-style-transfer/blob/main/notebooks/WNC_oneword_EDA.ipynb">EDA findings</a> have shown that the presence of certain modifiers (adjectives and adverbs) are strong indicators of subjective content.</p>
<p><a href="https://arxiv.org/pdf/1904.02295.pdf">Previous efforts</a> have approached this style disentanglement process by isolating just the content-related words in each sentence (i.e. masking out any style-related words). They do this by training a style classifier and inspecting the model for its most important features (i.e. words). These strong features form a <em>style lexicon</em>. At evaluation time, any style-related words from the lexicon that exist in the input or output texts are masked out &ndash; thus leaving behind only content-related words. These ‚Äústyle-free‚Äù sentences can then be compared with one of the many similarity measures to produce a content preservation score.</p>
<p>We draw inspiration from the aforementioned tactic of ‚Äústyle masking‚Äù as a means to separate style from content, but implement it in a different manner.</p>
<h4 id="implementation-1">Implementation</h4>
<p>Rather than construct a global style lexicon based on model-level feature importances, we dynamically calculate local, sentence-level feature importances at evaluation time. We prefer this method because the success of the Transformer architecture has shown that contextual language representations are stronger than static ones. This approach allows us to selectively mask style-related tokens depending on their function within a particular sentence (i.e. some words take on different meaning depending on how they are used in context) instead of relying on a contextually-unaware lexicon lookup.</p>
<p>We accomplish this by applying a popular model interpretability technique called <a href="https://arxiv.org/pdf/1703.01365.pdf">Integrated Gradients</a> to our fine-tuned BERT subjectivity classifier which explains a model‚Äôs prediction in terms of its features. This method produces <em>word attributions</em>, which are essentially importance scores for each token in a sentence that indicate how much of the prediction outcome is attributed to that token.</p>
<p><img src="/images/hugo/image4-tst3.png" alt="">
<strong>Figure 2:</strong> Word attributions visualized with <a href="https://github.com/cdpierse/transformers-interpret">Transformers Interpret</a> for two sentences using integrated gradients on the fine-tuned BERT classification model. Positive attribution numbers (green) indicate a token contributes positively towards the predicted class (‚Äúsubjective‚Äù), while negative numbers (red) indicate a word contributes negatively towards the predicted class.</p>
<p>The figure above demonstrates the power of contextual representations. In the first sentence (1), we see that the word ‚Äúpassing‚Äù is strongly attributed to the subjective classification of this sentence. That‚Äôs because the term ‚Äúpassing‚Äù is a euphemism for ‚Äúdeath‚Äù; a common NPOV-related correction in the WNC dataset. However, the term ‚Äúpassing‚Äù also appears in the second sentence (2), but is not attributed to the overall classification. That&rsquo;s because the BERT model recognizes that when used in this context, ‚Äúpassing‚Äù does not suggest death, but rather the act of physical movement, which is neutral in tone. Had we used a global style lexicon to replace subjective words, ‚Äúpassing‚Äù would have been erroneously removed from the second sentence.</p>
<p>Provided these token level attribution scores, we must then select which are considered stylistic elements to be masked out. To do so, we sort tokens in each sentence by the absolute, normalized attribution score and calculate a cumulative sum. This vector allows us to enforce a threshold on how much of the ‚Äútotal style‚Äù should be masked from the sentence without having to specify an explicit number of tokens.</p>
<p><img src="/images/hugo/image6-tst3.png" alt="">
<strong>Figure 3:</strong> Style masking logic determines which tokens are considered style elements and therefore masked out from the sentence prior to calculating similarity measure.</p>
<p>In Figure 3 above, we see how cumulative attribution scores form the basis of style token selection. In this example, the terms ‚Äúelegant‚Äù and ‚Äústriking‚Äù combined account for ~35% of the style classification importance. This methodology allows us to set a tunable threshold whereby we mask out all tokens that contribute to the top X% of classification importance. To ‚Äúmask out‚Äù style tokens, we simply replace them with either the informationless ‚Äú[PAD]‚Äù token or remove them completely by deleting in-place.</p>
<p>The goal of this masking activity is to create ‚Äústyle-independent‚Äù versions of the original input and output sentences. These style-independent texts are then encoded using a generic, pre-trained SentenceBERT model to produce sentence level embeddings. SentenceBERT is a modified version of BERT that uses siamese and triplet network structures to derive semantically meaningful sentence representations that can be compared easily using cosine similarity (see the section <em>‚ÄúTo BERT or not to BERT‚Äù</em> from our report on <a href="https://few-shot-text-classification.fastforwardlabs.com/">Few-Shot Text Classification</a> for more on SentenceBERT). We chose this embedding-based similarity method because it overcomes the limitations of strict string matching methods (like BLEU) by comparing continuous representations rather than lexical tokens.</p>
<p>Figure 4 below summarizes the logical workflow used in our implementation of Content Preservation Score.</p>
<p><img src="/images/hugo/image5-tst3.png" alt="">
<strong>Figure 4:</strong> Content Preservation Score metric using BERT-based word attributions for style masking and SentenceBERT embeddings for similarity.</p>
<p>To begin (1), a fine-tuned text style transfer model (BART) is used to generate neutralized text <em>(X<sub>N</sub>)</em> from a subjective input <em>(X<sub>S</sub>)</em> .</p>
<p>Style tokens are then masked from both texts (2) using the methodology described in Figure 3 above to produce versions that contain only content-related tokens.</p>
<p>Next (3), the content-only texts are passed through a generic, pre-trained SentenceBERT model to produce a sentence embedding for each text (<em>e<sub>S</sub></em> , <em>e<sub>N</sub></em>). Finally (4), we calculate cosine similarity between the embedding representations.</p>
<h3 id="considerations">Considerations</h3>
<p>While the high-level reasoning behind our implementations of STI and CPS make logical sense, there are nuances to the implementation that create room for error and jeopardize their effectiveness in measuring text style transfer. This is true of all automated metrics, and so we discuss these considerations below and recognize their importance as focus areas for future research.</p>
<h4 id="experimentally-determining-cps-parameters">Experimentally determining CPS parameters</h4>
<p>To determine a default attribution threshold and masking strategy for our CPS metric, we experimentally searched over threshold values of 10% - 50% by 10% increments and masking strategies of ‚Äú[PAD]‚Äù vs. removal while monitoring content preservation score on the held out test split. We also compare these parameter combinations to a case where no style masking is performed at all.</p>
<p><img src="/images/hugo/image2-tst3.png" alt="">
<strong>Figure 5:</strong> Content preservation score distributions across various experimental settings for <em>style threshold</em> and <em>masking strategy</em>.</p>
<p>We found that for each incremental threshold value, token removal produces a slightly higher average CPS score than replacement with ‚Äú[PAD]‚Äù token. We also see that regardless of the parameter combination, all cases result in a lower median similarity score than had no style-masking been applied at all (see far right column in Figure 5). This makes sense because the more tokens we mask, the more opportunity there is to erroneously remove a piece of content instead of a stylistic element.</p>
<p>The only true way to determine the ‚Äúbest‚Äù parameter combination is to look at how CPS correlates with human evaluated scores. However, since we don‚Äôt have access to manual evaluation scores, we select the combination that produces the highest outright CPS, which happens to be the case with no-style masking. For this reason, our CPS metric logic boils down to simply comparing SentenceBERT embeddings with cosine similarity, a similar landing place that others<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup><sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup><sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> have also arrived at.</p>
<p>Manual error analysis has revealed that our classifier-based attribution scores and style-token selection logic isn‚Äôt consistent, nor precise enough at isolating only stylistic elements. As a result, meaningful content tokens are mistakenly removed which hinders content preservation measurement more than just leaving all tokens in. We discuss these challenges further in the following sections.</p>
<h4 id="dependence-on-style-classifier">Dependence on style classifier</h4>
<p>Both of the metrics we‚Äôve implemented depend on the availability of a well-fit style classification model. This requirement translates to the need for labeled data. And while this isn‚Äôt an issue for parallel TST tasks, it becomes a non-starter for the vast majority of style attributes where parallel data isn‚Äôt available.</p>
<p>Even when parallel data is available, it&rsquo;s imperative that the trained model is performant. As we‚Äôll see in a later section, data quality issues can lead to a classifier that learns patterns that are unrepresentative of the true target attribute and result in an error-prone model. Since the STI metric is built directly on classifier distribution outputs, it is apparent that errors with the model will surface as errors in the style metric.</p>
<p>Similarly, the CPS metric uses the word attributions that are derived from the classifier as the basis for style token masking. If the model learns incorrect relationships between features and style classes, errors in word attribution scores can cause the wrong tokens to be masked. Unfortunately, there is minimal tolerance for error in style masking because incorrectly masking a content-related token (e.g. proper noun) can completely alter the semantics, producing a very low similarity score.</p>
<h4 id="style-token-selection">Style token selection</h4>
<p>While our method for isolating style tokens in a sentence is driven by robust, contextual feature importances, the actual token selection methodology has room for improvement.</p>
<p>To begin, feature importances are attributed per token. Because BERT uses a word-piece tokenizer, we can see fragments of the same word with drastically different attribution values. For example, in Figure 3 above, we find that the term ‚Äústrikingly‚Äù is tokenized in to the word pieces ‚Äústriking‚Äù and ‚Äú##ly‚Äù, with the former attributed to ~15% importance and the latter just ~1%. Our current implementation considers these independently, and therefore applies a mask to just the root word alone. A considerable improvement would be to introduce logic that looks at combined scores for word pieces.</p>
<p>In addition, our method applies a ‚Äúglobal‚Äù threshold to determine the amount of style (and therefore corresponding tokens) that are masked out. At a minimum, one token is always masked. This logic could be improved as there is likely a relationship between length of sentence and maximum token feature importance. There are also cases where a sentence doesn‚Äôt contain any style-related terms, and therefore masking one token incorrectly removes content by default.</p>
<h4 id="decoupling-style-from-content">Decoupling style from content</h4>
<p>Our content preservation metric naively assumes that style can in fact be disentangled from content.</p>
<p>This remains an open question in the NLP research community<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>, but from our experience and research, it seems there is growing consensus that style is woven into the content of written language, and the degree to which it can be separated is highly attribute-dependent. For the attribute of subjectivity, we believe that style is (at least partially) separable from content. For example, removing subjective modifiers (e.g. adjectives and adverbs) can change the style of a sentence without unwanted impact to semantics.</p>
<p>However, the challenge arises when theory meets practice. As we‚Äôve found, automated methods for disentangling stylistic elements are consistently fraught with error, especially when operating in the lexical space (i.e. masking tokens). Newer approaches to text style transfer propose the separation of style from semantics in latent space representations with both supervised and unsupervised methods. We are encouraged by these efforts and look forward to continued research on this topic.</p>
<h4 id="are-these-metrics-better-than-bleu">Are these metrics better than BLEU?</h4>
<p>While we believe the STI and CPS metrics enable a more nuanced evaluation of text style transfer output than a singular BLEU score, we cannot say if these metrics are ‚Äúbetter‚Äù without some human evaluated baseline to compare against. A ‚Äúbetter‚Äù evaluation metric is just one that correlates stronger with human judgment, as this is the ultimate goal of automated evaluation.</p>
<p>Unfortunately, conducting human evaluation is outside the scope of our current research endeavor, but we do propose this as future work to build upon. In particular, we suggest conducting human evaluation in accord with <a href="https://arxiv.org/pdf/1904.02295.pdf">this paper</a> as a means to produce reliable evaluation benchmarks.</p>
<p>It&rsquo;s important to note that while human evaluation is the ‚Äúbest‚Äù means for evaluating generated text, it still isn‚Äôt without issue. That‚Äôs because determining if something is subjective vs. neutral is itself subjective. Subjective evaluation tasks likely lead to a higher degree of variability even among human reviewers.</p>
<h2 id="evaluating-bart-with-sti--cps">Evaluating BART with STI &amp; CPS</h2>
<p>With our custom metrics defined, we utilize them to evaluate our <a href="https://huggingface.co/cffl/bart-base-styletransfer-subjective-to-neutral">fine-tuned BART model‚Äôs</a> ability to neutralize subjective language on the held out test set. We calculate STI and CPS scores between both the source and generated text, as well as the source and ground truth target annotation. Comparing metrics across these pairs helps build intuition for their overall usefulness and enables us to isolate edge cases of unexpected performance.</p>
<p><img src="/images/hugo/image8-tst3.png" alt="">
<strong>Figure 6:</strong> Distribution of STI and CPS scores on the held out test set. ‚ÄúPred‚Äù corresponds to scores between source and generated text, while ‚Äútarget‚Äù corresponds to scores between source and ground truth annotation.</p>
<h3 id="content-preservation-score-cps">Content Preservation Score (CPS)</h3>
<p>CPS score is built on cosine similarity and so it naturally ranges from 0-1. Figure 6 highlights the strong, left-skewed distribution of both the source-to-target and source-to-prediction examples. This result makes sense because we expect input and output pairs to be largely similar in semantics as that is the essence of this task and dataset &ndash; to slightly modify style while retaining meaning.</p>
<p>We see very similar distributions between target and predicted samples, with the source-to-predicted pairs having slightly higher median CPS scores and a smaller standard deviation. As we‚Äôll see, this finding hints at the conservative nature of our model (i.e. modest edits compared to human-made edits), as well as the perceptible data quality issues present in the full WNC corpus.</p>
<p>We analyze edge cases of mismatched performance between the model outputs and ground truths in Figure 7 below to better understand the strengths and weaknesses of our metrics.</p>
<p><img src="/images/hugo/image7-tst3.png" alt=""></p>
<p><strong>Figure 7:</strong> Sample WNC pairs that demonstrate common themes around the CPS metric. Specifically, cases where target_cps &raquo; predicted_cps (1-3) and target_cps &laquo; predicted_cps (4).</p>
<p>From Figure 7, we see that examples 1-3 highlight the scenario where the ground truth annotation preserves content much better (as defined by CPS) than the model‚Äôs output, and the opposite for example 4. These examples demonstrate common themes (numerically matched below) that we‚Äôve found through our error analysis.</p>
<ol>
<li><strong><em>The BART model tends toward brevity -</em></strong> The trained seq2seq model has learned that omission of content is generally a good tactic for reducing subjectivity. This is seen in the example above where the model selects an abbreviated version of the input. Because the model omits part of the content (i.e. ‚Äúbeing the most successful club‚Äù), our CPS metric punishes the score relative to the ground truth.</li>
<li><strong><em>SentenceBERT penalizes missing content -</em></strong> As expected, SentenceBERT embedding similarity captures the omission of important words. In this example, the prediction is penalized for dropping the important subject ‚ÄúDenmark‚Äù.</li>
<li><strong><em>CPS slips when style tokens are the difference -</em></strong> In contrast with example #2, our CPS metric struggles when the omitted words (‚Äúmost serious‚Äù) are actually style related. In this example, CPS produces a disagreeably low score for the prediction as compared to the ground truth despite it largely retaining the semantic meaning. This demonstrates the imperative of isolating style elements from content. We tested removing these style-related terms (‚Äúmost serious‚Äù) which resulted in a CPS score more representative of the semantic alignment.</li>
<li><strong><em>Factual edits are out-of-scope -</em></strong> In this example, our model generated text produces a much higher CPS than the ground truth. This is due to the annotator&rsquo;s introduction of new facts, or out-of-context information, that the model should not be expected to produce. We consider edits of this type to be outside the scope of our intended modeling task.</li>
</ol>
<p>Overall, we see that our CPS metric has its strengths and weaknesses. We believe this metric is useful for providing a general indication of content preservation because low scores truly mark dissimilar content. However, this metric lacks marginal specificity and struggles to quantify small differences in content with accuracy.</p>
<h3 id="style-transfer-intensity-sti">Style Transfer Intensity (STI)</h3>
<p>Unlike CPS, style transfer intensity ranges from -1 to 1 because movements away from the target style are penalized. We see from Figure 6 (above) and Figure 8 (below) that source-to-target and source-to-prediction STI distributions are very similar, which suggests the style transfer model is generally doing a good job of neutralizing text to resemble that of the ground truth.</p>
<p><img src="/images/hugo/image9-tst3.png" alt="">
<strong>Figure 8:</strong> Histogram of STI scores on held out test set. ‚ÄúPred‚Äù corresponds to scores between source and generated text, while ‚Äútarget‚Äù corresponds to scores between source and ground truth annotation.</p>
<p>However, there is a clear discrepancy between the distributions at STI value of 0. Here we see a significant number of generations that result in no change in style &ndash; these are cases where we found the model simply repeats the input as output. This implies that model is conservative in nature (i.e. refrains from making unsure edits) and explains the lower median STI score for the source-to-target population (0.19 vs. 0.24)</p>
<p><img src="/images/hugo/image1-tst3.png" alt="">
<strong>Figure 9:</strong> Sample WNC pairs that demonstrate common themes around the STI metric. Specifcally, cases where target_sti &lt; 0 (1), target_sti &raquo; pred_sti (2-3), and target_sti &laquo; pred_sti (4).</p>
<p>Like in the CPS analysis, we can look at edge cases shown in Figure 9 to highlight themes about model and metric quality.</p>
<ol>
<li><strong><em>Incorrect target annotations -</em></strong> Figure 8 reveals that there are examples where the ground truth STI score is negative &ndash; implying that the ground truth annotation is more subjective than the source, which we can verify by looking at this first example. We see that the target text introduces the subjective modifier ‚Äúflawed‚Äù, which is clearly a labeling error. There are quite a few of these data quality issues that should be investigated and corrected in the dataset for future work.</li>
<li><strong><em>BART can be partially correct -</em></strong> As shown here, there are many instances where the style transfer model correctly edits one instance of subjectivity in a sentence (e.g. removes ‚Äúprestigious‚Äù), but misses additional occurrences (e.g. ‚Äúmoving‚Äù).</li>
<li><strong><em>Classifier error surfaces in STI metric -</em></strong> As discussed previously, STI depends on the quality of the style classification model. This example shows where the classifier incorrectly associates ‚Äúgrammy nominated‚Äù as a subjective modifier, when in fact the modifier phrase consists of neutral content.</li>
<li><strong><em>BART sometimes does better than ground truth -</em></strong> By inspecting cases where target_sti &lt;&lt; pred_sti, we find examples where the fine-tuned style transfer model legitimately outperforms the ground truth &ndash; a hopeful insight into the potential usefulness of the model.</li>
</ol>
<h4 id="interpreting-the-sti-metric">Interpreting the STI metric</h4>
<p>Style transfer intensity, as defined above, produces a directional magnitude indicating the distributional shift between style classifications from an input and output text. While this is a useful metric, it is difficult to compare across examples because the value is not normalized. For example, an STI score of 0.1 appears to be a weak indication of style transfer. But if that score corresponds to a distribution shift from [0.1, 0.9] to [0.0, 1.0], it actually represents the maximum possible shift in style because the distribution only had little room for improvement. Therefore, what appears to be a low STI score actually captured 100% of the possible target style gap. It would make little sense to put this example on the same footing as a distribution shift from [0.9, 0.1] to [0.8, 0.2].</p>
<p>This highlights the fact that STI should be measured relative to the total <em>potential</em> for style transfer. For this reason, we recommend representing STI as a percentage of the total possible, directionally corrected STI gain. If the output text distribution moves closer towards the target style class, the metric represents the percentage of the possible <em>target</em> style distribution that was captured. If output text distribution moves further from the target style class, the metric represents the percentage of the possible <em>source</em> style distribution that was captured.</p>
<h2 id="wrapping-up">Wrapping up</h2>
<p>In this post, we took a close look at what it means to comprehensively evaluate text style transfer and the associated challenges in doing so. We proposed a set of metrics that aim to quantify transferred style strength and content preservation. Through our metric development process, we got a first hand look at the tradeoffs that exist when substituting manual, human evaluation for an automated proxy.</p>
<p>Evaluating our BART model with the STI and CPS metrics revealed strengths and weaknesses of the metrics themselves, the dataset, and the model. We encourage further benchmarking of these metrics against human evaluated scores and recognize the need for cleaning up data quality concerns with the full WNC corpus (e.g. grammar, punctuation, &amp; factual based-edits, labeling errors, incomplete ground truth annotations).</p>
<p>We‚Äôve openly released our models (<a href="https://huggingface.co/cffl/bart-base-styletransfer-subjective-to-neutral">seq2seq</a>, <a href="https://huggingface.co/cffl/bert-base-styleclassification-subjective-neutral">classification</a>), datasets, and <a href="https://github.com/fastforwardlabs/text-style-transfer">code</a> to enable further experimentation with this text style transfer use case, and look forward to the continued progress on this exciting NLP task!</p>
<h2 id="references">References</h2>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p><a href="https://arxiv.org/pdf/1904.09675.pdf">BERTSCORE: Evaluating Text Generation with BERT</a> <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p><a href="https://arxiv.org/pdf/2202.08479.pdf">Revisiting the Evaluation Metrics of Paraphrase Generation</a> <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p><a href="https://arxiv.org/pdf/1904.02295.pdf">Evaluating Style Transfer for Text</a> <a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p><a href="https://arxiv.org/pdf/2004.05001.pdf">Style-transfer and Paraphrase: Looking for a Sensible Semantic Similarity Metric</a> <a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p><a href="https://arxiv.org/pdf/2011.00416.pdf">Deep Learning for Text Style Transfer: A Survey</a> <a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6" role="doc-endnote">
<p><a href="https://arxiv.org/pdf/1808.04365.pdf">What is wrong with style transfer for texts?</a> <a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

    <div class="spacer"></div>
    <div>
      <div style="width: 100%; height: 2px; background: #ccc; margin-top: -1px; margin-bottom: -1px;"></div>
    </div>
    <div class="spacer"></div>
  </div>
</div>


<div class="container">
  <div class="spacer"></div>
  <h2 class="clear">Read more</h2>
  <div class="spacer"></div>
  <div style="display: grid; grid-template-columns: 1fr 1fr; grid-column-gap: 2ch;">
    <div>
      
      <div class="small">Newer</div>
      <div>
  <h5 style="margin-bottom: 0px;">
    <span>Jul 29, 2022</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2022/07/29/ethical-considerations-when-designing-an-nlg-system.html"><strong>Ethical Considerations When Designing an NLG System</strong></a></div>
  
  <div class="spacer"></div>
</div>


      
    </div>
    <div>
      
      <div class="small">Older</div>
      <div>
  <h5 style="margin-bottom: 0px;">
    <span>May 5, 2022</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2022/05/05/neutralizing-subjectivity-bias-with-huggingface-transformers.html"><strong>Neutralizing Subjectivity Bias with HuggingFace Transformers</strong></a></div>
  
  <div class="spacer"></div>
</div>


      
    </div>
  </div>
</div>

<div class="container">
<div class="spacer"></div>
<div>
  <div style="width: 100%; height: 2px; background: #ccc; margin-top: -1px; margin-bottom: -1px;"></div>
</div>
<div class="spacer"></div>
<div class="spacer"></div>
</div>

<div class="container">
  

<h2 class="clear">Latest posts</h2>
<div class="spacer"></div>

<div id="posts-holder"> 
  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Jul 29, 2022</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
  <a href="/2022/07/29/ethical-considerations-when-designing-an-nlg-system.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/image1-tst4.png" />
  </a>
  
  <div>
    
    <a href="/2022/07/29/ethical-considerations-when-designing-an-nlg-system.html"
       ><h2 style="margin-bottom: 4px;">Ethical Considerations When Designing an NLG System</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="https://twitter.com/andrewrreed">Andrew Reed</a>
        &middot; </span
      >
    </span>
    Blog Series This post serves as Part 4 of a four part blog series on the NLP task of Text Style Transfer. In this post, we expand our modeling efforts to a more challenging dataset and propose a set of custom evaluation metrics specific to our task.   Part 1: An Introduction to Text Style Transfer    Part 2: Neutralizing Subjectivity Bias with HuggingFace Transformers    Part 3: Automated Metrics for Evaluating Text Style Transfer    Part 4: Ethical Considerations When Designing an NLG System      At last, we‚Äôve made it to the final chapter of this blog series.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/2022/07/29/ethical-considerations-when-designing-an-nlg-system.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Jul 11, 2022</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
  <a href="/2022/07/11/automated-metrics-for-evaluating-text-style-transfer.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/image3-tst3.png" />
  </a>
  
  <div>
    
    <a href="/2022/07/11/automated-metrics-for-evaluating-text-style-transfer.html"
       ><h2 style="margin-bottom: 4px;">Automated Metrics for Evaluating Text Style Transfer</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by  Andrew &amp; Melanie &middot; 
    </span>
    By Andrew Reed and Melanie Beck
Blog Series This post serves as Part 3 of a four part blog series on the NLP task of Text Style Transfer. In this post, we expand our modeling efforts to a more challenging dataset and propose a set of custom evaluation metrics specific to our task.   Part 1: An Introduction to Text Style Transfer    Part 2: Neutralizing Subjectivity Bias with HuggingFace Transformers    Part 3: Automated Metrics for Evaluating Text Style Transfer    Part 4: Ethical Considerations When Designing an NLG System      In our previous blog post, we took an in-depth look at how to neutralize subjectivity bias in text using HuggingFace transformers.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/2022/07/11/automated-metrics-for-evaluating-text-style-transfer.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>May 5, 2022</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
  <a href="/2022/05/05/neutralizing-subjectivity-bias-with-huggingface-transformers.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/fig7-tst2.png" />
  </a>
  
  <div>
    
    <a href="/2022/05/05/neutralizing-subjectivity-bias-with-huggingface-transformers.html"
       ><h2 style="margin-bottom: 4px;">Neutralizing Subjectivity Bias with HuggingFace Transformers</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="https://twitter.com/andrewrreed">Andrew Reed</a>
        &middot; </span
      >
    </span>
    Blog Series This post serves as Part 2 of a four part blog series on the NLP task of Text Style Transfer. In this post, we expand our modeling efforts to a more challenging dataset and propose a set of custom evaluation metrics specific to our task.   Part 1: An Introduction to Text Style Transfer    Part 2: Neutralizing Subjectivity Bias with HuggingFace Transformers    Part 3: Automated Metrics for Evaluating Text Style Transfer    Part 4: Ethical Considerations When Designing an NLG System      Subjective language is all around us &ndash; product advertisements, social marketing campaigns, personal opinion blogs, political propaganda, and news media, just to name a few examples.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/2022/05/05/neutralizing-subjectivity-bias-with-huggingface-transformers.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Mar 22, 2022</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
  <a href="/2022/03/22/an-introduction-to-text-style-transfer.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/parallel_nonparallel-1647959058.png" />
  </a>
  
  <div>
    
    <a href="/2022/03/22/an-introduction-to-text-style-transfer.html"
       ><h2 style="margin-bottom: 4px;">An Introduction to Text Style Transfer</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="https://twitter.com/andrewrreed/">Andrew Reed</a>
        &middot; </span
      >
    </span>
    Blog Series This post serves as Part 1 of a four part blog series on the NLP task of Text Style Transfer. In this post, we expand our modeling efforts to a more challenging dataset and propose a set of custom evaluation metrics specific to our task.   Part 1: An Introduction to Text Style Transfer    Part 2: Neutralizing Subjectivity Bias with HuggingFace Transformers    Part 3: Automated Metrics for Evaluating Text Style Transfer    Part 4: Ethical Considerations When Designing an NLG System      Today‚Äôs world of natural language processing (NLP) is driven by powerful transformer-based models that can automatically caption images, answer open-ended questions, engage in free dialog, and summarize long-form bodies of text &ndash; of course, with varying degrees of success.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/2022/03/22/an-introduction-to-text-style-transfer.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Jan 31, 2022</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
  <a href="/2022/01/31/why-and-how-convolutions-work-for-video-classification.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/Fig_04_3D_conv_gray_video_kernel_2-1643658549.png" />
  </a>
  
  <div>
    
    <a href="/2022/01/31/why-and-how-convolutions-work-for-video-classification.html"
       ><h2 style="margin-bottom: 4px;">Why and How Convolutions Work for Video Classification</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="https://uk.linkedin.com/in/daniel-valdez-balderas-9051323b">Daniel Valdez-Balderas</a>
        &middot; </span
      >
    </span>
    Video classification is perhaps the simplest and most fundamental of the tasks in the field of video understanding. In this blog post, we‚Äôll take a deep dive into why and how convolutions work for video classification. Our goal is to help the reader develop an intuition about the relationship between space (the image part of video) and time (the sequence part of video), and pave the way to a deep understanding of video classification algorithms.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/2022/01/31/why-and-how-convolutions-work-for-video-classification.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Dec 14, 2021</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
  <a href="/2021/12/14/an-introduction-to-video-understanding-capabilities-and-applications.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/video_classification-1639064585.png" />
  </a>
  
  <div>
    
    <a href="/2021/12/14/an-introduction-to-video-understanding-capabilities-and-applications.html"
       ><h2 style="margin-bottom: 4px;">An Introduction to Video Understanding: Capabilities and Applications</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="https://uk.linkedin.com/in/daniel-valdez-balderas-9051323b">Daniel Valdez Balderas</a>
        &middot; </span
      >
    </span>
    Video footage constitutes a significant portion of all data in the world. The 30 thousand hours of video uploaded to Youtube every hour is a part of that data; another portion is produced by 770 million surveillance cameras globally.¬†In addition to being plentiful, video data has tremendous capacity to store useful information. Its vastness, richness, and applicability make the understanding of video a key activity within the field of computer vision.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/2021/12/14/an-introduction-to-video-understanding-capabilities-and-applications.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
</div>

<div>
  <button id="load_more" style="width: 100%;">load more</button>
</div>
<div class="spacer"></div>
<div class="spacer"></div>

<script>
  window.addEventListener('load', () => {
    let $posts_holder = document.getElementById('posts-holder')
    let $load_more = document.getElementById('load_more')
    let next_page = 2
    $load_more.addEventListener('click', () => {
      fetch(`/posts/page/${next_page}.html`).then(r =>r.text()).then(r => {
        let el = document.createElement('html')
        el.innerHTML = r
        next_page += 1
        let $posts = el.querySelector('#posts-holder').children
        for (let i=0; i< $posts.length; i++) {
          let $post = $posts[i].cloneNode(true)
          $posts_holder.appendChild($post)
        }
      })
    })
  })
</script>


  <h3 class="clear">Popular posts</h3>
<div class="spacer"></div>
<div>
  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Oct 30, 2019</span> &middot;
    <span style="text-transform: capitalize;">
      newsletter
    </span>
  </h5>
  
    <div><a href="/2019/10/30/exciting-applications-of-graph-neural-networks.html"><strong>Exciting Applications of Graph Neural Networks</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Nov 14, 2018</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2018/11/14/federated-learning-distributed-machine-learning-with-data-locality-and-privacy.html"><strong>Federated learning: distributed machine learning with data locality and privacy</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Apr 10, 2018</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2018/04/10/pytorch-for-recommenders-101.html"><strong>PyTorch for Recommenders 101</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Oct 4, 2017</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2017/10/04/first-look-using-three.js-for-2d-data-visualization.html"><strong>First Look: Using Three.js for 2D Data Visualization</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Aug 22, 2016</span> &middot;
    <span style="text-transform: capitalize;">
      whitepaper
    </span>
  </h5>
  
    <div><a href="/2016/08/22/under-the-hood-of-the-variational-autoencoder-in-prose-and-code.html"><strong>Under the Hood of the Variational Autoencoder (in Prose and Code)</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Feb 24, 2016</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2016/02/24/hello-world-in-keras-or-scikit-learn-versus-keras.html"><strong>&#34;Hello world&#34; in Keras (or, Scikit-learn versus Keras)</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
</div>

</div>

<div class="spacer"></div>
<div style="background: #efefef;">
  <div class="spacer"></div>
  <div class="spacer"></div>
  <div class="container">
  <h1 class="clear">Reports</h1>
  <div style="color: #444;">In-depth guides to specific machine learning capabilities</div>
</div>
<div class="spacer"></div>
<div style="max-width: 96ch; margin: 0 auto; padding-left: 1ch; padding-right: 1ch;">
  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF22</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://concept-drift.fastforwardlabs.com/" target="_blank">Inferring Concept Drift Without Labeled Data</a></h2>
  <a class="report-image" href="https://concept-drift.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff22-combo.png" />
  </a>
  <div class="small">Concept drift occurs when the statistical properties of a target domain change overtime causing model performance to degrade. Drift detection is generally achieved by monitoring a performance metric of interest and triggering a retraining pipeline when that metric falls below some designated threshold. However, this approach assumes ample labeled data is available at prediction time - an unrealistic constraint for many production systems. In this report, we explore various approaches for dealing with concept drift when labeled data is not readily accessible.</div>
  <div><a href="https://concept-drift.fastforwardlabs.com/" target="_blank">Read the report&nbsp; ‚Üí</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF19</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://session-based-recommenders.fastforwardlabs.com/" target="_blank">Session-based Recommender Systems</a></h2>
  <a class="report-image" href="https://session-based-recommenders.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff19-combo.png" />
  </a>
  <div class="small">Being able to recommend an item of interest to a user (based on their past preferences) is a highly relevant problem in practice. A key trend over the past few years has been session-based recommendation algorithms that provide recommendations solely based on a user‚Äôs interactions in an ongoing session, and which do not require the existence of user profiles or their entire historical preferences. This report explores a simple, yet powerful, NLP-based approach (word2vec) to recommend a next item to a user. While NLP-based approaches are generally employed for linguistic tasks, here we exploit them to learn the structure induced by a user‚Äôs behavior or an item‚Äôs nature.</div>
  <div><a href="https://session-based-recommenders.fastforwardlabs.com/" target="_blank">Read the report&nbsp; ‚Üí</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF18</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://few-shot-text-classification.fastforwardlabs.com/" target="_blank">Few-Shot Text Classification</a></h2>
  <a class="report-image" href="https://few-shot-text-classification.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff18-combo.png" />
  </a>
  <div class="small">Text classification can be used for sentiment analysis, topic assignment, document identification, article recommendation, and more. While dozens of techniques now exist for this fundamental task, many of them require massive amounts of labeled data in order to be useful. Collecting annotations for your use case is typically one of the most costly parts of any machine learning application. In this report, we explore how latent text embeddings can be used with few (or even zero) training examples and provide insights into best practices for implementing this method.</div>
  <div><a href="https://few-shot-text-classification.fastforwardlabs.com/" target="_blank">Read the report&nbsp; ‚Üí</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF16</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://structural-time-series.fastforwardlabs.com" target="_blank">Structural Time Series</a></h2>
  <a class="report-image" href="https://structural-time-series.fastforwardlabs.com" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff16-combo.png" />
  </a>
  <div class="small">Time series data is ubiquitous. This report examines generalized additive models, which give us a simple, flexible, and interpretable means for modeling time series by decomposing them into structural components. We look at the benefits and trade-offs of taking a curve-fitting approach to time series, and demonstrate its use via Facebook‚Äôs Prophet library on a demand forecasting problem.</div>
  <div><a href="https://structural-time-series.fastforwardlabs.com" target="_blank">Read the report&nbsp; ‚Üí</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF15</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://meta-learning.fastforwardlabs.com" target="_blank">Meta-Learning</a></h2>
  <a class="report-image" href="https://meta-learning.fastforwardlabs.com" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff15-combo.png" />
  </a>
  <div class="small">In contrast to how humans learn, deep learning algorithms need vast amounts of data and compute and may yet struggle to generalize. Humans are successful in adapting quickly because they leverage their knowledge acquired from prior experience when faced with new problems. In this report, we explain how meta-learning can leverage previous knowledge acquired from data to solve novel tasks quickly and more efficiently during test time</div>
  <div><a href="https://meta-learning.fastforwardlabs.com" target="_blank">Read the report&nbsp; ‚Üí</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF14</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://qa.fastforwardlabs.com" target="_blank">Automated Question Answering</a></h2>
  <a class="report-image" href="https://qa.fastforwardlabs.com" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff14-combo.png" />
  </a>
  <div class="small">Automated question answering is a user-friendly way to extract information from data using natural language. Thanks to recent advances in natural language processing, question answering capabilities from unstructured text data have grown rapidly. This blog series offers a walk-through detailing the technical and practical aspects of building an end-to-end question answering system.</div>
  <div><a href="https://qa.fastforwardlabs.com" target="_blank">Read the report&nbsp; ‚Üí</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF13</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://ff13.fastforwardlabs.com" target="_blank">Causality for Machine Learning</a></h2>
  <a class="report-image" href="https://ff13.fastforwardlabs.com" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff13-combo.png" />
  </a>
  <div class="small">The intersection of causal inference and machine learning is a rapidly expanding area of research that&#39;s already yielding capabilities to enable building more robust, reliable, and fair machine learning systems. This report offers an introduction to causal reasoning including causal graphs and invariant prediction and how to apply causal inference tools together with classic machine learning techniques in multiple use-cases.</div>
  <div><a href="https://ff13.fastforwardlabs.com" target="_blank">Read the report&nbsp; ‚Üí</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF06-2020</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://ff06-2020.fastforwardlabs.com" target="_blank">Interpretability</a></h2>
  <a class="report-image" href="https://ff06-2020.fastforwardlabs.com" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff06-2020-combo.png" />
  </a>
  <div class="small">Interpretability, or the ability to explain why and how a system makes a decision, can help us improve models, satisfy regulations, and build better products. Black-box techniques like deep learning have delivered breakthrough capabilities at the cost of interpretability. In this report, recently updated to include techniques like SHAP, we show how to make models interpretable without sacrificing their capabilities or accuracy.</div>
  <div><a href="https://ff06-2020.fastforwardlabs.com" target="_blank">Read the report&nbsp; ‚Üí</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF12</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://ff12.fastforwardlabs.com" target="_blank">Deep Learning for Anomaly Detection</a></h2>
  <a class="report-image" href="https://ff12.fastforwardlabs.com" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff12-combo.png" />
  </a>
  <div class="small">From fraud detection to flagging abnormalities in imaging data, there are countless applications for automatic identification of abnormal data. This process can be challenging, especially when working with large, complex data. This report explores deep learning approaches (sequence models, VAEs, GANs) for anomaly detection, when to use them, performance benchmarks, and product possibilities.</div>
  <div><a href="https://ff12.fastforwardlabs.com" target="_blank">Read the report&nbsp; ‚Üí</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF11</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://transfer-learning.fastforwardlabs.com/" target="_blank">Transfer Learning for Natural Language Processing</a></h2>
  <a class="report-image" href="https://transfer-learning.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff11-combo.png" />
  </a>
  <div class="small">Natural language processing (NLP) technologies using deep learning can translate language, answer questions, and generate human-like text But these deep learning techniques require large, costly labeled datasets, expensive infrastructure, and scarce expertise. Transfer learning lifts these constraints by reusing and adapting a model‚Äôs understanding of language. Transfer learning is a good fit for any NLP application. In this report, we show how to use transfer learning to build high-performance NLP systems with minimal resources.</div>
  <div><a href="https://transfer-learning.fastforwardlabs.com/" target="_blank">Read the report&nbsp; ‚Üí</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF10</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://lwlld.fastforwardlabs.com/" target="_blank">Learning with Limited Labeled Data</a></h2>
  <a class="report-image" href="https://lwlld.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff10-combo.png" />
  </a>
  <div class="small">Being able to learn with limited labeled data relaxes the stringent labeled data requirement for supervised machine learning. This report focuses on active learning, a technique that relies on collaboration between machines and humans to label smartly. Active learning reduces the number of labeled examples required to train a model, saving time and money while obtaining comparable performance to models trained with much more data. With active learning, enterprises can leverage their large pool of unlabeled data to open up new product possibilities.</div>
  <div><a href="https://lwlld.fastforwardlabs.com/" target="_blank">Read the report&nbsp; ‚Üí</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF09</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://federated.fastforwardlabs.com/" target="_blank">Federated Learning</a></h2>
  <a class="report-image" href="https://federated.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff09-combo.png" />
  </a>
  <div class="small">Federated Learning makes it possible to build machine learning systems without direct access to training data. The data remains in its original location, which helps to ensure privacy and reduces communication costs. Federated learning is a great fit for smartphones and edge hardware, healthcare and other privacy-sensitive use cases, and industrial applications such as predictive maintenance.</div>
  <div><a href="https://federated.fastforwardlabs.com/" target="_blank">Read the report&nbsp; ‚Üí</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF07</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://semantic-recommendations.fastforwardlabs.com/" target="_blank">Semantic Recommendations</a></h2>
  <a class="report-image" href="https://semantic-recommendations.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff07-combo.png" />
  </a>
  <div class="small">The internet has given us an avalanche of options for what to read, watch and buy. Because of this, recommendation algorithms, which find items that will interest a particular person, are more important than ever. In this report we explore recommendation systems that make use of the semantic content of items and users to deliver richer recommendations across multiple industries.</div>
  <div><a href="https://semantic-recommendations.fastforwardlabs.com/" target="_blank">Read the report&nbsp; ‚Üí</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF04</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://summarization.fastforwardlabs.com/" target="_blank">Summarization</a></h2>
  <a class="report-image" href="https://summarization.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff04-combo.png" />
  </a>
  <div class="small">This report explores methods for extractive summarization, a capability that allows one to automatically summarize documents.  This technique has a wealth of applications: from the ability to distill thousands of product reviews, extract the most important content from long news articles, or automatically cluster customer bios into personas.</div>
  <div><a href="https://summarization.fastforwardlabs.com/" target="_blank">Read the report&nbsp; ‚Üí</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF03-2019</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://deep-learning-image-analysis.fastforwardlabs.com/" target="_blank">Deep Learning for Image Analysis: 2019 Edition</a></h2>
  <a class="report-image" href="https://deep-learning-image-analysis.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff03-2019-combo.png" />
  </a>
  <div class="small">Convolutional Neural Networks (CNN) excel at learning meaningful representations of features and concepts within images. These capabilities make CNNs extremely valuable for solving problems in domains such as medical imaging, autonomous driving, manufacturing, robotics, and urban planning. In this report, we show how to select the right deep learning models for image analysis tasks and techniques for debugging deep learning models.</div>
  <div><a href="https://deep-learning-image-analysis.fastforwardlabs.com/" target="_blank">Read the report&nbsp; ‚Üí</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF03</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://deep-learning-image-classic.fastforwardlabs.com/" target="_blank">Deep Learning: Image Analysis</a></h2>
  <a class="report-image" href="https://deep-learning-image-classic.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff03-classic-combo.png" />
  </a>
  <div class="small">Deep learning, or highly-connected neural networks, offers fascinating new capabilities for image analysis. Using deep learning, computers can now learn to identify objects in images. This report explores the history and current state of the field, predicts future developments, and explains how to apply deep learning today.</div>
  <div><a href="https://deep-learning-image-classic.fastforwardlabs.com/" target="_blank">Read the report&nbsp; ‚Üí</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF02</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://probabilistic-methods.fastforwardlabs.com/" target="_blank">Probabilistic Methods for Realtime Streams</a></h2>
  <a class="report-image" href="https://probabilistic-methods.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ffl-probabilistic-methods-report-thumb-1661363289.png" />
  </a>
  <div class="small">Since the days of analog computers built on cams and gears, we‚Äôve been engineering systems around the flow of data and the critical calculations we must perform. While the philosophy of our designs has remained consistent, our engineering constraints are constantly evolving. In the past five years we‚Äôve seen the emergence of ‚Äúbig data,‚Äù or the ability to use commodity infrastructure to analyze very large data sets in a batch. We‚Äôre currently in the midst of a significant step forward in the tools, methods, and technologies available for working with realtime streams of data.</div>
  <div><a href="https://probabilistic-methods.fastforwardlabs.com/" target="_blank">Read the report&nbsp; ‚Üí</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
</div>


</div>

<div class="spacer"></div>
<div class="spacer"></div>
 
<div class="container">
  <h1 class="clear">Prototypes</h1>
  <div style="color: #444;">Machine learning prototypes and interactive notebooks</div>
  <div class="spacer"></div>
</div>
<div id="prototypes-holder">
  
  <div style="display: flex; flex-direction: column; position: relative; padding-left: 1ch; padding-right: 1ch; height: 360px;">
  <div>
    <h5 style="margin-bottom: 0">Library</h5>
    <h2 style="padding-top: 0; margin-bottom: 0;"><a href="https://neuralqa.fastforwardlabs.com" target="_blank">NeuralQA</a></h2>
  </div>
  <div style="flex: 1 1 auto; position: relative;">
    <a href="https://neuralqa.fastforwardlabs.com" target="_blank" style="display: block; position: absolute; top: 0.375rem; width: 100%; height: calc(100% - 0.875rem); background-image: url('/images/hugo/neuralqa-1596123511.jpg'); background-size: contain; background-position: center left; background-repeat: no-repeat;"></a>
  </div>
  <div>
    <div class="small">A usable library for question answering on large datasets.</div>
    <div style="width: 100%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"><a href="https://neuralqa.fastforwardlabs.com" target="_blank">https://neuralqa.fastforwardlabs.com</a></div>
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
  <div style="display: flex; flex-direction: column; position: relative; padding-left: 1ch; padding-right: 1ch; height: 360px;">
  <div>
    <h5 style="margin-bottom: 0">Notebook</h5>
    <h2 style="padding-top: 0; margin-bottom: 0;"><a href="https://colab.research.google.com/drive/1tTiOgJ7xvy3sjfiFC9OozbjAX1ho8WN9?usp=sharing" target="_blank">Explain BERT for Question Answering Models</a></h2>
  </div>
  <div style="flex: 1 1 auto; position: relative;">
    <a href="https://colab.research.google.com/drive/1tTiOgJ7xvy3sjfiFC9OozbjAX1ho8WN9?usp=sharing" target="_blank" style="display: block; position: absolute; top: 0.375rem; width: 100%; height: calc(100% - 0.875rem); background-image: url('/images/hugo/distilexplanation-1592852137.jpg'); background-size: contain; background-position: center left; background-repeat: no-repeat;"></a>
  </div>
  <div>
    <div class="small">Tensorflow 2.0 notebook to explain and visualize a HuggingFace BERT for Question Answering model.</div>
    <div style="width: 100%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"><a href="https://colab.research.google.com/drive/1tTiOgJ7xvy3sjfiFC9OozbjAX1ho8WN9?usp=sharing" target="_blank">https://colab.research.google.com/drive/1tTiOgJ7xvy3sjfiFC9OozbjAX1ho8WN9?usp=sharing</a></div>
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
  <div style="display: flex; flex-direction: column; position: relative; padding-left: 1ch; padding-right: 1ch; height: 360px;">
  <div>
    <h5 style="margin-bottom: 0">Notebooks</h5>
    <h2 style="padding-top: 0; margin-bottom: 0;"><a href="https://qa.fastforwardlabs.com" target="_blank">NLP for Question Answering</a></h2>
  </div>
  <div style="flex: 1 1 auto; position: relative;">
    <a href="https://qa.fastforwardlabs.com" target="_blank" style="display: block; position: absolute; top: 0.375rem; width: 100%; height: calc(100% - 0.875rem); background-image: url('/images/uploads/qa.png'); background-size: contain; background-position: center left; background-repeat: no-repeat;"></a>
  </div>
  <div>
    <div class="small">Ongoing posts and code documenting the process of building a question answering model.</div>
    <div style="width: 100%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"><a href="https://qa.fastforwardlabs.com" target="_blank">https://qa.fastforwardlabs.com</a></div>
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
  <div style="display: flex; flex-direction: column; position: relative; padding-left: 1ch; padding-right: 1ch; height: 360px;">
  <div>
    <h5 style="margin-bottom: 0">Notebook</h5>
    <h2 style="padding-top: 0; margin-bottom: 0;"><a href="https://colab.research.google.com/drive/1pjPzsw_uZew-Zcz646JTkRDhF2GkPk0N" target="_blank">Interpretability Revisited: SHAP and LIME</a></h2>
  </div>
  <div style="flex: 1 1 auto; position: relative;">
    <a href="https://colab.research.google.com/drive/1pjPzsw_uZew-Zcz646JTkRDhF2GkPk0N" target="_blank" style="display: block; position: absolute; top: 0.375rem; width: 100%; height: calc(100% - 0.875rem); background-image: url('/images/uploads/shap-and-lime.png'); background-size: contain; background-position: center left; background-repeat: no-repeat;"></a>
  </div>
  <div>
    <div class="small">Explore how to use LIME and SHAP for interpretability.</div>
    <div style="width: 100%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"><a href="https://colab.research.google.com/drive/1pjPzsw_uZew-Zcz646JTkRDhF2GkPk0N" target="_blank">https://colab.research.google.com/drive/1pjPzsw_uZew-Zcz646JTkRDhF2GkPk0N</a></div>
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
</div>
<div class="container">
  <div ><button id="load_all_prototypes" style="width: 100%;">load all</button></div>
</div>

<script>
  
  window.addEventListener('load', () => {
    let $prototypes_holder = document.getElementById('prototypes-holder')
    let $load_more = document.getElementById('load_all_prototypes')
    $load_more.addEventListener('click', () => {
      fetch(`/prototypes.html`).then(r =>r.text()).then(r => {
        $load_more.remove()
        let el = document.createElement('html')
        el.innerHTML = r
        let $posts = el.querySelector('#prototypes-holder')
        $prototypes_holder.innerHTML = $posts.innerHTML
      })
    })
  })
</script>



<div class="spacer"></div>
<div class="spacer"></div>

<div class="container">
  <div>
    <h1 class="clear">Cloudera Fast Forward Labs</h1>
    <div>
      <i>Making the recently possible useful.</i><br />
      <p></p>
      <p>Cloudera Fast Forward Labs is an applied machine learning research group. Our mission is to empower enterprise data science practitioners to apply emergent academic research to production machine learning use cases in practical and socially responsible ways, while also driving innovation through the Cloudera ecosystem.  Our team brings thoughtful, creative, and diverse perspectives to deeply researched work. In this way, we strive to help organizations make the most of their ML investment as well as educate and inspire the broader machine learning and data science community.</p>
      <a
        href="https://www.cloudera.com/products/fast-forward-labs-research.html"
        >Cloudera</a
      >&nbsp;&nbsp;
      <a
        href="https://blog.fastforwardlabs.com"
        >Blog</a
      >&nbsp;&nbsp;
      <a href="https://twitter.com/fastforwardlabs">Twitter</a>
    </div>
  </div>
</div>



<div class="spacer"></div>
<div class="spacer"></div>


      </main>
 </body>
</html>
