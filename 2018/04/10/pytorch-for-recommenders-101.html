<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />
    
<title>PyTorch for Recommenders 101</title>
<meta property="og:title" content="PyTorch for Recommenders 101">
<meta property="description" content="Recommenders, generally associated with e-commerce, sift though a huge inventory of available items to find and recommend ones that a user will like. Different from search, recommenders rely on historical data to tease out user preference. How does a recommender accomplish this? In this post we explore building simple recommendation systems in PyTorch using the Movielens 100K data, which has 100,000 ratings (1-5) that 943 users provided on 1682 movies.">
<meta property="og:description" content="Recommenders, generally associated with e-commerce, sift though a huge inventory of available items to find and recommend ones that a user will like. Different from search, recommenders rely on historical data to tease out user preference. How does a recommender accomplish this? In this post we explore building simple recommendation systems in PyTorch using the Movielens 100K data, which has 100,000 ratings (1-5) that 943 users provided on 1682 movies.">
<meta property="og:image" content="http://blog.fastforwardlabs.com/images/editor_uploads/2018-04-11-175944-02_07.png">
<meta property="og:url" content="http://blog.fastforwardlabs.com/2018/04/10/pytorch-for-recommenders-101.html">
<meta property="twitter:card" content="summary_large_image">

    <link rel="stylesheet" type="text/css" href="/style.css" />
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-53030428-5', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

  <body>
  </head>
      <div class="container">
        <div class="spacer"></div>
        <div style="height: 1.5rem; padding-top: 0.35rem;">
          <a target="_blank" href="https://www.cloudera.com/products/fast-forward-labs-research.html">
            <img style="height: 0.8rem;" src="/images/cloudera-fast-forward-logo.png" />
          </a>
        </div>
        <div class="spacer"></div>
      </div>
      <main id="main">
        
<div class="container">
  <div>
    <h3 class="clear"><a href="/">Blog</a></h3>
  </div>
  <div class="spacer"></div>
  <div class="post">
    <h5 class="clear">
      <span>Apr 10, 2018</span> &middot;
      <span style="text-transform: capitalize;">
        post
      </span>
    </h5>
    <h1>PyTorch for Recommenders 101</h1>
    <p>Recommenders, generally associated with e-commerce, sift though a huge inventory
of available items to find and recommend ones that a user will like. Different
from search, recommenders rely on historical data to tease out user
preference. How does a recommender accomplish this? In this post we explore
building simple recommendation systems in <a href="http://pytorch.org/">PyTorch</a> using
the <a href="https://grouplens.org/datasets/movielens/100k/">Movielens 100K data</a>, which
has 100,000 ratings (1-5) that 943 users provided on 1682 movies.</p>
<h2 id="matrix-factorization">Matrix Factorization</h2>
<p><img src="/images/editor_uploads/2018-04-11-175944-02_07.png" alt="Diagram of a simple recommendation system. It moves from Step 1: User, to Step 2: Connected Factors, to Step 3: Recommended items based on the connected factors."></p>
<p>We first build a traditional recommendation system based on <a href="https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf">matrix
factorization</a>. The
input data is an interaction matrix where each row represents a user and each
column represents an item. The rating assigned by a user for a particular item
is found in the corresponding row and column of the interaction matrix. This
matrix is generally large but sparse; there are many items and users but a
single user would only have interacted with a small subset of items. Matrix
factorization decomposes this larger matrix into two smaller matrices - the
first one maps users into a set of factors and the second maps items into the
same set of factors. Multiplying these two smaller matrices together gives an
approximation to the original matrix, with values for empty elements
inferred. To predict a rating for a user-item pair, we simply multiply the row
representing the user from the first matrix with the column representing the
item from the second matrix.</p>
<p>In PyTorch we can implement a version of matrix factorization by using the
embedding layer to &ldquo;map&rdquo; users into a set of factors. The number of factors
determine the size of the embedding vector. Similarly we map items into their
own embedding layer. Both user and item embeddings have the same size. To
predict a user-item rating, we multiply the user embeddings with item embeddings
and sum to obtain one number. The following code draws from <a href="https://github.com/EthanRosenthal/torchmf/blob/master/torchmf.py">Ethan Rosenthal&rsquo;s
work on matrix factorization in
PyTorch</a>.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> torch
<span style="color:#f92672">from</span> torch.autograd <span style="color:#f92672">import</span> Variable

<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MatrixFactorization</span>(torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Module):
    <span style="color:#66d9ef">def</span> __init__(self, n_users, n_items, n_factors<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>):
        super()<span style="color:#f92672">.</span>__init__()
	<span style="color:#75715e"># create user embeddings</span>
        self<span style="color:#f92672">.</span>user_factors <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Embedding(n_users, n_factors,
                                               sparse<span style="color:#f92672">=</span>True)
	<span style="color:#75715e"># create item embeddings</span>
        self<span style="color:#f92672">.</span>item_factors <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Embedding(n_items, n_factors,
                                               sparse<span style="color:#f92672">=</span>True)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, user, item):
    	<span style="color:#75715e"># matrix multiplication</span>
        <span style="color:#66d9ef">return</span> (self<span style="color:#f92672">.</span>user_factors(user)<span style="color:#f92672">*</span>self<span style="color:#f92672">.</span>item_factors(item))<span style="color:#f92672">.</span>sum(<span style="color:#ae81ff">1</span>)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict</span>(self, user, item):
        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>forward(user, item)

</code></pre></div><p>To fit the matrix factorization model we need to pick a loss function and an
optimizer. In this example we use the average squared distance between the
prediction and the actual value as a loss function, this is known as
mean-squared error. We then try to minimize this loss by using stochastic
gradient descent. The code below shows how the model is fitted in four steps: i)
pass in a user-item pair, ii) forward pass to compute the predicted rating, iii)
compute the loss, and iv) backpropagate to compute gradients and update the
weights.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model <span style="color:#f92672">=</span> MatrixFactorization(n_users, n_items, n_factors<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
loss_fn <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>MSELoss() 
optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>SGD(model<span style="color:#f92672">.</span>parameters(),
                            lr<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-6</span>)

<span style="color:#66d9ef">for</span> user, item <span style="color:#f92672">in</span> zip(users, items):
    <span style="color:#75715e"># get user, item and rating data</span>
    rating <span style="color:#f92672">=</span> Variable(torch<span style="color:#f92672">.</span>FloatTensor([ratings[user, item]]))
    user <span style="color:#f92672">=</span> Variable(torch<span style="color:#f92672">.</span>LongTensor([int(user)]))
    item <span style="color:#f92672">=</span> Variable(torch<span style="color:#f92672">.</span>LongTensor([int(item)]))

    <span style="color:#75715e"># predict</span>
    prediction <span style="color:#f92672">=</span> model(user, item)
    loss <span style="color:#f92672">=</span> loss_fn(prediction, rating)

    <span style="color:#75715e"># backpropagate</span>
    loss<span style="color:#f92672">.</span>backward()

    <span style="color:#75715e"># update weights</span>
    optimizer<span style="color:#f92672">.</span>step()
</code></pre></div><p>We train this model on the Movielens dataset with ratings scaled between [0, 1]
to help with convergence. Applied on the test set, we obtain a root mean-squared
error(RMSE) of 0.66. This means that on average, the difference between our
prediction and the actual value is 0.66!</p>
<h2 id="dense-feedforward-neural-network">Dense Feedforward Neural Network</h2>
<p>Given the underwhelming performance of our matrix factorization model, we try a
simple feedforward recommendation system instead. The input to this neural
network is a pair of user and item represented by their IDs. Both user and item
IDs first pass through an embedding layer. The output of the embedding layer,
which are two embedding vectors, are then concatenated into one and passed into
a linear network. The output of the linear network is one dimensional -
representing the rating for the user-item pair. The model is fit the same way as
the matrix factorization model and uses the standard PyTorch approach of forward
passing, computing the loss, backpropagating and updating weights.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> torch
<span style="color:#f92672">import</span> torch.nn.functional <span style="color:#f92672">as</span> F
<span style="color:#f92672">from</span> torch.autograd <span style="color:#f92672">import</span> Variable
<span style="color:#f92672">from</span> torch <span style="color:#f92672">import</span> nn

<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">DenseNet</span>(nn<span style="color:#f92672">.</span>Module):

    <span style="color:#66d9ef">def</span> __init__(self, n_users, n_items, n_factors, H1, D_out):
        <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">        Simple Feedforward with Embeddings
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        super()<span style="color:#f92672">.</span>__init__()
   	<span style="color:#75715e"># user and item embedding layers</span>
        self<span style="color:#f92672">.</span>user_factors <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Embedding(n_users, n_factors,
                                               sparse<span style="color:#f92672">=</span>True)
        self<span style="color:#f92672">.</span>item_factors <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Embedding(n_items, n_factors,
                                               sparse<span style="color:#f92672">=</span>True)
   	<span style="color:#75715e"># linear layers</span>
        self<span style="color:#f92672">.</span>linear1 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Linear(n_factors<span style="color:#f92672">*</span><span style="color:#ae81ff">2</span>, H1)
        self<span style="color:#f92672">.</span>linear2 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Linear(H1, D_out)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, users, items):
        users_embedding <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>user_factors(users)
        items_embedding <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>item_factors(items)
	<span style="color:#75715e"># concatenate user and item embeddings to form input</span>
        x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([users_embedding, items_embedding], <span style="color:#ae81ff">1</span>)
        h1_relu <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>linear1(x))
        output_scores <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>linear2(h1_relu)
        <span style="color:#66d9ef">return</span> output_scores

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict</span>(self, users, items):
        <span style="color:#75715e"># return the score</span>
        output_scores <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>forward(users, items)
        <span style="color:#66d9ef">return</span> output_scores

</code></pre></div><p>Once again, we train this model on the Movielens dataset with ratings scaled
between [0, 1] to help with convergence. Applied on the test set, we obtain a
root mean-squared error(RMSE) of 0.28, a substantial improvement!</p>
<h2 id="sequence-based-recommendation-system">Sequence based Recommendation System</h2>
<p>Finally we build a recommendation system that takes into account the sequence of
item interactions. The heart of this is a Long Short-Term Memory (LSTM) cell, a
variant of Recurrent Neural Networks (RNN) with faster convergence and better
long term memory. The input to this system is a history of item interactions and
their corresponding ratings. In the following example of an input, we show a
sequence of item interaction of length 10 (arbitrarily chosen) and the
corresponding rating. Elements in the first array correspond to items(movies),
and elements in the second array correspond to ratings. We see that, for
example, movie 209 has a rating of 4, and movie 32 has a rating of 5. Sequences
shorter than 10 are padded with zeros.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">In [<span style="color:#ae81ff">2</span>]: training_data[<span style="color:#ae81ff">0</span>]
Out[<span style="color:#ae81ff">2</span>]: 
(array([<span style="color:#ae81ff">209</span>,  <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">189</span>, <span style="color:#ae81ff">242</span>, <span style="color:#ae81ff">171</span>, <span style="color:#ae81ff">111</span>, <span style="color:#ae81ff">256</span>,   <span style="color:#ae81ff">5</span>,  <span style="color:#ae81ff">74</span>, <span style="color:#ae81ff">102</span>], dtype<span style="color:#f92672">=</span>int32),
 array([<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>], dtype<span style="color:#f92672">=</span>int32))
</code></pre></div><p>Items are passed through an embedding layer before going into the LSTM. The
output of the LSTM is then fed into a linear layer with an output dimension of
one. The LSTM has 2 hidden states, one for short term memory and one for long
term. Both states need to be initialized.</p>
<p>PyTorch expects LSTM inputs to be a three dimensional tensor. The first
dimension is the length of the sequence itself, the second represents the number
of instances in a mini-batch, the third is the size of the actual input into the
LSTM. Using our training data example with sequence of length 10 and embedding
dimension of 20, input to the LSTM is a tensor of size 10x1x20 when we do not
use mini batches. For a mini-batch size of 2, each forward pass will have two
sequences, and the input to the LSTM needs to have a dimension of 10x2x20. LSTMs
take variable input sequence lengths but for batch training purposes the input
data is generally processed(with padding if necessary) to have a fixed length.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> torch
<span style="color:#f92672">import</span> torch.nn <span style="color:#f92672">as</span> nn
<span style="color:#f92672">from</span> torch.autograd <span style="color:#f92672">import</span> Variable

<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LSTMRating</span>(nn<span style="color:#f92672">.</span>Module):

    <span style="color:#66d9ef">def</span> __init__(self, embedding_dim, hidden_dim, num_items, num_output):
        super()<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>hidden_dim <span style="color:#f92672">=</span> hidden_dim
        self<span style="color:#f92672">.</span>item_embeddings <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Embedding(num_items, embedding_dim)
        self<span style="color:#f92672">.</span>lstm <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LSTM(embedding_dim, hidden_dim)
        self<span style="color:#f92672">.</span>linear <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(hidden_dim, num_output)
        self<span style="color:#f92672">.</span>hidden <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>init_hidden()

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">init_hidden</span>(self):
    	<span style="color:#75715e"># initialize both hidden layers</span>
        <span style="color:#66d9ef">return</span> (Variable(torch<span style="color:#f92672">.</span>zeros(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>hidden_dim)),
                Variable(torch<span style="color:#f92672">.</span>zeros(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>hidden_dim)))

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, sequence):
        embeddings <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>item_embeddings(sequence)
        output, self<span style="color:#f92672">.</span>hidden <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lstm(embeddings<span style="color:#f92672">.</span>view(len(sequence), <span style="color:#ae81ff">1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>),
                                        self<span style="color:#f92672">.</span>hidden)
        rating_scores <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>linear(output<span style="color:#f92672">.</span>view(len(sequence), <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>))
        <span style="color:#66d9ef">return</span> rating_scores

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict</span>(self, sequence):
        rating_scores <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>forward(sequence)
        <span style="color:#66d9ef">return</span> rating_scores

</code></pre></div><p>Once the neural network is defined, we fit the training data using stochastic
gradient descent with a mean squared error loss function.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">embedding_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">64</span>
hidden_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span>
n_output <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>

<span style="color:#75715e"># add one to represent padding when there is not enough history</span>
model <span style="color:#f92672">=</span> LSTMRating(embedding_dim, hidden_dim, n_items<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>, n_output)
loss_fn <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>MSELoss()
optimizer <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>SGD(model<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>)

<span style="color:#66d9ef">for</span> sequence, target_ratings <span style="color:#f92672">in</span> training_data:
    model<span style="color:#f92672">.</span>zero_grad()
    <span style="color:#75715e"># initialize hidden layers</span>
    model<span style="color:#f92672">.</span>hidden <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>init_hidden()
    <span style="color:#75715e"># convert sequence to PyTorch variables</span>
    sequence_var <span style="color:#f92672">=</span> Variable(torch<span style="color:#f92672">.</span>LongTensor(sequence<span style="color:#f92672">.</span>astype(<span style="color:#e6db74">&#39;int64&#39;</span>)))
    <span style="color:#75715e"># forward pass</span>
    ratings_scores <span style="color:#f92672">=</span> model(sequence_var)
    target_ratings_var <span style="color:#f92672">=</span> Variable(torch<span style="color:#f92672">.</span>FloatTensor(target_ratings<span style="color:#f92672">.</span>astype(<span style="color:#e6db74">&#39;float32&#39;</span>)))
    <span style="color:#75715e"># compute loss</span>
    loss <span style="color:#f92672">=</span> loss_fn(ratings_scores, target_ratings_var)
    <span style="color:#75715e"># backpropagate</span>
    loss<span style="color:#f92672">.</span>backward()
    <span style="color:#75715e"># update weights</span>
    optimizer<span style="color:#f92672">.</span>step()

</code></pre></div><p>Similar to other models, we train the LSTM-based model on the Movielens dataset
with ratings scaled between [0, 1] to help with convergence. Applied on the test
set, we obtain a root mean-squared error(RMSE) of 0.43 - the LSTM model
underperforms the dense feed forward network.</p>
<h2 id="post-amble">Post-amble</h2>
<p>The models discussed in this post are basic building blocks for a recommendation
system in PyTorch. There are no bells and whistles and we did not attempt to
fine tune any hyperparameters. Our first pass result suggests that the dense
network performs best, followed by the LSTM network and finally the matrix
factorization model. The root mean-squared error (RMSE) are 0.28, 0.43 and 0.66
respectively on the Movielens 100K dataset with ratings scaled between [0,
1]. We thought PyTorch was fun to use; models can be built and swapped out
relatively easily. When we did encounter errors, most of them were triggered by
incorrect data types.</p>
<p>For more on recommendations, please see our <a href="https://www.cloudera.com/products/fast-forward-labs-research.html">Semantic Recommendations report</a>,
where we focus on how machines can better understand content!</p>

    <div class="spacer"></div>
    <div>
      <div style="width: 100%; height: 2px; background: #ccc; margin-top: -1px; margin-bottom: -1px;"></div>
    </div>
    <div class="spacer"></div>
  </div>
</div>


<div class="container">
  <div class="spacer"></div>
  <h2 class="clear">Read more</h2>
  <div class="spacer"></div>
  <div style="display: grid; grid-template-columns: 1fr 1fr; grid-column-gap: 2ch;">
    <div>
      
      <div class="small">Newer</div>
      <div>
  <h5 style="margin-bottom: 0px;">
    <span>Apr 18, 2018</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2018/04/18/introducing-scifi.html"><strong>Introducing SciFi</strong></a></div>
  
  <div class="spacer"></div>
</div>


      
    </div>
    <div>
      
      <div class="small">Older</div>
      <div>
  <h5 style="margin-bottom: 0px;">
    <span>Mar 28, 2018</span> &middot;
    <span style="text-transform: capitalize;">
      newsletter
    </span>
  </h5>
  
    <div><a href="/2018/03/28/unemployment-vs.-augmentation.html"><strong>Unemployment vs. Augmentation</strong></a></div>
  
  <div class="spacer"></div>
</div>


      
    </div>
  </div>
</div>

<div class="container">
<div class="spacer"></div>
<div>
  <div style="width: 100%; height: 2px; background: #ccc; margin-top: -1px; margin-bottom: -1px;"></div>
</div>
<div class="spacer"></div>
<div class="spacer"></div>
</div>

<div class="container">
  

<h2 class="clear">Latest posts</h2>
<div class="spacer"></div>

<div id="posts-holder"> 
  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Jun 16, 2020</span> &middot;
    <span style="text-transform: capitalize;">
      notebook
    </span>
  </h5>
  
  <a href="/2020/06/16/evaluating-qa-metrics-predictions-and-the-null-response.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/shotwin-2020-06-16_09-31-48-1592314597.png" />
  </a>
  
  <div>
    
    <a href="https://qa.fastforwardlabs.com/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html" target="_blank">
      <h2 style="margin-bottom: 4px;">Evaluating QA: Metrics, Predictions, and the Null Response →</h2></a
    >
    
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="https://www.linkedin.com/in/melanierbeck">Melanie</a>
        &middot; </span
      >
    </span>
    A deep dive into computing QA predictions and when to tell BERT to zip it! In our last post, Building a QA System with BERT on Wikipedia, we used the HuggingFace framework to train BERT on the SQuAD2.0 dataset and built a simple QA system on top of the Wikipedia search engine. This time, we&rsquo;ll look at how to assess the quality of a BERT-like model for Question Answering.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="https://qa.fastforwardlabs.com/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html" target="_blank">
        	
        qa.fastforwardlabs.com
      </a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>May 19, 2020</span> &middot;
    <span style="text-transform: capitalize;">
      notebook
    </span>
  </h5>
  
  <a href="/2020/05/19/building-a-qa-system-with-bert-on-wikipedia.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/markus-spiske-C0koz3G1I4I-unsplash.jpg" />
  </a>
  
  <div>
    
    <a href="https://qa.fastforwardlabs.com/pytorch/hugging%20face/wikipedia/bert/transformers/2020/05/19/Getting_Started_with_QA.html" target="_blank">
      <h2 style="margin-bottom: 4px;">Building a QA System with BERT on Wikipedia →</h2></a
    >
    
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="https://www.linkedin.com/in/melanierbeck">Melanie</a>
        &middot; </span
      >
    </span>
    So you&rsquo;ve decided to build a QA system. You want to start with something simple and general so you plan to make it open domain using Wikipedia as a corpus for answering questions. You want to use the best NLP that your compute resources allow (you&rsquo;re lucky enough to have access to a GPU) so you&rsquo;re going to focus on the big, flashy Transformer models that are all the rage these days.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="https://qa.fastforwardlabs.com/pytorch/hugging%20face/wikipedia/bert/transformers/2020/05/19/Getting_Started_with_QA.html" target="_blank">
        	
        qa.fastforwardlabs.com
      </a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Apr 28, 2020</span> &middot;
    <span style="text-transform: capitalize;">
      notebook
    </span>
  </h5>
  
  <a href="/2020/04/28/intro-to-automated-question-answering.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/qa-workflow.png" />
  </a>
  
  <div>
    
    <a href="https://qa.fastforwardlabs.com/methods/background/2020/04/28/Intro-to-QA.html" target="_blank">
      <h2 style="margin-bottom: 4px;">Intro to Automated Question Answering →</h2></a
    >
    
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="https://www.linkedin.com/in/melanierbeck">Melanie</a>
        &middot; </span
      >
    </span>
    Welcome to the first edition of the Cloudera Fast Forward blog on Natural Language Processing for Question Answering! Throughout this series, we’ll build a Question Answering (QA) system with off-the-shelf algorithms and libraries and blog about our process and what we find along the way. We hope to wind up with a beginning-to-end documentary that provides:
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="https://qa.fastforwardlabs.com/methods/background/2020/04/28/Intro-to-QA.html" target="_blank">
        	
        qa.fastforwardlabs.com
      </a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Apr 1, 2020</span> &middot;
    <span style="text-transform: capitalize;">
      newsletter
    </span>
  </h5>
  
  <a href="/2020/04/01/enterprise-grade-ml.html" class="preview-image-holder">
    <img class="preview-image" src="/images/2020/03/Screen_Shot_2020_03_27_at_4_17_39_PM-1585340376058.png" />
  </a>
  
  <div>
    
    <a href="/2020/04/01/enterprise-grade-ml.html"
       ><h2 style="margin-bottom: 4px;">Enterprise Grade ML</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="https://twitter.com/shioulin_sam">Shioulin</a>
        &middot; </span
      >
    </span>
    At Cloudera Fast Forward, one of the mechanisms we use to tightly couple machine learning research with application is through application development projects for both internal and external clients. The problems we tackle in these projects are wide ranging and cut across various industries; the end goal is a production system that translates data into business impact.
What is Enterprise Grade Machine Learning? Enterprise grade ML, a term mentioned in a paper put forth by Microsoft, refers to ML applications where there is a high level of scrutiny for data handling, model fairness, user privacy, and debuggability.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/2020/04/01/enterprise-grade-ml.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Apr 1, 2020</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
  <a href="/2020/04/01/bias-in-knowledge-graphs-part-1.html" class="preview-image-holder">
    <img class="preview-image" src="/images/editor_uploads/2020-03-28-150645-balance_2108024_1920.jpg" />
  </a>
  
  <div>
    
    <a href="/2020/04/01/bias-in-knowledge-graphs-part-1.html"
       ><h2 style="margin-bottom: 4px;">Bias in Knowledge Graphs - Part 1</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="https://twitter.com/keitabr">Keita</a>
        &middot; </span
      >
    </span>
    Introduction This is the first part of a series to review Bias in Knowledge Graphs (KG). We aim to describe methods of identifying bias, measuring its impact, and mitigating that impact. For this part, we’ll give a broad overview of this topic.
image credit: Mediamodifier from Pixabay Motivation Knowledge graphs, graphs with built-in ontologies, create unique opportunities for data analytics, machine learning, and data mining. They do this by enhancing data with the power of connections and human knowledge.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/2020/04/01/bias-in-knowledge-graphs-part-1.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Feb 27, 2020</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
  <a href="/2020/02/27/privacy-data-governance-and-machine-learning-the-regulatory-perspective.html" class="preview-image-holder">
    <img class="preview-image" src="/images/editor_uploads/2020-02-28-180128-markus_spiske_FXFz_sW0uwo_unsplash.jpg" />
  </a>
  
  <div>
    
    <a href="/2020/02/27/privacy-data-governance-and-machine-learning-the-regulatory-perspective.html"
       ><h2 style="margin-bottom: 4px;">Privacy, data governance, and machine learning: the regulatory perspective</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="https://www.linkedin.com/in/varun-bhatnagar-74303437/">Varun</a>
        &middot; </span
      >
    </span>
    Why do privacy and governance matter? Data privacy has been a common conversation topic among the general public since the Cambridge Analytica scandal in 2018. The data &ldquo;breach,&rdquo; in which user information was hoovered up through a Facebook quiz and subsequently misrepresented as being used for academic purposes, resulted in over $5 billion in fines for Facebook. However, Facebook&rsquo;s infringements were, in fact, relatively narrow in scope (though nonetheless egregious) compared to the growing remit of privacy law.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/2020/02/27/privacy-data-governance-and-machine-learning-the-regulatory-perspective.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
</div>

<div>
  <button id="load_more" style="width: 100%;">load more</button>
</div>
<div class="spacer"></div>
<div class="spacer"></div>

<script>
  window.addEventListener('load', () => {
    let $posts_holder = document.getElementById('posts-holder')
    let $load_more = document.getElementById('load_more')
    let next_page = 2
    $load_more.addEventListener('click', () => {
      fetch(`/posts/page/${next_page}.html`).then(r =>r.text()).then(r => {
        let el = document.createElement('html')
        el.innerHTML = r
        next_page += 1
        let $posts = el.querySelector('#posts-holder').children
        for (let i=0; i< $posts.length; i++) {
          let $post = $posts[i].cloneNode(true)
          $posts_holder.appendChild($post)
        }
      })
    })
  })
</script>


  <h3 class="clear">Popular posts</h3>
<div class="spacer"></div>
<div>
  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Oct 30, 2019</span> &middot;
    <span style="text-transform: capitalize;">
      newsletter
    </span>
  </h5>
  
    <div><a href="/2019/10/30/exciting-applications-of-graph-neural-networks.html"><strong>Exciting Applications of Graph Neural Networks</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Nov 14, 2018</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2018/11/14/federated-learning-distributed-machine-learning-with-data-locality-and-privacy.html"><strong>Federated learning: distributed machine learning with data locality and privacy</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Apr 10, 2018</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2018/04/10/pytorch-for-recommenders-101.html"><strong>PyTorch for Recommenders 101</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Oct 4, 2017</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2017/10/04/first-look-using-three.js-for-2d-data-visualization.html"><strong>First Look: Using Three.js for 2D Data Visualization</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Aug 22, 2016</span> &middot;
    <span style="text-transform: capitalize;">
      whitepaper
    </span>
  </h5>
  
    <div><a href="/2016/08/22/under-the-hood-of-the-variational-autoencoder-in-prose-and-code.html"><strong>Under the Hood of the Variational Autoencoder (in Prose and Code)</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Feb 24, 2016</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2016/02/24/hello-world-in-keras-or-scikit-learn-versus-keras.html"><strong>&#34;Hello world&#34; in Keras (or, Scikit-learn versus Keras)</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
</div>

</div>

<div class="spacer"></div>
<div style="background: #efefef;">
  <div class="spacer"></div>
  <div class="spacer"></div>
  <div class="container">
  <h1 class="clear">Reports</h1>
  <div style="color: #444;">In-depth guides to specific machine learning capabilities</div>
</div>
<div class="spacer"></div>
<div style="max-width: 96ch; margin: 0 auto; padding-left: 1ch; padding-right: 1ch;">
  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF13</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://ff13.fastforwardlabs.com" target="_blank">Causality for Maching Learning</a></h2>
  <a class="report-image" href="https://ff13.fastforwardlabs.com" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff13-combo.png" />
  </a>
  <div class="small">The intersection of causal inference and machine learning is a rapidly expanding area of research that&#39;s already yielding capabilities to enable building more robust, reliable, and fair machine learning systems. This report offers an introduction to causal reasoning including causal graphs and invariant prediction and how to apply causal inference tools together with classic machine learning techniques in multiple use-cases.</div>
  <div><a href="https://ff13.fastforwardlabs.com" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF06-2020</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://ff06-2020.fastforwardlabs.com" target="_blank">Interpretability</a></h2>
  <a class="report-image" href="https://ff06-2020.fastforwardlabs.com" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff06-2020-combo.png" />
  </a>
  <div class="small">Interpretability, or the ability to explain why and how a system makes a decision, can help us improve models, satisfy regulations, and build better products. Black-box techniques like deep learning have delivered breakthrough capabilities at the cost of interpretability. In this report, recently updated to include techniques like SHAP, we show how to make models interpretable without sacrificing their capabilities or accuracy.</div>
  <div><a href="https://ff06-2020.fastforwardlabs.com" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF12</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://ff12.fastforwardlabs.com" target="_blank">Deep Learning for Anomaly Detection</a></h2>
  <a class="report-image" href="https://ff12.fastforwardlabs.com" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff12-combo.png" />
  </a>
  <div class="small">From fraud detection to flagging abnormalities in imaging data, there are countless applications for automatic identification of abnormal data. This process can be challenging, especially when working with large, complex data. This report explores deep learning approaches (sequence models, VAEs, GANs) for anomaly detection, when to use them, performance benchmarks, and product possibilities.</div>
  <div><a href="https://ff12.fastforwardlabs.com" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
</div>


</div>

<div class="spacer"></div>
<div class="spacer"></div>
 
<div class="container">
  <h1 class="clear">Prototypes</h1>
  <div style="color: #444;">Machine learning prototypes and interactive notebooks</div>
  <div class="spacer"></div>
</div>
<div id="prototypes-holder">
  
  <div style="display: flex; flex-direction: column; position: relative; padding-left: 1ch; padding-right: 1ch; height: 360px;">
  <div>
    <h5 style="margin-bottom: 0">Notebooks</h5>
    <h2 style="padding-top: 0; margin-bottom: 0;"><a href="https://qa.fastforwardlabs.com" target="_blank">NLP for Question Answering</a></h2>
  </div>
  <div style="flex: 1 1 auto; position: relative;">
    <a href="https://qa.fastforwardlabs.com" target="_blank" style="display: block; position: absolute; top: 0.375rem; width: 100%; height: calc(100% - 0.875rem); background-image: url('https://experiments.fastforwardlabs.com/images/uploads/qa.png'); background-size: contain; background-position: center left; background-repeat: no-repeat;"></a>
  </div>
  <div>
    <div class="small">Ongoing posts and code documenting the process of building a question answering model.</div>
    <div style="width: 100%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"><a href="https://qa.fastforwardlabs.com" target="_blank">https://qa.fastforwardlabs.com</a></div>
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
  <div style="display: flex; flex-direction: column; position: relative; padding-left: 1ch; padding-right: 1ch; height: 360px;">
  <div>
    <h5 style="margin-bottom: 0">Notebook</h5>
    <h2 style="padding-top: 0; margin-bottom: 0;"><a href="https://colab.research.google.com/drive/1pjPzsw_uZew-Zcz646JTkRDhF2GkPk0N" target="_blank">Interpretability Revisited: SHAP and LIME</a></h2>
  </div>
  <div style="flex: 1 1 auto; position: relative;">
    <a href="https://colab.research.google.com/drive/1pjPzsw_uZew-Zcz646JTkRDhF2GkPk0N" target="_blank" style="display: block; position: absolute; top: 0.375rem; width: 100%; height: calc(100% - 0.875rem); background-image: url('https://experiments.fastforwardlabs.com/images/uploads/shap-and-lime.png'); background-size: contain; background-position: center left; background-repeat: no-repeat;"></a>
  </div>
  <div>
    <div class="small">Explore how to use LIME and SHAP for interpretability.</div>
    <div style="width: 100%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"><a href="https://colab.research.google.com/drive/1pjPzsw_uZew-Zcz646JTkRDhF2GkPk0N" target="_blank">https://colab.research.google.com/drive/1pjPzsw_uZew-Zcz646JTkRDhF2GkPk0N</a></div>
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
  <div style="display: flex; flex-direction: column; position: relative; padding-left: 1ch; padding-right: 1ch; height: 360px;">
  <div>
    <h5 style="margin-bottom: 0">Prototype</h5>
    <h2 style="padding-top: 0; margin-bottom: 0;"><a href="https://refractor.fastforwardlabs.com" target="_blank">Refractor</a></h2>
  </div>
  <div style="flex: 1 1 auto; position: relative;">
    <a href="https://refractor.fastforwardlabs.com" target="_blank" style="display: block; position: absolute; top: 0.375rem; width: 100%; height: calc(100% - 0.875rem); background-image: url('https://experiments.fastforwardlabs.com/images/uploads/refractor.gif'); background-size: contain; background-position: center left; background-repeat: no-repeat;"></a>
  </div>
  <div>
    <div class="small">Refractor predicts churn probabilities for telecom customers and shows which customer attributes contribute to those predictions.</div>
    <div style="width: 100%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"><a href="https://refractor.fastforwardlabs.com" target="_blank">https://refractor.fastforwardlabs.com</a></div>
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
  <div style="display: flex; flex-direction: column; position: relative; padding-left: 1ch; padding-right: 1ch; height: 360px;">
  <div>
    <h5 style="margin-bottom: 0">Prototype</h5>
    <h2 style="padding-top: 0; margin-bottom: 0;"><a href="https://anomagram.fastforwardlabs.com" target="_blank">Anomagram</a></h2>
  </div>
  <div style="flex: 1 1 auto; position: relative;">
    <a href="https://anomagram.fastforwardlabs.com" target="_blank" style="display: block; position: absolute; top: 0.375rem; width: 100%; height: calc(100% - 0.875rem); background-image: url('https://experiments.fastforwardlabs.com/images/uploads/anomagram.gif'); background-size: contain; background-position: center left; background-repeat: no-repeat;"></a>
  </div>
  <div>
    <div class="small">An interactive visualization tool for exploring how a deep learning model can be applied to the task of anomaly detection.</div>
    <div style="width: 100%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"><a href="https://anomagram.fastforwardlabs.com" target="_blank">https://anomagram.fastforwardlabs.com</a></div>
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
</div>
<div class="container">
  <div ><button id="load_all_prototypes" style="width: 100%;">load all</button></div>
</div>

<script>
  
  window.addEventListener('load', () => {
    let $prototypes_holder = document.getElementById('prototypes-holder')
    let $load_more = document.getElementById('load_all_prototypes')
    $load_more.addEventListener('click', () => {
      fetch(`/prototypes.html`).then(r =>r.text()).then(r => {
        $load_more.remove()
        let el = document.createElement('html')
        el.innerHTML = r
        let $posts = el.querySelector('#prototypes-holder')
        $prototypes_holder.innerHTML = $posts.innerHTML
      })
    })
  })
</script>



<div class="spacer"></div>
<div class="spacer"></div>

<div class="container">
  <div>
    <h1 class="clear">About</h1>
    <div>
      Cloudera Fast Forward is an applied machine learning reseach group.<br />
      <a
        href="https://www.cloudera.com/products/fast-forward-labs-research.html"
        >Cloudera</a
      >&nbsp;&nbsp;
      <a
        href="https://blog.fastforwardlabs.com"
        >Blog</a
      >&nbsp;&nbsp;
      <a href="https://twitter.com/fastforwardlabs">Twitter</a>
    </div>
  </div>
</div>



<div class="spacer"></div>
<div class="spacer"></div>


      </main>
 </body>
</html>
