<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />
    
<title>Under the Hood of the Variational Autoencoder (in Prose and Code)</title>
<meta property="og:title" content="Under the Hood of the Variational Autoencoder (in Prose and Code)">
<meta property="description" content="The Variational Autoencoder (VAE) neatly synthesizes unsupervised deep learning and variational Bayesian methods into one sleek package. In Part I of this series, we introduced the theory and intuition behind the VAE, an exciting development in machine learning for combined generative modeling and inference—“machines that imagine and reason.”
To recap: VAEs put a probabilistic spin on the basic autoencoder paradigm—treating their inputs, hidden representations, and reconstructed outputs as probabilistic random variables within a directed graphical model. With this Bayesian perspective, the encoder becomes a variational inference network, mapping observed inputs to (approximate) posterior distributions over latent space, and the decoder becomes a generative network, capable of mapping arbitrary latent coordinates back to distributions over the original data space.


The beauty of this setup is that we can take a principled Bayesian approach toward building systems with a rich internal “mental model” of the observed world, all by training a single, cleverly-designed deep neural network.
These benefits derive from an enriched understanding of data as merely the tip of the iceberg—the observed result of an underlying causative probabilistic process.
The power of the resulting model is captured by Feynman’s famous chalkboard quote: “What I cannot create, I do not understand.” When trained on MNIST handwritten digits, our VAE model can parse the information spread thinly over the high-dimensional observed world of pixels, and condense the most meaningful features into a structured distribution over reduced latent dimensions.
Having recovered the latent manifold and assigned it a coordinate system, it becomes trivial to walk from one point to another along the manifold, creatively generating realistic digits all the while:


In this post, we’ll take a look under the hood at the math and technical details that allow us to optimize the VAE model we sketched in Part I.
Along the way, we’ll show how to implement a VAE in TensorFlow—a library for efficient numerical computation using data flow graphs, with key features like automatic differentiation and parallelizability (across clusters, CPUs, GPUs…and TPUs if you’re lucky). You can find (and tinker with!) the full implementation here, along with a couple pre-trained models.">
<meta property="og:description" content="The Variational Autoencoder (VAE) neatly synthesizes unsupervised deep learning and variational Bayesian methods into one sleek package. In Part I of this series, we introduced the theory and intuition behind the VAE, an exciting development in machine learning for combined generative modeling and inference—“machines that imagine and reason.”
To recap: VAEs put a probabilistic spin on the basic autoencoder paradigm—treating their inputs, hidden representations, and reconstructed outputs as probabilistic random variables within a directed graphical model. With this Bayesian perspective, the encoder becomes a variational inference network, mapping observed inputs to (approximate) posterior distributions over latent space, and the decoder becomes a generative network, capable of mapping arbitrary latent coordinates back to distributions over the original data space.


The beauty of this setup is that we can take a principled Bayesian approach toward building systems with a rich internal “mental model” of the observed world, all by training a single, cleverly-designed deep neural network.
These benefits derive from an enriched understanding of data as merely the tip of the iceberg—the observed result of an underlying causative probabilistic process.
The power of the resulting model is captured by Feynman’s famous chalkboard quote: “What I cannot create, I do not understand.” When trained on MNIST handwritten digits, our VAE model can parse the information spread thinly over the high-dimensional observed world of pixels, and condense the most meaningful features into a structured distribution over reduced latent dimensions.
Having recovered the latent manifold and assigned it a coordinate system, it becomes trivial to walk from one point to another along the manifold, creatively generating realistic digits all the while:


In this post, we’ll take a look under the hood at the math and technical details that allow us to optimize the VAE model we sketched in Part I.
Along the way, we’ll show how to implement a VAE in TensorFlow—a library for efficient numerical computation using data flow graphs, with key features like automatic differentiation and parallelizability (across clusters, CPUs, GPUs…and TPUs if you’re lucky). You can find (and tinker with!) the full implementation here, along with a couple pre-trained models.">
<meta property="og:image" content="https://blog.fastforwardlabs.comhttp://fastforwardlabs.github.io/blog-images/miriam/imgs_code/160816_1754_reloaded_latent_784_500_500_50_round_65536_morph_4730816952.gif">
<meta property="og:url" content="https://blog.fastforwardlabs.com/2016/08/22/under-the-hood-of-the-variational-autoencoder-in-prose-and-code.html">
<meta property="twitter:card" content="summary_large_image">
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <link rel="stylesheet" type="text/css" href="/style.css" />
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-53030428-5', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

  </head>
  <body>
      <div class="container">
        <div class="spacer"></div>
        <div style="height: 2.5rem; padding-top: 0.35rem;">
          <a target="_blank" href="https://www.cloudera.com/products/fast-forward-labs-research.html">
            <img style="height: 2rem;" src="/images/cloudera-fast-forward-logo.png" />
          </a>
        </div>
        <div class="spacer"></div>
      </div>
      <main id="main">
        
<div class="container">
  <div>
    <h3 class="clear"><a href="/">Blog</a></h3>
  </div>
  <div class="spacer"></div>
  <div class="post">
    <h5 class="clear">
      <span>Aug 22, 2016</span> &middot;
      <span style="text-transform: capitalize;">
        whitepaper
      </span>
    </h5>
    <h1>Under the Hood of the Variational Autoencoder (in Prose and Code)</h1>
    <h5 id="the-a-hrefhttpsarxivorgabs13126114variationala-a-hrefhttpsarxivorgabs14014082autoencodera-vae-neatly-synthesizes-unsupervised-deep-learning-and-variational-bayesian-methods-into-one-sleek-package-in-a-hrefhttpblogfastforwardlabscom20160812introducing-variational-autoencoders-in-prose-andhtmlpart-ia-of-this-series-we-introduced-the-theory-and-intuition-behind-the-vae-an-exciting-development-in-machine-learning-for-combined-generative-modeling-and-inferencea-hrefhttpshakirmcomslidesdlsummerschool_aug2016_compresspdfmachines-that-imagine-and-reasona">The <a href="https://arxiv.org/abs/1312.6114">Variational</a> <a href="https://arxiv.org/abs/1401.4082">Autoencoder</a> (VAE) neatly synthesizes unsupervised deep learning and variational Bayesian methods into one sleek package. In <a href="http://blog.fastforwardlabs.com/2016/08/12/introducing-variational-autoencoders-in-prose-and.html">Part I</a> of this series, we introduced the theory and intuition behind the VAE, an exciting development in machine learning for combined generative modeling and inference—<a href="http://shakirm.com/slides/DLSummerSchool_Aug2016_compress.pdf">“machines that imagine and reason.”</a></h5>
<p>To recap: VAEs put a probabilistic spin on the basic autoencoder paradigm—treating their inputs, hidden representations, and reconstructed outputs as probabilistic random variables within a directed graphical model. With this <a href="https://xkcd.com/1236/">Bayesian</a> perspective, the encoder becomes a variational <em>inference network</em>, mapping observed inputs to (approximate) posterior distributions over latent space, and the decoder becomes a <em>generative network</em>, capable of mapping arbitrary latent coordinates back to distributions over the original data space.</p>
<div class="figure">
<img src="http://fastforwardlabs.github.io/blog-images/miriam/imgs_code/vae.4.png" title="A variational autoencoder" style="width:80.0%"/></div>
<p>The beauty of this setup is that we can take a principled Bayesian approach toward building systems with a rich internal “mental model” of the observed world, all by training a single, cleverly-designed deep neural network.</p>
<p>These benefits derive from an enriched understanding of data as merely the tip of the iceberg—the observed result of an underlying causative probabilistic process.</p>
<p>The power of the resulting model is captured by Feynman’s famous <a href="http://archives-dc.library.caltech.edu/islandora/object/ct1:483">chalkboard quote</a>: “What I cannot create, I do not understand.” When trained on MNIST handwritten digits, our VAE model can parse the information spread thinly over the high-dimensional observed world of pixels, and condense the most meaningful features into a structured distribution over reduced latent dimensions.</p>
<p>Having recovered the latent manifold and assigned it a coordinate system, it becomes trivial to walk from one point to another along the manifold, creatively generating realistic digits all the while:</p>
<div class="figure">
<img src="http://fastforwardlabs.github.io/blog-images/miriam/imgs_code/160816_1754_reloaded_latent_784_500_500_50_round_65536_morph_4730816952.gif" title="Generatively morphing digits"/></div>
<p>In this post, we’ll take a look under the hood at the math and technical details that allow us to optimize the VAE model we sketched in <a href="http://fastforwardlabs.github.io/2016/08/12/introducing-variational-autoencoders-in-prose-and.html">Part I</a>.</p>
<p>Along the way, we’ll show how to implement a VAE in <a href="http://tensorflow.org/">TensorFlow</a>—a library for efficient numerical computation using data flow graphs, with key features like <a href="http://alexey.radul.name/ideas/2013/introduction-to-automatic-differentiation/">automatic differentiation</a> and parallelizability (across clusters, CPUs, GPUs…and <a href="https://cloudplatform.googleblog.com/2016/05/Google-supercharges-machine-learning-tasks-with-custom-chip.html">TPUs</a> if you’re lucky). You can find (and tinker with!) the full implementation <a href="https://github.com/fastforwardlabs/vae-tf/tree/master">here</a>, along with a couple <a href="https://github.com/fastforwardlabs/vae-tf/tree/master/out">pre-trained models</a>.</p>
<h2 id="building-the-model">Building the Model</h2>
<p>Let’s dive into code (Python 3.4), starting with the necessary imports:</p>
```python
import functools
<p>from functional import compose, partial
import numpy as np
import tensorflow as tf</p>
<pre><code>
&lt;p&gt;One perk of these models is their modularity—VAEs are naturally amenable to swapping in whatever encoder/decoder architecture is most fitting for the task at hand: &lt;a href=&quot;https://arxiv.org/abs/1502.04623&quot;&gt;recurrent&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/1511.06349&quot;&gt;neural&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/1412.6581&quot;&gt;networks&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1411.5928&quot;&gt;convolutional&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1503.03167&quot;&gt;deconvolutional&lt;/a&gt; networks, etc.&lt;/p&gt;
&lt;p&gt;For our purposes, we will model the relatively simple &lt;a href=&quot;http://yann.lecun.com/exdb/mnist/&quot;&gt;MNIST&lt;/a&gt; dataset using densely-connected layers, wired symmetrically around the hidden code.&lt;/p&gt;

```python
class Dense():
    &quot;&quot;&quot;Fully-connected layer&quot;&quot;&quot;
    def __init__(self, scope=&quot;dense_layer&quot;, size=None, dropout=1.,
                 nonlinearity=tf.identity):
        # (str, int, (float | tf.Tensor), tf.op)
        assert size, &quot;Must specify layer size (num nodes)&quot;
        self.scope = scope
        self.size = size
        self.dropout = dropout # keep_prob
        self.nonlinearity = nonlinearity

    def __call__(self, x):
        &quot;&quot;&quot;Dense layer currying, to apply layer to any input tensor `x`&quot;&quot;&quot;
        # tf.Tensor -&amp;gt; tf.Tensor
        with tf.name_scope(self.scope):
            while True:
                try: # reuse weights if already initialized
                    return self.nonlinearity(tf.matmul(x, self.w) + self.b)
                except(AttributeError):
                    self.w, self.b = self.wbVars(x.get_shape()[1].value, self.size)
                    self.w = tf.nn.dropout(self.w, self.dropout)
    ...
</code></pre><p>We can initialize a <code>Dense</code> layer with our choice of <code>nonlinearity</code> for the layer nodes (i.e. neural network units that apply a nonlinear activation function to a linear combination of their inputs, as per line <code>18</code>).</p>
<p>We’ll use <a href="https://arxiv.org/abs/1511.07289">ELUs</a> (Exponential Linear Units), a <a href="http://www.picalike.com/blog/2015/11/28/relu-was-yesterday-tomorrow-comes-elu/">recent advance</a> in building nodes that learn quickly by avoiding the problem of vanishing gradients. We wrap up the class with a <a href="https://github.com/fastforwardlabs/vae-tf/blob/master/layers.py#L26-L38">helper function</a> (<code>Dense.wbVars</code>) for compatible random initialization of weights and biases, to further accelerate learning.</p>
<p>In TensorFlow, neural networks are defined as numerical computation graphs. We will build the graph using partial function composition of sequential layers, which is amenable to an arbitrary number of hidden layers.</p>
```python
def composeAll(*args):
    """Util for multiple function composition
<pre><code>i.e. composed = composeAll([f, g, h])
     composed(x) # == f(g(h(x)))
&quot;&quot;&quot;
# adapted from https://docs.python.org/3.1/howto/functional.html
return partial(functools.reduce, compose)(*args)
</code></pre>
<pre><code>
&lt;p&gt;Now that we’ve defined our model primitives, we can tackle the VAE itself.&lt;/p&gt;
&lt;p&gt;Keep in mind: the TensorFlow computational graph is cleanly divorced from the numerical computations themselves. In other words, a &lt;code&gt;tf.Graph&lt;/code&gt; wireframes the underlying skeleton of the model, upon which we may hang values only within the context of a &lt;code&gt;tf.Session&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Below, we initialize class &lt;code&gt;VAE&lt;/code&gt; and activate a session for future convenience (so we can initialize and evaluate tensors within a single session, e.g. to persist weights and biases across rounds of training).&lt;/p&gt;
&lt;p&gt;Here are some relevant snippets, cobbled together from the &lt;a href=&quot;https://github.com/fastforwardlabs/vae-tf/blob/master/vae.py&quot;&gt;full source code&lt;/a&gt;:&lt;/p&gt;

```python
class VAE():
    &quot;&quot;&quot;Variational Autoencoder

    see: Kingma &amp;amp; Welling - Auto-Encoding Variational Bayes
    (https://arxiv.org/abs/1312.6114)
    &quot;&quot;&quot;
    DEFAULTS = {
        &quot;batch_size&quot;: 128,
        &quot;learning_rate&quot;: 1E-3,
        &quot;dropout&quot;: 1., # keep_prob
        &quot;lambda_l2_reg&quot;: 0.,
        &quot;nonlinearity&quot;: tf.nn.elu,
        &quot;squashing&quot;: tf.nn.sigmoid
    }
    RESTORE_KEY = &quot;to_restore&quot;

    def __init__(self, architecture, d_hyperparams={}, meta_graph=None,
                 save_graph_def=True, log_dir=&quot;./log&quot;):
        &quot;&quot;&quot;(Re)build a symmetric VAE model with given:

         * architecture (list of nodes per encoder layer); e.g.
           [1000, 500, 250, 10] specifies a VAE with 1000-D inputs, 10-D latents,
           &amp;amp; end-to-end architecture [1000, 500, 250, 10, 250, 500, 1000]

         * hyperparameters (optional dictionary of updates to `DEFAULTS`)
        &quot;&quot;&quot;
        self.architecture = architecture
        self.__dict__.update(VAE.DEFAULTS, **d_hyperparams)
        self.sesh = tf.Session()

        if not meta_graph: # new model
            handles = self._buildGraph()
            ...
            self.sesh.run(tf.initialize_all_variables())
</code></pre><p>Assuming that we are building a model from scratch (rather than restoring a <a href="https://www.tensorflow.org/versions/r0.9/how_tos/meta_graph/index.html">saved</a> <code>meta_graph</code>), the key initialization step is the call to <code>VAE._buildGraph</code> (line <code>32</code>). This internal method constructs nodes representing the placeholders and operations through which the data will flow—<em>before</em> any data is actually piped in.</p>
<p>Finally, we unpack the iterable <code>handles</code> (populated by <code>_buildGraph</code>) into convenient class attributes—pointers not to numerical values, but rather to nodes in the graph:</p>
```python
        ...
        # unpack handles for tensor ops to feed or fetch
        (self.x_in, self.dropout_, self.z_mean, self.z_log_sigma,
         self.x_reconstructed, self.z_, self.x_reconstructed_,
         self.cost, self.global_step, self.train_op) = handles
```
<p>How are these nodes defined? The <code>_buildGraph</code> method encapsulates the core of the VAE model framework—starting with the encoder/inference network:</p>
```python
    def _buildGraph(self):
        x_in = tf.placeholder(tf.float32, shape=[None, # enables variable batch size
                                                 self.architecture[0]], name="x")
        dropout = tf.placeholder_with_default(1., shape=[], name="dropout")
<pre><code>    # encoding / &quot;recognition&quot;: q(z|x)
    encoding = [Dense(&quot;encoding&quot;, hidden_size, dropout, self.nonlinearity)
                # hidden layers reversed for function composition: outer -&amp;gt; inner
                for hidden_size in reversed(self.architecture[1:-1])]
    h_encoded = composeAll(encoding)(x_in)

    # latent distribution parameterized by hidden encoding
    # z ~ N(z_mean, np.exp(z_log_sigma)**2)
    z_mean = Dense(&quot;z_mean&quot;, self.architecture[-1], dropout)(h_encoded)
    z_log_sigma = Dense(&quot;z_log_sigma&quot;, self.architecture[-1], dropout)(h_encoded)
</code></pre>
<pre><code>
&lt;p&gt;Here, we build a pipe from &lt;code&gt;x_in&lt;/code&gt; (an empty placeholder for input data &lt;span class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt;), through the sequential hidden encoding, to the corresponding distribution over latent space—the variational approximate posterior, or hidden representation, &lt;span class=&quot;math inline&quot;&gt;\(z \sim q_\phi(z|x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As observed in lines &lt;code&gt;14&lt;/code&gt; - &lt;code&gt;15&lt;/code&gt;, latent &lt;span class=&quot;math inline&quot;&gt;\(z\)&lt;/span&gt; is distributed as a multivariate &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2465539/figure/fig1/&quot;&gt;normal&lt;/a&gt; with mean &lt;span class=&quot;math inline&quot;&gt;\(\mu\)&lt;/span&gt; and diagonal covariance values &lt;span class=&quot;math inline&quot;&gt;\(\sigma^2\)&lt;/span&gt; (the square of the “sigma” in &lt;code&gt;z_log_sigma&lt;/code&gt;) directly parameterized by the encoder: &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{N}(\mu, \sigma^2I)\)&lt;/span&gt;. In other words, we set out to “explain” highly complex observations as the consequence of an unobserved collection of simplified latent variables, i.e. independent Gaussians. (This is dictated by our choice of a conjugate spherical Gaussian prior over &lt;span class=&quot;math inline&quot;&gt;\(z\)&lt;/span&gt;—see &lt;a href=&quot;http://blog.fastforwardlabs.com/2016/08/12/introducing-variational-autoencoders-in-prose-and.html&quot;&gt;Part I&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;Next, we sample from this latent distribution (in practice, &lt;a href=&quot;https://arxiv.org/abs/1312.6114&quot;&gt;one draw is enough&lt;/a&gt; given sufficient minibatch size, i.e. &amp;gt;100). This method involves a trick—can you figure out why?—that we will explore in more detail later.&lt;/p&gt;
```python
        z = self.sampleGaussian(z_mean, z_log_sigma)
</code></pre><p>The sampled <span class="math inline">\(z\)</span> is then passed to the decoder/generative network, which symmetrically builds back out to generate the conditional distribution over input space, reconstruction <span class="math inline">\(\tilde{x} \sim p_\theta(x|z)\)</span>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">        <span style="color:#75715e"># decoding / &#34;generative&#34;: p(x|z)</span>
        decoding <span style="color:#f92672">=</span> [Dense(<span style="color:#e6db74">&#34;decoding&#34;</span>, hidden_size, dropout, self<span style="color:#f92672">.</span>nonlinearity)
                    <span style="color:#66d9ef">for</span> hidden_size <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>architecture[<span style="color:#ae81ff">1</span>:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]] <span style="color:#75715e"># assumes symmetry</span>
        <span style="color:#75715e"># final reconstruction: restore original dims, squash outputs [0, 1]</span>
        decoding<span style="color:#f92672">.</span>insert(<span style="color:#ae81ff">0</span>, Dense( <span style="color:#75715e"># prepend as outermost function</span>
            <span style="color:#e6db74">&#34;reconstruction&#34;</span>, self<span style="color:#f92672">.</span>architecture[<span style="color:#ae81ff">0</span>], dropout, self<span style="color:#f92672">.</span>squashing))
        x_reconstructed <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>identity(composeAll(decoding)(z), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;x_reconstructed&#34;</span>)
</code></pre></div><p>Alternately, we add a placeholder to directly feed arbitrary values of <span class="math inline">\(z\)</span> to the generative network (to fabricate realistic outputs—no input data necessary!):</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">        <span style="color:#75715e"># ops to directly explore latent space</span>
        <span style="color:#75715e"># defaults to prior z ~ N(0, I)</span>
        z_ <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>placeholder_with_default(tf<span style="color:#f92672">.</span>random_normal([<span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>architecture[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]]),
                                         shape<span style="color:#f92672">=</span>[None, self<span style="color:#f92672">.</span>architecture[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]],
                                         name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;latent_in&#34;</span>)
        x_reconstructed_ <span style="color:#f92672">=</span> composeAll(decoding)(z_)
</code></pre></div><p>TensorFlow automatically flows data through the appropriate subgraph, based on the nodes that we fetch and feed with the <code>tf.Session.run</code> method. Defining the <a href="https://github.com/fastforwardlabs/vae-tf/blob/master/vae.py#L190-L196">encoder</a>, <a href="https://github.com/fastforwardlabs/vae-tf/blob/master/vae.py#L198-L209">decoder</a>, and <a href="https://github.com/fastforwardlabs/vae-tf/blob/master/vae.py#L211-L214">end-to-end VAE</a> is then trivial (see linked code).</p>
<p>We’ll finish the <code>VAE._buildGraph</code> method later in the post, as we walk through the nuances of the model.</p>
<h2 id="the-reparameterization-trick">The Reparameterization Trick</h2>
<p>In order to estimate the latent representation <span class="math inline">\(z\)</span> for a given observation <span class="math inline">\(x\)</span>, we want to sample from the approximate posterior <span class="math inline">\(q_\phi(z|x)\)</span> according to the distribution defined by the encoder.</p>
<p>However, model training by <a href="http://mathworld.wolfram.com/MethodofSteepestDescent.html">gradient descent</a> requires that our model be differentiable with respect to its learned parameters (which is how we propagate the gradients). This presupposes that the model is deterministic—i.e. a given input always returns the same output for a fixed set of parameters, so the only source of stochasticity are the inputs. Incorporating a probabilistic “sampling” node would make the model itself stochastic!</p>
<p>Instead, we inject randomness into the model by introducing input from an auxiliary random variable: <span class="math inline">\(\epsilon \sim p(\epsilon)\)</span>.</p>
<p>For our purposes, rather than sampling <span class="math inline">\(z\)</span> directly from <span class="math inline">\(q_\phi(z|x) \sim \mathcal{N}(\mu, \sigma^2I)\)</span>, we generate Gaussian noise <span class="math inline">\(\epsilon \sim \mathcal{N}(0, I)\)</span> and compute <span class="math display">\[z = \mu + \sigma \odot \epsilon\]</span> (where <span class="math inline">\(\odot\)</span> is the element-wise product). In code:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sampleGaussian</span>(self, mu, log_sigma):
        <span style="color:#e6db74">&#34;&#34;&#34;Draw sample from Gaussian with given shape, subject to random noise epsilon&#34;&#34;&#34;</span>
        <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>name_scope(<span style="color:#e6db74">&#34;sample_gaussian&#34;</span>):
            <span style="color:#75715e"># reparameterization trick</span>
            epsilon <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>random_normal(tf<span style="color:#f92672">.</span>shape(log_sigma), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;epsilon&#34;</span>)
            <span style="color:#66d9ef">return</span> mu <span style="color:#f92672">+</span> epsilon <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>exp(log_sigma) <span style="color:#75715e"># N(mu, sigma**2)</span>
</code></pre></div><p>By “reparameterizing” this step, inference and generation become entirely differentiable and hence, learnable.</p>
<h2 id="cost-function">Cost Function</h2>
<p>Now, in order to optimize the model, we need a metric for how well its parameters capture the true data-generating and latent distributions. That is, how likely is observation <span class="math inline">\(x\)</span> under the joint distribution <span class="math inline">\(p(x, z)\)</span>?</p>
<p>Recall that we represent the global encoder and decoder parameters (i.e. neural network weights and biases) as <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\theta\)</span>, respectively.</p>
<p>In other words, we want to simultaneously tune these complementary parameters such that we maximize <span class="math inline">\(log(p(x|\phi, \theta))\)</span>—the log-likelihood across all datapoints <span class="math inline">\(x\)</span> under the current model settings, after marginalizing out the latent variables <span class="math inline">\(z\)</span>. This term is also known as the model <em>evidence</em>.</p>
<p>We can express this marginal likelihood as the sum of what we’ll call the <em>variational</em> or <em>evidence lower bound</em> <span class="math inline">\(\mathcal{L}\)</span> and the <em>Kullback-Leibler (KL) divergence</em> <span class="math inline">\(\mathcal{D}_{KL}\)</span> between the approximate and true latent posteriors: <span class="math display">\[ log(p(x)) = \mathcal{L}(\phi, \theta; x) + \mathcal{D}_{KL}(q_\phi(z|x) || p_\theta(z|x)) \]</span></p>
<p>Here, the <!-- The --> KL divergence can be (<a href="http://mathworld.wolfram.com/RelativeEntropy.html">fuzzily</a>!) intuited as a metric for the misfit of the approximate posterior <span class="math inline">\(q_\phi\)</span>. We’ll delve into this further in a moment, but for now the important thing is that it is non-negative by definition; consequently, the first term acts as a <em>lower bound</em> on the total. So, we maximize the lower bound <span class="math inline">\(\mathcal{L}\)</span> as a (computationally-tractable) proxy for the total marginal likelihood of the data under the model. (And the better our approximate posterior, the tighter the gap between the lower bound and the total model evidence.)</p>
<p>With some <a href="https://arxiv.org/abs/1312.6114">mathematical wrangling</a>, we can decompose <span class="math inline">\(\mathcal{L}\)</span> into the following objective function: <span class="math display">\[ \mathcal{L}(\phi, \theta; x) = \mathbb{E}_{z \sim q_\phi(z|x)}[log(p_\theta(x|z))] - \mathcal{D}_{KL}(q_\phi(z|x) || p_\theta(z)) \]</span> (Phrased as a cost, we optimize the model by minimizing <span class="math inline">\({-\mathcal{L}}\)</span>.)</p>
<p>Here, the perhaps unfriendly-looking first term is, in fact, familiar! It’s the probability density of generated output <span class="math inline">\(\tilde{x}\)</span> given the inferred latent distribution over <span class="math inline">\(z\)</span>—i.e. the (negative) expected <em>reconstruction error</em>. This loss term is intrinsic to perhaps every autoencoder: how accurately does the output replicate the input?</p>
<p>Choosing an appropriate metric for image resemblance is hard (but that’s another <a href="https://arxiv.org/abs/1512.09300">story</a>). We’ll use the binary <a href="http://neuralnetworksanddeeplearning.com/chap3.html#the_cross-entropy_cost_function">cross-entropy</a>, which is commonly used for data like MNIST that can be modeled as <a href="http://mathworld.wolfram.com/BernoulliDistribution.html">Bernoulli trials</a>. Expressed as a static method of the <code>VAE</code> class:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#a6e22e">@staticmethod</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">crossEntropy</span>(obs, actual, offset<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-7</span>):
        <span style="color:#e6db74">&#34;&#34;&#34;Binary cross-entropy, per training example&#34;&#34;&#34;</span>
        <span style="color:#75715e"># (tf.Tensor, tf.Tensor, float) -&amp;gt; tf.Tensor</span>
        <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>name_scope(<span style="color:#e6db74">&#34;cross_entropy&#34;</span>):
            <span style="color:#75715e"># bound by clipping to avoid nan</span>
            obs_ <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>clip_by_value(obs, offset, <span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> offset)
            <span style="color:#66d9ef">return</span> <span style="color:#f92672">-</span>tf<span style="color:#f92672">.</span>reduce_sum(actual <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>log(obs_) <span style="color:#f92672">+</span>
                                  (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> actual) <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> obs_), <span style="color:#ae81ff">1</span>)
</code></pre></div><p>The second term in the objective is the KL divergence of the prior <span class="math inline">\(p\)</span> from the (approximate) posterior <span class="math inline">\(q\)</span> over the latent space. We’ll approach this conceptually, then mathematically.</p>
<p>The KL divergence <span class="math inline">\(\mathcal{D}_{KL}(q||p)\)</span> is defined as the relative entropy between probability density functions <span class="math inline">\(q\)</span> and <span class="math inline">\(p\)</span>. In information theory, entropy represents information content (measured in nats), so <span class="math inline">\(\mathcal{D}_{KL}\)</span> quantifies the information gained by revising the candidate prior <span class="math inline">\(p\)</span> to match some “ground truth” <span class="math inline">\(q\)</span>.</p>
<p>In a related vein, the KL divergence between posterior and prior beliefs (i.e. distributions) can be conceived as a measure of “<a href="http://ilab.usc.edu/surprise">surprise</a>”: the extent to which the model must update its “worldview” (parameters) to accomodate new observations.</p>
<p>(Note that the formula is asymmetric—i.e. <span class="math inline">\(\mathcal{D}_{KL}(q||p) \neq \mathcal{D}_{KL}(p||q)\)</span>—with <a href="http://www.inference.vc/how-to-train-your-generative-models-why-generative-adversarial-networks-work-so-well-2/">implications</a> for its use in generative models. This is also why it is not a true metric.)</p>
<p>By inducing the learned approximation <span class="math inline">\(q_\phi(z|x)\)</span> (the encoder) to match the continuous imposed prior <span class="math inline">\(p(z)\)</span>, the KL term encourages robustness to small perturbations along the latent manifold, enabling smooth interpolation within and between classes (e.g. MNIST digits). This reduces “spottiness” in the latent space that is often observed in autoencoders without such regularization.</p>
<p>Mathematical bonus: we can strategically choose certain conjugate priors over <span class="math inline">\(z\)</span> that let us analytically integrate the KL divergence, yielding a closed-form equation. This is true of the spherical Gaussian we chose, such that <span class="math display">\[ {-\mathcal{D}}_{KL}(q_\phi(z|x) || p_\theta(z)) = \frac{1} 2 \sum{(1 + log(\sigma^2) - \mu^2 - \sigma^2)} \]</span> (summed over the latent dimensions). In TensorFlow, that looks like this:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#a6e22e">@staticmethod</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">kullbackLeibler</span>(mu, log_sigma):
        <span style="color:#e6db74">&#34;&#34;&#34;(Gaussian) Kullback-Leibler divergence KL(q||p), per training example&#34;&#34;&#34;</span>
        <span style="color:#75715e"># (tf.Tensor, tf.Tensor) -&amp;gt; tf.Tensor</span>
        <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>name_scope(<span style="color:#e6db74">&#34;KL_divergence&#34;</span>):
            <span style="color:#75715e"># = -0.5 * (1 + log(sigma**2) - mu**2 - sigma**2)</span>
            <span style="color:#66d9ef">return</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">0.5</span> <span style="color:#f92672">*</span> tf<span style="color:#f92672">.</span>reduce_sum(<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> log_sigma <span style="color:#f92672">-</span> mu<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span> <span style="color:#f92672">-</span>
                                        tf<span style="color:#f92672">.</span>exp(<span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> log_sigma), <span style="color:#ae81ff">1</span>)
</code></pre></div><p>Together, these complementary loss terms capture the trade-off between expressivity and concision, between data complexity and simplicity of the prior. Reconstruction loss pushes the model toward perfectionist tendencies, while KL loss (along with the addition of auxiliary noise) encourages it to explore sensibly.</p>
<p>To elaborate (building on the <code>VAE._buildGraph</code> method started above):</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">        <span style="color:#75715e"># reconstruction loss: mismatch b/w x &amp;amp; x_reconstructed</span>
        <span style="color:#75715e"># binary cross-entropy -- assumes p(x) &amp;amp; p(x|z) are iid Bernoullis</span>
        rec_loss <span style="color:#f92672">=</span> VAE<span style="color:#f92672">.</span>crossEntropy(x_reconstructed, x_in)

        <span style="color:#75715e"># Kullback-Leibler divergence: mismatch b/w approximate posterior &amp;amp; imposed prior</span>
        <span style="color:#75715e"># KL[q(z|x) || p(z)]</span>
        kl_loss <span style="color:#f92672">=</span> VAE<span style="color:#f92672">.</span>kullbackLeibler(z_mean, z_log_sigma)

        <span style="color:#75715e"># average over minibatch</span>
        cost <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(rec_loss <span style="color:#f92672">+</span> kl_loss, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;cost&#34;</span>)
</code></pre></div><p>Beyond its concise elegance <!-- elegant concision  -->and solid grounding in Bayesian theory, the cost function lends itself well to intuitive metaphor:</p>
<p>Information theory-wise, the VAE is a terse game of Telephone, with the aim of finding the <em>minimum description length</em> to convey the input from end to end. Here, reconstruction loss is the information “lost in translation,” while KL loss captures how overly “wordy” the model must be to convey the message through an unpredictable medium (hidden code imperfectly optimized for the input data).</p>
<p>Or, framing the VAE as a lossy compression algorithm, reconstruction loss accounts for the fidelity of (de)compression while KL loss penalizes the model for using a sub-optimal compression scheme.</p>
<h2 id="training">Training</h2>
<p>At last, our VAE cost function in hand (after factoring in optional <a href="http://cs231n.github.io/neural-networks-2/#regularization"><span class="math inline">\(\ell_2\)</span>-regularization</a>), we finish <code>VAE._buildGraph</code> with optimization nodes to be evaluated at each step of SGD (with the <a href="https://arxiv.org/abs/1412.6980">Adam</a> optimizer)…</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">        <span style="color:#75715e"># optimization</span>
        global_step <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(<span style="color:#ae81ff">0</span>, trainable<span style="color:#f92672">=</span>False)
        <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>name_scope(<span style="color:#e6db74">&#34;Adam_optimizer&#34;</span>):
            optimizer <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>train<span style="color:#f92672">.</span>AdamOptimizer(self<span style="color:#f92672">.</span>learning_rate)
            tvars <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>trainable_variables()
            grads_and_vars <span style="color:#f92672">=</span> optimizer<span style="color:#f92672">.</span>compute_gradients(cost, tvars)
            clipped <span style="color:#f92672">=</span> [(tf<span style="color:#f92672">.</span>clip_by_value(grad, <span style="color:#f92672">-</span><span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>), tvar) <span style="color:#75715e"># gradient clipping</span>
                    <span style="color:#66d9ef">for</span> grad, tvar <span style="color:#f92672">in</span> grads_and_vars]
            train_op <span style="color:#f92672">=</span> optimizer<span style="color:#f92672">.</span>apply_gradients(clipped, global_step<span style="color:#f92672">=</span>global_step,
                                                 name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;minimize_cost&#34;</span>) <span style="color:#75715e"># back-prop</span>
</code></pre></div><p>…and return all of the nodes we want to access in the future to the <code>VAE.__init__</code> method where <code>buildGraph</code> was called.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">        <span style="color:#66d9ef">return</span> (x_in, dropout, z_mean, z_log_sigma, x_reconstructed,
                z_, x_reconstructed_, cost, global_step, train_op)
</code></pre></div><p>Using SGD to optimize the function parameters of the inference and generative networks simultaneously is called <em>Stochastic Gradient Variational Bayes</em>.</p>
<p>This is where TensorFlow really shines: all of the gradient backpropagation and parameter updates are performed via automatic differentation, and abstracted away from the researcher in the <code>train_op</code> (essentially) one-liner on line <code>48</code>.</p>
<p>Model training (with optional cross-validation) is then as simple as feeding minibatches from dataset <code>X</code> to the <code>x_in</code> placeholder and evaluating (“fetching”) the <code>train_op</code>. Here are some relevant chunks, excerpted from the <a href="https://github.com/fastforwardlabs/vae-tf/blob/master/vae.py#L216-L295">full class method</a>:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train</span>(self, X, max_iter<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>inf, max_epochs<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>inf, cross_validate<span style="color:#f92672">=</span>True,
              verbose<span style="color:#f92672">=</span>True, save<span style="color:#f92672">=</span>False, outdir<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;./out&#34;</span>, plots_outdir<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;./png&#34;</span>):
        <span style="color:#66d9ef">try</span>:
            err_train <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
            now <span style="color:#f92672">=</span> datetime<span style="color:#f92672">.</span>now()<span style="color:#f92672">.</span>isoformat()[<span style="color:#ae81ff">11</span>:]
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;------- Training begin: {} -------</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(now))

            <span style="color:#66d9ef">while</span> True:
                x, _ <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>train<span style="color:#f92672">.</span>next_batch(self<span style="color:#f92672">.</span>batch_size)
                feed_dict <span style="color:#f92672">=</span> {self<span style="color:#f92672">.</span>x_in: x, self<span style="color:#f92672">.</span>dropout_: self<span style="color:#f92672">.</span>dropout}
                fetches <span style="color:#f92672">=</span> [self<span style="color:#f92672">.</span>x_reconstructed, self<span style="color:#f92672">.</span>cost, self<span style="color:#f92672">.</span>global_step, self<span style="color:#f92672">.</span>train_op]
                x_reconstructed, cost, i, _ <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>sesh<span style="color:#f92672">.</span>run(fetches, feed_dict)

                err_train <span style="color:#f92672">+=</span> cost

                <span style="color:#66d9ef">if</span> i<span style="color:#f92672">%</span><span style="color:#ae81ff">1000</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">and</span> verbose:
                    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;round {} --&amp;gt; avg cost: &#34;</span><span style="color:#f92672">.</span>format(i), err_train <span style="color:#f92672">/</span> i)

                <span style="color:#66d9ef">if</span> i <span style="color:#f92672">&amp;</span>gt;<span style="color:#f92672">=</span> max_iter <span style="color:#f92672">or</span> X<span style="color:#f92672">.</span>train<span style="color:#f92672">.</span>epochs_completed <span style="color:#f92672">&amp;</span>gt;<span style="color:#f92672">=</span> max_epochs:
                    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;final avg cost (@ step {} = epoch {}): {}&#34;</span><span style="color:#f92672">.</span>format(
                        i, X<span style="color:#f92672">.</span>train<span style="color:#f92672">.</span>epochs_completed, err_train <span style="color:#f92672">/</span> i))
                    now <span style="color:#f92672">=</span> datetime<span style="color:#f92672">.</span>now()<span style="color:#f92672">.</span>isoformat()[<span style="color:#ae81ff">11</span>:]
                    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;------- Training end: {} -------</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(now))
                    <span style="color:#66d9ef">break</span>
</code></pre></div><p>Helpfully, TensorFlow comes with a built-in <a href="https://www.tensorflow.org/versions/r0.10/how_tos/summaries_and_tensorboard/index.html">visualization dashboard</a>. Here’s the computational graph for an end-to-end VAE with two hidden encoder/decoder layers (that’s what all the <code>tf.name_scope</code>-ing was for):</p>
<div class="figure">
<img src="http://fastforwardlabs.github.io/blog-images/miriam/imgs_code/tensorboard.png" title="TensorBoard" style="max-width: 398px !important"/><h2 id="wrapping-up">Wrapping Up
</h2><p>The future of deep latent models lies in models that can reason about the world—“understanding” complex observations, transforming them into meaningful internal representations, and even leveraging these representations to make decisions—all while coping with scarce data, and in semisupervised or unsupervised settings. VAEs are an important step toward this future, demonstrating the power of new ways of thinking that result from unifying variational Bayesian methods and deep learning.</p>
<p>We now understand how these fields come together to make the VAE possible, through a theoretically-sound objective function that balances accuracy (reconstruction loss) with variational regularization (KL loss), and efficient optimization of the fully differentiable model thanks to the reparameterization trick.</p>
<p>We’ll wrap up for now with one more way of visualizing the condensed information encapsulated in VAE latent space.</p>
<p><a href="http://blog.fastforwardlabs.com/2016/08/12/introducing-variational-autoencoders-in-prose-and.html">Previously</a>, we showed the correspondence between the inference and generative networks by plotting the encoder and decoder perspectives of the latent space in the same 2-D coordinate system. For the decoder perspective, this meant feeding linearly spaced latent coordinates to the generative network and plotting their corresponding outputs.</p>
<p>To get an undistorted sense of the full latent manifold, we can sample and decode latent space coordinates proportionally to the model’s distribution over latent space. In other words—thanks to variational regularization provided by the KL loss!—we simply sample relative to our chosen prior distribution over <span class="math inline">\(z\)</span>. In our case, this means sampling linearly spaced percentiles from the <a href="http://work.thaslwanter.at/Stats/html/statsDistributions.html#other-important-presentations-of-probability-densities">inverse CDF</a> of a spherical Gaussian.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p>Once again, evolving over (logarithmic) time:</p>
<div class="figure">
<img src="http://fastforwardlabs.github.io/blog-images/miriam/imgs_code/160816_1813_latent_784_500_500_2_explore_ppf30_16.0.500x500.slower47.gif" title="Decoder's undistorted view of latent space, over training"/></div>
<p>Interestingly, we can see that the slim tails of the distribution (edges of the frame) are not well-formed. Presumably, this results from few observed inputs being mapped to latent posteriors with significant density in these regions.</p>
<p>Here are a few resulting constellations (from a single model):</p>
<div class="figure">
<img src="http://fastforwardlabs.github.io/blog-images/miriam/imgs_code/160805_1646_reloaded_latent_784_500_500_2_round_131072_explore_ppf_30.png" title="Generative view of latent space: 30x30"/></div>
<div class="figure">
<img src="http://fastforwardlabs.github.io/blog-images/miriam/imgs_code/160805_1646_reloaded_latent_784_500_500_2_round_131072_explore_ppf_60.png" title="Generative view of latent space: 60x60"/></div>
<div class="figure">
<img src="http://fastforwardlabs.github.io/blog-images/miriam/imgs_code/160805_1646_reloaded_latent_784_500_500_2_round_131072_explore_ppf_100.png" title="Generative view of latent space: 100x100"/></div>
<p>Theoretically, we could subdivide the latent space into infinitely many points (limited in practice only by the computer’s floating point precision), and let the generative network dream up infinite constellations of creative variations on MNIST.</p>
<p>That’s enough digits for now! Keep your eyes out for the next installment, where we’ll tinker with the vanilla VAE model in the context of a new dataset.</p>
<p>– Miriam</p>
<div class="footnotes">
<hr><ol><li id="fn1"><p>Thanks Kyle McDonald (<span class="citation">@kcimc</span>) and Tom White (<span class="citation">@dribnet</span>) for noting this!<a href="#fnref1">↩</a></p></li>
</ol></div> </div>
    <div class="spacer"></div>
    <div>
      <div style="width: 100%; height: 2px; background: #ccc; margin-top: -1px; margin-bottom: -1px;"></div>
    </div>
    <div class="spacer"></div>
  </div>
</div>


<div class="container">
  <div class="spacer"></div>
  <h2 class="clear">Read more</h2>
  <div class="spacer"></div>
  <div style="display: grid; grid-template-columns: 1fr 1fr; grid-column-gap: 2ch;">
    <div>
      
      <div class="small">Newer</div>
      <div>
  <h5 style="margin-bottom: 0px;">
    <span>Aug 24, 2016</span> &middot;
    <span style="text-transform: capitalize;">
      interview
    </span>
  </h5>
  
    <div><a href="/2016/08/24/next-economics-interview-with-jimi-crawford.html"><strong>Next Economics: Interview with Jimi Crawford</strong></a></div>
  
  <div class="spacer"></div>
</div>


      
    </div>
    <div>
      
      <div class="small">Older</div>
      <div>
  <h5 style="margin-bottom: 0px;">
    <span>Aug 18, 2016</span> &middot;
    <span style="text-transform: capitalize;">
      guest post
    </span>
  </h5>
  
    <div><a href="/2016/08/18/giving-speech-a-voice-in-the-home.html"><strong>Giving Speech a Voice in the Home</strong></a></div>
  
  <div class="spacer"></div>
</div>


      
    </div>
  </div>
</div>

<div class="container">
<div class="spacer"></div>
<div>
  <div style="width: 100%; height: 2px; background: #ccc; margin-top: -1px; margin-bottom: -1px;"></div>
</div>
<div class="spacer"></div>
<div class="spacer"></div>
</div>

<div class="container">
  

<h2 class="clear">Latest posts</h2>
<div class="spacer"></div>

<div id="posts-holder"> 
  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Dec 14, 2021</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
  <a href="/2021/12/14/an-introduction-to-video-understanding-capabilities-and-applications.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/video_classification-1639064585.png" />
  </a>
  
  <div>
    
    <a href="/2021/12/14/an-introduction-to-video-understanding-capabilities-and-applications.html"
       ><h2 style="margin-bottom: 4px;">An Introduction to Video Understanding: Capabilities and Applications</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="https://uk.linkedin.com/in/daniel-valdez-balderas-9051323b">Daniel Valdez Balderas</a>
        &middot; </span
      >
    </span>
    Video footage constitutes a significant portion of all data in the world. The 30 thousand hours of video uploaded to Youtube every hour is a part of that data; another portion is produced by 770 million surveillance cameras globally. In addition to being plentiful, video data has tremendous capacity to store useful information. Its vastness, richness, and applicability make the understanding of video a key activity within the field of computer vision.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/2021/12/14/an-introduction-to-video-understanding-capabilities-and-applications.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Sep 22, 2021</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
  <a href="/2021/09/22/automatic-summarization-from-textrank-to-transformers.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/summarize_blog/summarize_crop.png" />
  </a>
  
  <div>
    
    <a href="/2021/09/22/automatic-summarization-from-textrank-to-transformers.html"
       ><h2 style="margin-bottom: 4px;">Automatic Summarization from TextRank to Transformers</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="https://www.linkedin.com/in/melanierbeck/">Melanie Beck</a>
        &middot; </span
      >
    </span>
    Automatic summarization is a task in which a machine distills a large amount of data into a subset (the summary) that retains the most relevant and important information from the whole. While traditionally applied to text, automatic summarization can include other formats such as images or audio. In this article we’ll cover the main approaches to automatic text summarization, talk about what makes for a good summary, and introduce Summarize. &ndash; a summarization prototype we built that showcases several automatic summarization techniques.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/2021/09/22/automatic-summarization-from-textrank-to-transformers.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Sep 21, 2021</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
  <a href="/2021/09/21/extractive-summarization-with-sentence-bert.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/extractabert_blog/extractivesummodel.png" />
  </a>
  
  <div>
    
    <a href="/2021/09/21/extractive-summarization-with-sentence-bert.html"
       ><h2 style="margin-bottom: 4px;">Extractive Summarization with Sentence-BERT</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="https://twitter.com/vykthur">Victor Dibia</a>
        &middot; </span
      >
    </span>
    In extractive summarization, the task is to identify a subset of text (e.g., sentences) from a document that can then be assembled into a summary. Overall, we can treat extractive summarization as a recommendation problem. That is, given a query, recommend a set of sentences that are relevant. The query here is the document, relevance is a measure of whether a given sentence belongs in the document summary.
How we go about obtaining this measure of relevance varies (a common dilemma for any recommendation system).
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/2021/09/21/extractive-summarization-with-sentence-bert.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Sep 20, 2021</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
  <a href="/2021/09/20/how-and-when-to-enable-early-stopping-for-gensims-word2vec.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/gensim_blog/earlystopping_schematic.png" />
  </a>
  
  <div>
    
    <a href="/2021/09/20/how-and-when-to-enable-early-stopping-for-gensims-word2vec.html"
       ><h2 style="margin-bottom: 4px;">How (and when) to enable early stopping for Gensim&#39;s Word2Vec</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    
    <span>
      by 
      <span
        ><a href="https://www.linkedin.com/in/melanierbeck/">Melanie Beck</a>
        &middot; </span
      >
    </span>
    The Gensim library is a staple of the NLP stack. While it primarily focuses on topic modeling and similarity for documents, it also supports several word embedding algorithms, including what is likely the best-known implementation of Word2Vec.
Word embedding models like Word2Vec use unlabeled data to learn vector representations for each token in a corpus. These embeddings can then be used as features in myriad downstream tasks such as classification, clustering, or recommendation systems.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/2021/09/20/how-and-when-to-enable-early-stopping-for-gensims-word2vec.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Jul 7, 2021</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
  <a href="/2021/07/07/exploring-multi-objective-hyperparameter-optimization.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/Single-objective-surrogate-function-1625741671.png" />
  </a>
  
  <div>
    
    <a href="/2021/07/07/exploring-multi-objective-hyperparameter-optimization.html"
       ><h2 style="margin-bottom: 4px;">Exploring Multi-Objective Hyperparameter Optimization</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    By Chris and Melanie.
The machine learning life cycle is more than data + model = API. We know there is a wealth of subtlety and finesse involved in data cleaning and feature engineering. In the same vein, there is more to model-building than feeding data in and reading off a prediction. ML model building requires thoughtfulness both in terms of which metric to optimize for a given problem, and how best to optimize your model for that metric!
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/2021/07/07/exploring-multi-objective-hyperparameter-optimization.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="post-link" style="position: relative">
  <h5 style="margin-bottom: 4px;">
    <span>Jun 9, 2021</span> &middot;
    <span style="text-transform: capitalize;">
      
    </span>
  </h5>
  
  <a href="/2021/06/09/deep-metric-learning-for-signature-verification.html" class="preview-image-holder">
    <img class="preview-image" src="/images/hugo/metricblog/onlinetraining.png" />
  </a>
  
  <div>
    
    <a href="/2021/06/09/deep-metric-learning-for-signature-verification.html"
       ><h2 style="margin-bottom: 4px;">Deep Metric Learning for Signature Verification</h2></a
     >
     
  </div>
  <div class="small" style="height: 4.5em; overflow: hidden;">
    By Victor and Andrew.
TLDR; This post provides an overview of metric learning loss functions (constrastive, triplet, quadruplet, and group loss), and results from applying contrastive and triplet loss to the task of signature verification. A complete list of the posts in this series is outlined below: Pretrained Models as Baselines for Signature Verification  --   Part 1: Deep Learning for Automatic Offline Signature Verification: An Introduction    Part 2: Pretrained Models as Baselines for Signature Verification    Part 3: Deep Metric Learning for Signature Verification      In our previous blog post, we discussed how pretrained models can serve as strong baselines for the task of signature verification.
  </div>
  <div
    style="width:100%;white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"
    >
    
      <a href="/2021/06/09/deep-metric-learning-for-signature-verification.html">...read more</a>
    
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
</div>

<div>
  <button id="load_more" style="width: 100%;">load more</button>
</div>
<div class="spacer"></div>
<div class="spacer"></div>

<script>
  window.addEventListener('load', () => {
    let $posts_holder = document.getElementById('posts-holder')
    let $load_more = document.getElementById('load_more')
    let next_page = 2
    $load_more.addEventListener('click', () => {
      fetch(`/posts/page/${next_page}.html`).then(r =>r.text()).then(r => {
        let el = document.createElement('html')
        el.innerHTML = r
        next_page += 1
        let $posts = el.querySelector('#posts-holder').children
        for (let i=0; i< $posts.length; i++) {
          let $post = $posts[i].cloneNode(true)
          $posts_holder.appendChild($post)
        }
      })
    })
  })
</script>


  <h3 class="clear">Popular posts</h3>
<div class="spacer"></div>
<div>
  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Oct 30, 2019</span> &middot;
    <span style="text-transform: capitalize;">
      newsletter
    </span>
  </h5>
  
    <div><a href="/2019/10/30/exciting-applications-of-graph-neural-networks.html"><strong>Exciting Applications of Graph Neural Networks</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Nov 14, 2018</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2018/11/14/federated-learning-distributed-machine-learning-with-data-locality-and-privacy.html"><strong>Federated learning: distributed machine learning with data locality and privacy</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Apr 10, 2018</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2018/04/10/pytorch-for-recommenders-101.html"><strong>PyTorch for Recommenders 101</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Oct 4, 2017</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2017/10/04/first-look-using-three.js-for-2d-data-visualization.html"><strong>First Look: Using Three.js for 2D Data Visualization</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Aug 22, 2016</span> &middot;
    <span style="text-transform: capitalize;">
      whitepaper
    </span>
  </h5>
  
    <div><a href="/2016/08/22/under-the-hood-of-the-variational-autoencoder-in-prose-and-code.html"><strong>Under the Hood of the Variational Autoencoder (in Prose and Code)</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
    <div>
  <h5 style="margin-bottom: 0px;">
    <span>Feb 24, 2016</span> &middot;
    <span style="text-transform: capitalize;">
      post
    </span>
  </h5>
  
    <div><a href="/2016/02/24/hello-world-in-keras-or-scikit-learn-versus-keras.html"><strong>&#34;Hello world&#34; in Keras (or, Scikit-learn versus Keras)</strong></a></div>
  
  <div class="spacer"></div>
</div>


  
</div>

</div>

<div class="spacer"></div>
<div style="background: #efefef;">
  <div class="spacer"></div>
  <div class="spacer"></div>
  <div class="container">
  <h1 class="clear">Reports</h1>
  <div style="color: #444;">In-depth guides to specific machine learning capabilities</div>
</div>
<div class="spacer"></div>
<div style="max-width: 96ch; margin: 0 auto; padding-left: 1ch; padding-right: 1ch;">
  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF22</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://concept-drift.fastforwardlabs.com/" target="_blank">Inferring Concept Drift Without Labeled Data</a></h2>
  <a class="report-image" href="https://concept-drift.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff22-combo.png" />
  </a>
  <div class="small">Concept drift occurs when the statistical properties of a target domain change overtime causing model performance to degrade. Drift detection is generally achieved by monitoring a performance metric of interest and triggering a retraining pipeline when that metric falls below some designated threshold. However, this approach assumes ample labeled data is available at prediction time - an unrealistic constraint for many production systems. In this report, we explore various approaches for dealing with concept drift when labeled data is not readily accessible.</div>
  <div><a href="https://concept-drift.fastforwardlabs.com/" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF19</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://session-based-recommenders.fastforwardlabs.com/" target="_blank">Session-based Recommender Systems</a></h2>
  <a class="report-image" href="https://session-based-recommenders.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff19-combo.png" />
  </a>
  <div class="small">Being able to recommend an item of interest to a user (based on their past preferences) is a highly relevant problem in practice. A key trend over the past few years has been session-based recommendation algorithms that provide recommendations solely based on a user’s interactions in an ongoing session, and which do not require the existence of user profiles or their entire historical preferences. This report explores a simple, yet powerful, NLP-based approach (word2vec) to recommend a next item to a user. While NLP-based approaches are generally employed for linguistic tasks, here we exploit them to learn the structure induced by a user’s behavior or an item’s nature.</div>
  <div><a href="https://session-based-recommenders.fastforwardlabs.com/" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF18</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://few-shot-text-classification.fastforwardlabs.com/" target="_blank">Few-Shot Text Classification</a></h2>
  <a class="report-image" href="https://few-shot-text-classification.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff18-combo.png" />
  </a>
  <div class="small">Text classification can be used for sentiment analysis, topic assignment, document identification, article recommendation, and more. While dozens of techniques now exist for this fundamental task, many of them require massive amounts of labeled data in order to be useful. Collecting annotations for your use case is typically one of the most costly parts of any machine learning application. In this report, we explore how latent text embeddings can be used with few (or even zero) training examples and provide insights into best practices for implementing this method.</div>
  <div><a href="https://few-shot-text-classification.fastforwardlabs.com/" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF16</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://structural-time-series.fastforwardlabs.com" target="_blank">Structural Time Series</a></h2>
  <a class="report-image" href="https://structural-time-series.fastforwardlabs.com" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff16-combo.png" />
  </a>
  <div class="small">Time series data is ubiquitous. This report examines generalized additive models, which give us a simple, flexible, and interpretable means for modeling time series by decomposing them into structural components. We look at the benefits and trade-offs of taking a curve-fitting approach to time series, and demonstrate its use via Facebook’s Prophet library on a demand forecasting problem.</div>
  <div><a href="https://structural-time-series.fastforwardlabs.com" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF15</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://meta-learning.fastforwardlabs.com" target="_blank">Meta-Learning</a></h2>
  <a class="report-image" href="https://meta-learning.fastforwardlabs.com" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff15-combo.png" />
  </a>
  <div class="small">In contrast to how humans learn, deep learning algorithms need vast amounts of data and compute and may yet struggle to generalize. Humans are successful in adapting quickly because they leverage their knowledge acquired from prior experience when faced with new problems. In this report, we explain how meta-learning can leverage previous knowledge acquired from data to solve novel tasks quickly and more efficiently during test time</div>
  <div><a href="https://meta-learning.fastforwardlabs.com" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF14</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://qa.fastforwardlabs.com" target="_blank">Automated Question Answering</a></h2>
  <a class="report-image" href="https://qa.fastforwardlabs.com" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff14-combo.png" />
  </a>
  <div class="small">Automated question answering is a user-friendly way to extract information from data using natural language. Thanks to recent advances in natural language processing, question answering capabilities from unstructured text data have grown rapidly. This blog series offers a walk-through detailing the technical and practical aspects of building an end-to-end question answering system.</div>
  <div><a href="https://qa.fastforwardlabs.com" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF13</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://ff13.fastforwardlabs.com" target="_blank">Causality for Machine Learning</a></h2>
  <a class="report-image" href="https://ff13.fastforwardlabs.com" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff13-combo.png" />
  </a>
  <div class="small">The intersection of causal inference and machine learning is a rapidly expanding area of research that&#39;s already yielding capabilities to enable building more robust, reliable, and fair machine learning systems. This report offers an introduction to causal reasoning including causal graphs and invariant prediction and how to apply causal inference tools together with classic machine learning techniques in multiple use-cases.</div>
  <div><a href="https://ff13.fastforwardlabs.com" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF06-2020</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://ff06-2020.fastforwardlabs.com" target="_blank">Interpretability</a></h2>
  <a class="report-image" href="https://ff06-2020.fastforwardlabs.com" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff06-2020-combo.png" />
  </a>
  <div class="small">Interpretability, or the ability to explain why and how a system makes a decision, can help us improve models, satisfy regulations, and build better products. Black-box techniques like deep learning have delivered breakthrough capabilities at the cost of interpretability. In this report, recently updated to include techniques like SHAP, we show how to make models interpretable without sacrificing their capabilities or accuracy.</div>
  <div><a href="https://ff06-2020.fastforwardlabs.com" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF12</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://ff12.fastforwardlabs.com" target="_blank">Deep Learning for Anomaly Detection</a></h2>
  <a class="report-image" href="https://ff12.fastforwardlabs.com" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff12-combo.png" />
  </a>
  <div class="small">From fraud detection to flagging abnormalities in imaging data, there are countless applications for automatic identification of abnormal data. This process can be challenging, especially when working with large, complex data. This report explores deep learning approaches (sequence models, VAEs, GANs) for anomaly detection, when to use them, performance benchmarks, and product possibilities.</div>
  <div><a href="https://ff12.fastforwardlabs.com" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF10</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://lwlld.fastforwardlabs.com/" target="_blank">Learning with Limited Labeled Data</a></h2>
  <a class="report-image" href="https://lwlld.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff10-combo.png" />
  </a>
  <div class="small">Being able to learn with limited labeled data relaxes the stringent labeled data requirement for supervised machine learning. This report focuses on active learning, a technique that relies on collaboration between machines and humans to label smartly. Active learning reduces the number of labeled examples required to train a model, saving time and money while obtaining comparable performance to models trained with much more data. With active learning, enterprises can leverage their large pool of unlabeled data to open up new product possibilities.</div>
  <div><a href="https://lwlld.fastforwardlabs.com/" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF09</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://federated.fastforwardlabs.com/" target="_blank">Federated Learning</a></h2>
  <a class="report-image" href="https://federated.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff09-combo.png" />
  </a>
  <div class="small">Federated Learning makes it possible to build machine learning systems without direct access to training data. The data remains in its original location, which helps to ensure privacy and reduces communication costs. Federated learning is a great fit for smartphones and edge hardware, healthcare and other privacy-sensitive use cases, and industrial applications such as predictive maintenance.</div>
  <div><a href="https://federated.fastforwardlabs.com/" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF04</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://summarization.fastforwardlabs.com/" target="_blank">Summarization</a></h2>
  <a class="report-image" href="https://summarization.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff04-combo.png" />
  </a>
  <div class="small">This report explores methods for extractive summarization, a capability that allows one to automatically summarize documents.  This technique has a wealth of applications: from the ability to distill thousands of product reviews, extract the most important content from long news articles, or automatically cluster customer bios into personas.</div>
  <div><a href="https://summarization.fastforwardlabs.com/" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF03-2019</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://deep-learning-image-analysis.fastforwardlabs.com/" target="_blank">Deep Learning for Image Analysis: 2019 Edition</a></h2>
  <a class="report-image" href="https://deep-learning-image-analysis.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff03-2019-combo.png" />
  </a>
  <div class="small">Convolutional Neural Networks (CNN) excel at learning meaningful representations of features and concepts within images. These capabilities make CNNs extremely valuable for solving problems in domains such as medical imaging, autonomous driving, manufacturing, robotics, and urban planning. In this report, we show how to select the right deep learning models for image analysis tasks and techniques for debugging deep learning models.</div>
  <div><a href="https://deep-learning-image-analysis.fastforwardlabs.com/" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
    <div class="report-link" style="position: relative;">
  <h5 style="margin-bottom: 0">FF03</h5>
  <h2 style="padding-top: 0; margin-bottom: 4px;"><a href="https://deep-learning-image-classic.fastforwardlabs.com/" target="_blank">Deep Learning: Image Analysis</a></h2>
  <a class="report-image" href="https://deep-learning-image-classic.fastforwardlabs.com/" target="_blank" style="display: block;">
    <img style="max-width: 31ch; display: block;" src="/images/reports/ff03-classic-combo.png" />
  </a>
  <div class="small">Deep learning, or highly-connected neural networks, offers fascinating new capabilities for image analysis. Using deep learning, computers can now learn to identify objects in images. This report explores the history and current state of the field, predicts future developments, and explains how to apply deep learning today.</div>
  <div><a href="https://deep-learning-image-classic.fastforwardlabs.com/" target="_blank">Read the report&nbsp; →</a></div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
</div>


</div>

<div class="spacer"></div>
<div class="spacer"></div>
 
<div class="container">
  <h1 class="clear">Prototypes</h1>
  <div style="color: #444;">Machine learning prototypes and interactive notebooks</div>
  <div class="spacer"></div>
</div>
<div id="prototypes-holder">
  
  <div style="display: flex; flex-direction: column; position: relative; padding-left: 1ch; padding-right: 1ch; height: 360px;">
  <div>
    <h5 style="margin-bottom: 0">Library</h5>
    <h2 style="padding-top: 0; margin-bottom: 0;"><a href="https://neuralqa.fastforwardlabs.com" target="_blank">NeuralQA</a></h2>
  </div>
  <div style="flex: 1 1 auto; position: relative;">
    <a href="https://neuralqa.fastforwardlabs.com" target="_blank" style="display: block; position: absolute; top: 0.375rem; width: 100%; height: calc(100% - 0.875rem); background-image: url('/images/hugo/neuralqa-1596123511.jpg'); background-size: contain; background-position: center left; background-repeat: no-repeat;"></a>
  </div>
  <div>
    <div class="small">A usable library for question answering on large datasets.</div>
    <div style="width: 100%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"><a href="https://neuralqa.fastforwardlabs.com" target="_blank">https://neuralqa.fastforwardlabs.com</a></div>
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
  <div style="display: flex; flex-direction: column; position: relative; padding-left: 1ch; padding-right: 1ch; height: 360px;">
  <div>
    <h5 style="margin-bottom: 0">Notebook</h5>
    <h2 style="padding-top: 0; margin-bottom: 0;"><a href="https://colab.research.google.com/drive/1tTiOgJ7xvy3sjfiFC9OozbjAX1ho8WN9?usp=sharing" target="_blank">Explain BERT for Question Answering Models</a></h2>
  </div>
  <div style="flex: 1 1 auto; position: relative;">
    <a href="https://colab.research.google.com/drive/1tTiOgJ7xvy3sjfiFC9OozbjAX1ho8WN9?usp=sharing" target="_blank" style="display: block; position: absolute; top: 0.375rem; width: 100%; height: calc(100% - 0.875rem); background-image: url('/images/hugo/distilexplanation-1592852137.jpg'); background-size: contain; background-position: center left; background-repeat: no-repeat;"></a>
  </div>
  <div>
    <div class="small">Tensorflow 2.0 notebook to explain and visualize a HuggingFace BERT for Question Answering model.</div>
    <div style="width: 100%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"><a href="https://colab.research.google.com/drive/1tTiOgJ7xvy3sjfiFC9OozbjAX1ho8WN9?usp=sharing" target="_blank">https://colab.research.google.com/drive/1tTiOgJ7xvy3sjfiFC9OozbjAX1ho8WN9?usp=sharing</a></div>
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
  <div style="display: flex; flex-direction: column; position: relative; padding-left: 1ch; padding-right: 1ch; height: 360px;">
  <div>
    <h5 style="margin-bottom: 0">Notebooks</h5>
    <h2 style="padding-top: 0; margin-bottom: 0;"><a href="https://qa.fastforwardlabs.com" target="_blank">NLP for Question Answering</a></h2>
  </div>
  <div style="flex: 1 1 auto; position: relative;">
    <a href="https://qa.fastforwardlabs.com" target="_blank" style="display: block; position: absolute; top: 0.375rem; width: 100%; height: calc(100% - 0.875rem); background-image: url('/images/uploads/qa.png'); background-size: contain; background-position: center left; background-repeat: no-repeat;"></a>
  </div>
  <div>
    <div class="small">Ongoing posts and code documenting the process of building a question answering model.</div>
    <div style="width: 100%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"><a href="https://qa.fastforwardlabs.com" target="_blank">https://qa.fastforwardlabs.com</a></div>
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
  <div style="display: flex; flex-direction: column; position: relative; padding-left: 1ch; padding-right: 1ch; height: 360px;">
  <div>
    <h5 style="margin-bottom: 0">Notebook</h5>
    <h2 style="padding-top: 0; margin-bottom: 0;"><a href="https://colab.research.google.com/drive/1pjPzsw_uZew-Zcz646JTkRDhF2GkPk0N" target="_blank">Interpretability Revisited: SHAP and LIME</a></h2>
  </div>
  <div style="flex: 1 1 auto; position: relative;">
    <a href="https://colab.research.google.com/drive/1pjPzsw_uZew-Zcz646JTkRDhF2GkPk0N" target="_blank" style="display: block; position: absolute; top: 0.375rem; width: 100%; height: calc(100% - 0.875rem); background-image: url('/images/uploads/shap-and-lime.png'); background-size: contain; background-position: center left; background-repeat: no-repeat;"></a>
  </div>
  <div>
    <div class="small">Explore how to use LIME and SHAP for interpretability.</div>
    <div style="width: 100%; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;"><a href="https://colab.research.google.com/drive/1pjPzsw_uZew-Zcz646JTkRDhF2GkPk0N" target="_blank">https://colab.research.google.com/drive/1pjPzsw_uZew-Zcz646JTkRDhF2GkPk0N</a></div>
  </div>
  <div class="spacer"></div>
  <div class="spacer"></div>
</div>


  
</div>
<div class="container">
  <div ><button id="load_all_prototypes" style="width: 100%;">load all</button></div>
</div>

<script>
  
  window.addEventListener('load', () => {
    let $prototypes_holder = document.getElementById('prototypes-holder')
    let $load_more = document.getElementById('load_all_prototypes')
    $load_more.addEventListener('click', () => {
      fetch(`/prototypes.html`).then(r =>r.text()).then(r => {
        $load_more.remove()
        let el = document.createElement('html')
        el.innerHTML = r
        let $posts = el.querySelector('#prototypes-holder')
        $prototypes_holder.innerHTML = $posts.innerHTML
      })
    })
  })
</script>



<div class="spacer"></div>
<div class="spacer"></div>

<div class="container">
  <div>
    <h1 class="clear">About</h1>
    <div>
      Cloudera Fast Forward is an applied machine learning reseach group.<br />
      <a
        href="https://www.cloudera.com/products/fast-forward-labs-research.html"
        >Cloudera</a
      >&nbsp;&nbsp;
      <a
        href="https://blog.fastforwardlabs.com"
        >Blog</a
      >&nbsp;&nbsp;
      <a href="https://twitter.com/fastforwardlabs">Twitter</a>
    </div>
  </div>
</div>



<div class="spacer"></div>
<div class="spacer"></div>


      </main>
 </body>
</html>
